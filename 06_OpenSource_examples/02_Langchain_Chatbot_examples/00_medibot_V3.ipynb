{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Interface - Medical Clinic\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "In this notebook, we will build a chatbot using the Foundation Models (FMs) in Amazon Bedrock. For our use-case we use Claude V3 Sonnet as our foundation models.  For more details refer to [Documentation](https://aws.amazon.com/bedrock/claude/). The ideal balance between intelligence and speed—particularly for enterprise workloads. It excels at complex reasoning, nuanced content creation, scientific queries, math, and coding. Data teams can use Sonnet for RAG, as well as search and retrieval across vast amounts of information while sales teams can leverage Sonnet for product recommendations, forecasting, and targeted marketing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers.Chatbots uses natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. They can be accessed through various channels such as websites, social media platforms, and messaging apps.\n",
    "\n",
    "\n",
    "## Chatbot using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/chatbot_bedrock.png)\n",
    "\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "1. **Chatbot (Basic)** - Zero Shot chatbot with a FM model\n",
    "2. **Chatbot using prompt** - template(Langchain) - Chatbot with some context provided in the prompt template\n",
    "3. **Chatbot with persona** - Chatbot with defined roles. i.e. Career Coach and Human interactions\n",
    "4. **Contextual-aware chatbot** - Passing in context through an external file by generating embeddings.\n",
    "\n",
    "## Langchain framework for building Chatbot with Amazon Bedrock\n",
    "In Conversational interfaces such as chatbots, it is highly important to remember previous interactions, both at a short term but also at a long term level.\n",
    "\n",
    "LangChain provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular and useful regardless of how they are used. Secondly, LangChain provides easy ways to incorporate these utilities into chains.\n",
    "It allows us to easily define and interact with different types of abstractions, which make it easy to build powerful chatbots.\n",
    "\n",
    "## Building Chatbot with Context - Key Elements\n",
    "\n",
    "The first process in a building a contextual-aware chatbot is to **generate embeddings** for the context. Typically, you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using Titan Embeddings model for this\n",
    "\n",
    "![Embeddings](./images/embeddings_lang.png)\n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "![Chatbot](./images/chatbot_lang.png)\n",
    "\n",
    "## Architecture [Context Aware Chatbot]\n",
    "![4](./images/context-aware-chatbot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've run the [Bedrock boto3 setup notebook](../00_Prerequisites/bedrock_basics.ipynb) notebook. ⚠️ ⚠️ ⚠️ Then run these installs below\n",
    "\n",
    "**please note**\n",
    "\n",
    "for we are tracking an annoying warning when using the RunnableWithMessageHistory [Runnable History Issue]('https://github.com/langchain-ai/langchain-aws/issues/150'). Please ignore the warning mesages for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-community==0.2.12\n",
    "# %pip install -U --no-cache-dir  \\\n",
    "#     \"langchain>=0.2.12\" \\\n",
    "#     sqlalchemy -U \\\n",
    "#     \"faiss-cpu>=1.7,<2\" \\\n",
    "#     \"pypdf>=3.8,<4\" \\\n",
    "#     pinecone-client>=5.0.1 \\\n",
    "#     tiktoken>=0.7.0 \\\n",
    "#     \"ipywidgets>=7,<8\" \\\n",
    "#     matplotlib>=3.9.0 \\\n",
    "#     anthropic>=0.32.0 \\\n",
    "#     \"langchain-aws>=0.1.15\"\n",
    "# - boto3-1.34.162 botocore-1.34.162 langchain-0.2.14 langchain-aws-0.1.17 langchain-core-0.2.34 langchain-community-0.2.12\n",
    "#%pip install -U --no-cache-dir transformers\n",
    "#%pip install -U --no-cache-dir boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_bedrock_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if runtime:\n",
    "        service_name='bedrock-runtime'\n",
    "    else:\n",
    "        service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region='us-west-2' #os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot (Basic - without context)\n",
    "\n",
    "We use [CoversationChain](https://python.langchain.com/en/latest/modules/models/llms/integrations/bedrock.html?highlight=ConversationChain#using-in-a-conversation-chain) from LangChain to start the conversation. We also use the [ConversationBufferMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer.html) for storing the messages. We can also get the history as a list of messages (this is very useful in a chat model).\n",
    "\n",
    "Chatbots needs to remember the previous interactions. Conversational memory allows us to do that. There are several ways that we can implement conversational memory. In the context of LangChain, they are all built on top of the ConversationChain.\n",
    "\n",
    "**Note:** The model outputs are non-deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'msg_bdrk_01U5KGtg7oWsxXWqE9kTRbpe', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': 'Sure, the concept of discrete or quantized energies is a key principle in quantum mechanics. It states that the energy of particles or systems can only take on certain specific values, rather than varying continuously.\\n\\nSome key points about discrete energies:\\n\\n- It contradicts classical physics, which assumed energy could have any value.\\n\\n- The allowed energy values are discrete, meaning there are gaps between the permitted values rather than a smooth continuum.\\n\\n- These discrete energy levels arise from the wave'}], 'stop_reason': 'max_tokens', 'stop_sequence': None, 'usage': {'input_tokens': 48, 'output_tokens': 100}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'msg_bdrk_01Y4SQo1cj4iPxjeP2d6XR2M',\n",
       " 'type': 'message',\n",
       " 'role': 'assistant',\n",
       " 'model': 'claude-3-sonnet-20240229',\n",
       " 'content': [{'type': 'text',\n",
       "   'text': 'Quantum mechanics is a fundamental theory in physics that describes the behavior of matter and energy on the atomic and subatomic scale. It is a mathematical framework that provides a way to understand and predict the properties and interactions of particles at the quantum level.\\n\\nHere are some key aspects of quantum mechanics:\\n\\n1. Wave-particle duality: In quantum mechanics, particles can exhibit both wave-like and particle-like behavior, depending on the experimental situation. This duality is a fundamental principle that'}],\n",
       " 'stop_reason': 'max_tokens',\n",
       " 'stop_sequence': None,\n",
       " 'usage': {'input_tokens': 11, 'output_tokens': 100}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "\n",
    "messages=[\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': \"What is quantum mechanics? \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'assistant', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': \"It is a branch of physics that describes how matter and energy interact with discrete energy values \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': \"Can you explain a bit more about discrete energies?\"\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 100,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.9\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "response = boto3_bedrock.invoke_model(body=body, modelId=modelId)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "print(response_body)\n",
    "\n",
    "\n",
    "def test_sample_claude_invoke(prompt_str,boto3_bedrock ):\n",
    "    modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "    messages=[{ \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': prompt_str\n",
    "        }]\n",
    "    }]\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 100,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.9\n",
    "        }  \n",
    "    )  \n",
    "    response = boto3_bedrock.invoke_model(body=body, modelId=modelId)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    return response_body\n",
    "\n",
    "\n",
    "test_sample_claude_invoke(\"what is quantum mechanics\", boto3_bedrock)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to ChatBedrock\n",
    "\n",
    "**Supports the following**\n",
    "1. Multiple Models from Bedrock \n",
    "2. Converse API\n",
    "3. Ability to do tool binding\n",
    "4. Ability to plug with LangGraph flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a overview of the typical weather in Seattle, Washington:\\n\\n- Seattle has a marine west coast climate, which means it gets a good amount of rainfall and moderate temperatures year-round.\\n\\n- Summers (June-August) are mild, with average highs around 75°F and lows around 55°F. It's the driest time of year, though light rain can still occur.\\n\\n- Winters (December-February) are cool and wet. Average highs are in the mid 40s°F and lows are in the mid 30s°F. Rainfall is frequent, with some snowfall possible as well.\\n\\n- Spring (March-May) and fall (September-November) are transitional seasons, with a mix of rainy periods and drier stretches. Highs range from the 50s to 60s°F.\\n\\n- Seattle gets an average of 37 inches of rainfall per year, with the wettest months being November through January.\\n\\n- Despite the rain reputation, Seattle also gets stretches of nice weather, especially in summer when highs in the 70s and low 80s are common.\\n\\nSo in general, you can expect cool, wet winters and mild, drier summers in this Pacific Northwest city. Having an umbrella and rain jacket handy year-round is advisable.\", response_metadata={'ResponseMetadata': {'RequestId': 'ecc6ba4d-e550-43ce-bd18-4bdd6128325a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 00:46:00 GMT', 'content-type': 'application/json', 'content-length': '1317', 'connection': 'keep-alive', 'x-amzn-requestid': 'ecc6ba4d-e550-43ce-bd18-4bdd6128325a'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 5388}}, id='run-19206489-7e64-4c51-bf67-9d06bfddb606-0', usage_metadata={'input_tokens': 16, 'output_tokens': 290, 'total_tokens': 306})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in Seattle WA\"\n",
    "    )\n",
    "]\n",
    "bedrock_llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to the converse api flag -- this class corectly formulates the messages correctly\n",
    "\n",
    "so we can directly use the string mesages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a overview of the typical weather in Seattle, Washington:\\n\\n- Seattle has a marine west coast climate, which means it gets a good amount of rain and moderate temperatures year-round.\\n\\n- Summers (June-August) are mild, with average highs around 75°F and lows around 55°F. It's the driest time of year.\\n\\n- Winters (December-February) are cool and wet. Average highs are in the mid 40s°F and lows are in the mid 30s°F. It rains frequently during the winter months.\\n\\n- Spring and fall tend to be rainy as well, with average temps in the 50s and 60s°F.\\n\\n- Seattle gets around 37 inches of rainfall per year on average, with the wettest months being November through January.\\n\\n- Despite the rain reputation, Seattle actually gets less annual rainfall than many East Coast cities like New York or Miami. However, the rain is spread out over many days.\\n\\n- Snowfall is relatively light, with only a few inches each year on average in Seattle proper.\\n\\nSo in summary - cool, wet winters and mild, drier summers characterize the maritime Pacific Northwest climate of Seattle. The rain and clouds are an iconic part of the city's weather pattern.\", response_metadata={'ResponseMetadata': {'RequestId': '97dfbb49-e9aa-47a8-b673-3088089d5792', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 00:46:07 GMT', 'content-type': 'application/json', 'content-length': '1344', 'connection': 'keep-alive', 'x-amzn-requestid': '97dfbb49-e9aa-47a8-b673-3088089d5792'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 6529}}, id='run-93190c86-2809-4030-9d84-b6a15b6497ce-0', usage_metadata={'input_tokens': 17, 'output_tokens': 280, 'total_tokens': 297})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_llm.invoke(\"what is the weather like in Seattle WA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask a follow on\n",
    "\n",
    "because we have not plugged in any History or context or api's the model wil not be able to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't actually experience temperature or seasons myself since I'm an AI assistant without a physical form. However, based on typical weather patterns, most places tend to have warmer temperatures during the summer months compared to other seasons of the year. The exact temperatures would depend on the specific geographic location being asked about. Many regions experience hot, sunny summer weather, while others may have more mild summer conditions.\", response_metadata={'ResponseMetadata': {'RequestId': 'aeefc423-4d5f-405f-88c8-bee753b95fc7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 00:46:10 GMT', 'content-type': 'application/json', 'content-length': '636', 'connection': 'keep-alive', 'x-amzn-requestid': 'aeefc423-4d5f-405f-88c8-bee753b95fc7'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 2454}}, id='run-412a3a96-24f1-4fe4-ac87-f95feb638183-0', usage_metadata={'input_tokens': 13, 'output_tokens': 83, 'total_tokens': 96})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_llm.invoke(\"is it warm in summers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the same question Meta Llama models\n",
    "\n",
    "**please make sure you have the models enabled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nSeattle, Washington is known for its mild and wet climate, with significant rainfall throughout the year. Here's a breakdown of the typical weather patterns in Seattle:\\n\\n1. Rainfall: Seattle is famous for its rain, with an average annual rainfall of around 37 inches (94 cm). The rainiest months are November to March, with an average of 15-20 rainy days per month.\\n2. Temperature: Seattle's average temperature ranges from 35°F (2°C) in January (the coldest month) to 77°F (25°C) in July (the warmest month). The average temperature is around 50°F (10°C) throughout the year.\\n3. Sunshine: Seattle gets an average of 154 sunny days per year, with the sunniest months being July and August. However, the sun can be obscured by clouds and fog, reducing the amount of direct sunlight.\\n4. Fog: Seattle is known for its fog, especially during the winter months. The city can experience fog for several days at a time, especially in the mornings.\\n5. Wind: Seattle is known for its strong winds, especially during the winter months. The city can experience gusts of up to 40 mph (64 km/h) during storms.\\n6. Snow: Seattle rarely sees significant snowfall, with an average annual snowfall of around 6 inches (15 cm). The snowiest month is usually January, with an average of 1-2 inches (2.5-5 cm) of snow.\\n7. Summer: Seattle's summer months (June to August) are mild and pleasant, with average highs in the mid-70s to low 80s (23-27°C). However, the city can experience occasional heatwaves, with temperatures reaching up to 90°F (32°C) or more.\\n8. Winter: Seattle's winter months (December to February) are cool and wet, with average lows in the mid-30s to low 40s (2-6°C). The city can experience occasional cold snaps, with temperatures dropping below 20°F (-7°C) for short periods.\\n\\nOverall, Seattle's weather is characterized by mild temperatures, significant rainfall, and overcast skies. It's essential to pack layers and waterproof clothing when visiting the city, especially during the winter months.\", response_metadata={'ResponseMetadata': {'RequestId': 'ddc4fd80-514e-4efb-a25f-f090df9d644c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 00:46:17 GMT', 'content-type': 'application/json', 'content-length': '2211', 'connection': 'keep-alive', 'x-amzn-requestid': 'ddc4fd80-514e-4efb-a25f-f090df9d644c'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 6179}}, id='run-b3efdd9b-13b2-4471-8c06-30fab2703911-0', usage_metadata={'input_tokens': 22, 'output_tokens': 472, 'total_tokens': 494})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in Seattle WA\"\n",
    "    )\n",
    "]\n",
    "bedrock_llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding prompt templates \n",
    "\n",
    "1. You can define prompts as a list of messages, all modesl expect SystemMessage, and then alternate with HumanMessage and AIMessage\n",
    "2. This means Context needs to be part of the System message \n",
    "3. Further the CHAT HISTORY needs to be right after the system message as a MessagePlaceholder which is a list of alternating [Human/AI]\n",
    "4. The Variables defined in the chat template need to be send into the chain as dict with the keys being the variable names\n",
    "5. You can define the template as a tuple with (\"system\", \"message\") or can be using the class SystemMessage \n",
    "6. Invoke creates a final resulting object of type <class 'langchain_core.prompt_values.ChatPromptValue'> with the variables substituted with their values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you\n",
      "can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy\n",
      "matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in\n",
      "Seattle.\"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"\\n    You are an assistant for question-answering tasks. ONLY Use\n",
      "the following pieces of retrieved context to answer the question.\\n    If the answer is not in the\n",
      "context below , just say you do not have enough context. \\n    If you don't know the answer, just\n",
      "say that you don't know. \\n    Use three sentences maximum and keep the answer concise.\\n\n",
      "Context: this is a test context \\n    \"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"You are an assistant for question-answering tasks. Use the\n",
      "following pieces of retrieved context to answer the question. If you don't know the answer, say that\n",
      "you don't know. Use three sentences maximum and keep the answer concise.\\n\\nthis is a test\n",
      "context\"), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy\n",
      "matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in\n",
      "Seattle.\"), HumanMessage(content='Explain this  test_input.')]\n",
      "\n",
      "\n",
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat_history_messages = [\n",
    "        HumanMessage(\"What is the weather like in Seattle WA?\"), # - normal string converts it to a Human message always but we need ai/human pairs\n",
    "        AIMessage(\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages( # can create either as System Message Object or as TUPLE -- system, message\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"), # this assumes the messages are in list of messages format and this becomes MessagePlaceholder object\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- variable chat_history should be a list of base messages, got test_chat_history of type <class 'str'>\n",
    "#- this gets converted as a LIST of messages -- with each of the TUPLE or Object being executed with the variables when invoked\n",
    "print_ww(prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages}))\n",
    "\n",
    "# -- condense question prompt with CONTEXT\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- missing variables {'context'}. chat history will get ignored - variables are passed in as keys in the dict\n",
    "print(\"\\n\")\n",
    "print_ww(condense_question_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))\n",
    "\n",
    "# - Chat prompt template with Place holders\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{contex}\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"Explain this  {input}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print_ww(qa_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(type(qa_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\"), HumanMessage(content='test_input')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ").invoke({'input': 'test_input', 'chat_history' : chat_history_messages})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Conversation chain \n",
    "\n",
    "**Uses the In memory Chat Message History**\n",
    "\n",
    "The above example uses the same history for all sessions. The example below shows how to use a different chat history for each session.\n",
    "\n",
    "**Note**\n",
    "1. `Chat History` is a variable is a place holder in the prompt template. which will have Human/Ai alternative messages\n",
    "2. Human query is the final question as `Input` variable\n",
    "3. config is the `{\"configurable\": {'session_id_variable':'value,....other keys}` These are passed into the any and all Runnable and wrappers of runnable\n",
    "4. `RunnableWithMessageHistory` is the class which we wrap the `chain` in to run with history. which is in [Docs link]('https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#')\n",
    "5. For production use cases, you will want to use a persistent implementation of chat message history, such as `RedisChatMessageHistory`.\n",
    "6. This class needs a DICT as a input\n",
    "7. chain has .input_schema.schema to get the json of how to pass in the input\n",
    "\n",
    "8. Configuration gets passed in as invoke({dict}, config={\"configurable\": {\"session_id\": \"abc123\"}}) and it gets converted to `RunnableConfig` which is passed into every invoke method. To access this we need to extend the Runnable class and access it\n",
    "\n",
    "\n",
    "Wrap the rag_chain with RunnableWithMessageHistory to automatically handle chat history:\n",
    "\n",
    "Any Chain wrapped with RunnableWithMessageHistory - will manage chat history variables appropriately, however the ChatTemplate should have the Placeholder for history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the same manually by configuring the chain with the chat history being Added and invoked automatically\n",
    "\n",
    "if we configue the chain manually not necessary all variables have to be invluded in the inputs. If those are being used or accessed then it will provide those\n",
    "\n",
    "1. For runnable we can either extend the runnable class\n",
    "2. Or we can define a method and create a runnable lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatUserAdd::input_dict:{'input': 'what is the weather like in Seattle WA?', 'chat_history':\n",
      "[]}::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks':\n",
      "<langchain_core.callbacks.manager.CallbackManager object at 0x11464df10>, 'recursion_limit': 25,\n",
      "'configurable': {'session_id': 'abc123'}}\n",
      "ChatHistoryAdd::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks':\n",
      "<langchain_core.callbacks.manager.CallbackManager object at 0x110256050>, 'recursion_limit': 25,\n",
      "'configurable': {'session_id': 'abc123'}}::history_object=Human: what is the weather like in Seattle\n",
      "WA?::input=content=\"I don't actually have specific current weather information for Seattle. As an AI\n",
      "assistant without direct access to real-time meteorological data, I can only provide general details\n",
      "about the typical weather patterns in that region. Seattle has a maritime climate with cool, rainy\n",
      "winters and mild, dry summers. However, I don't have an accurate report on the exact temperature,\n",
      "precipitation levels, etc. at this moment in Seattle.\" response_metadata={'ResponseMetadata':\n",
      "{'RequestId': 'dccb8a6f-aacb-4d9f-9608-7a3f7952f967', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date':\n",
      "'Sat, 24 Aug 2024 01:07:54 GMT', 'content-type': 'application/json', 'content-length': '615',\n",
      "'connection': 'keep-alive', 'x-amzn-requestid': 'dccb8a6f-aacb-4d9f-9608-7a3f7952f967'},\n",
      "'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 3329}}\n",
      "id='run-b2089b22-e6b8-444b-8b54-04eceaf2b3f7-0' usage_metadata={'input_tokens': 43, 'output_tokens':\n",
      "87, 'total_tokens': 130}::\n",
      "I don't actually have specific current weather information for Seattle. As an AI assistant without\n",
      "direct access to real-time meteorological data, I can only provide general details about the typical\n",
      "weather patterns in that region. Seattle has a maritime climate with cool, rainy winters and mild,\n",
      "dry summers. However, I don't have an accurate report on the exact temperature, precipitation\n",
      "levels, etc. at this moment in Seattle.\n",
      "\n",
      "\n",
      " chat_history after invocation is -- >Human: what is the weather like in Seattle WA?\n",
      "AI: I don't actually have specific current weather information for Seattle. As an AI assistant without direct access to real-time meteorological data, I can only provide general details about the typical weather patterns in that region. Seattle has a maritime climate with cool, rainy winters and mild, dry summers. However, I don't have an accurate report on the exact temperature, precipitation levels, etc. at this moment in Seattle.\n",
      "ChatUserAdd::input_dict:{'input': 'How is it in winters?', 'chat_history':\n",
      "[HumanMessage(content='what is the weather like in Seattle WA?'), AIMessage(content=\"I don't\n",
      "actually have specific current weather information for Seattle. As an AI assistant without direct\n",
      "access to real-time meteorological data, I can only provide general details about the typical\n",
      "weather patterns in that region. Seattle has a maritime climate with cool, rainy winters and mild,\n",
      "dry summers. However, I don't have an accurate report on the exact temperature, precipitation\n",
      "levels, etc. at this moment in Seattle.\")]}::config={'tags': [], 'metadata': {'session_id':\n",
      "'abc123'}, 'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x11cebda50>,\n",
      "'recursion_limit': 25, 'configurable': {'session_id': 'abc123'}}\n",
      "ChatHistoryAdd::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks':\n",
      "<langchain_core.callbacks.manager.CallbackManager object at 0x11ce542d0>, 'recursion_limit': 25,\n",
      "'configurable': {'session_id': 'abc123'}}::history_object=Human: what is the weather like in Seattle\n",
      "WA?\n",
      "AI: I don't actually have specific current weather information for Seattle. As an AI assistant\n",
      "without direct access to real-time meteorological data, I can only provide general details about the\n",
      "typical weather patterns in that region. Seattle has a maritime climate with cool, rainy winters and\n",
      "mild, dry summers. However, I don't have an accurate report on the exact temperature, precipitation\n",
      "levels, etc. at this moment in Seattle.\n",
      "Human: How is it in winters?::input=content='Ahoy matey! As a pirate, I don\\'t pay much mind to the\n",
      "landlubbers\\' notions of \"winters\" and such. But if ye must know about the dreary months in Seattle,\n",
      "here\\'s what an old salt like meself can tell ye:\\n\\nThe winters in Seattle can be quite soggy and\n",
      "gray. The skies are often overcast, and the rains come down in a steady drizzle fer months on end.\n",
      "The temperatures are cool but rarely get too frozen over - usually in the 40s Fahrenheit.\n",
      "\\n\\nHowever, every once in a while, a big storm will blow in off the Pacific, bringin\\' heavy rains\n",
      "and howlin\\' winds to batter the ships in the harbor. On occasions like that, ye\\'d best batten down\n",
      "the hatches and take cover belowdecks with a mug o\\' grog!\\n\\nThe damp chill can really settle into\n",
      "an old pirate\\'s bones durin\\' those Seattle winters. But we salts are a hardy bunch - a bit o\\'\n",
      "foul weather won\\'t stop us from raidin\\' and pillagin\\' as we please! Arrrr!'\n",
      "response_metadata={'ResponseMetadata': {'RequestId': 'dc80a7c6-70b7-447d-9bb6-a9fe3c203103',\n",
      "'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 01:08:02 GMT', 'content-type':\n",
      "'application/json', 'content-length': '1093', 'connection': 'keep-alive', 'x-amzn-requestid':\n",
      "'dc80a7c6-70b7-447d-9bb6-a9fe3c203103'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics':\n",
      "{'latencyMs': 7274}} id='run-892135c2-f240-4fe5-afc7-78ffa45fc920-0' usage_metadata={'input_tokens':\n",
      "135, 'output_tokens': 253, 'total_tokens': 388}::\n",
      "Ahoy matey! As a pirate, I don't pay much mind to the landlubbers' notions of \"winters\" and such.\n",
      "But if ye must know about the dreary months in Seattle, here's what an old salt like meself can tell\n",
      "ye:\n",
      "\n",
      "The winters in Seattle can be quite soggy and gray. The skies are often overcast, and the rains come\n",
      "down in a steady drizzle fer months on end. The temperatures are cool but rarely get too frozen over\n",
      "- usually in the 40s Fahrenheit.\n",
      "\n",
      "However, every once in a while, a big storm will blow in off the Pacific, bringin' heavy rains and\n",
      "howlin' winds to batter the ships in the harbor. On occasions like that, ye'd best batten down the\n",
      "hatches and take cover belowdecks with a mug o' grog!\n",
      "\n",
      "The damp chill can really settle into an old pirate's bones durin' those Seattle winters. But we\n",
      "salts are a hardy bunch - a bit o' foul weather won't stop us from raidin' and pillagin' as we\n",
      "please! Arrrr!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "prompt_with_history = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "# - add the history to the in-memory chat history\n",
    "class ChatHistoryAdd(Runnable):\n",
    "    def __init__(self, chat_history):\n",
    "        self.chat_history = chat_history\n",
    "\n",
    "    def invoke(self, input: str, config: RunnableConfig = None) -> str:\n",
    "        try:\n",
    "            print_ww(f\"ChatHistoryAdd::config={config}::history_object={self.chat_history}::input={input}::\")\n",
    "            \n",
    "            self.chat_history.add_ai_message(input.content)\n",
    "            return input\n",
    "        except Exception as e:\n",
    "            return f\"Error processing input: {str(e)}\"\n",
    "\n",
    "# Usage\n",
    "chat_add = ChatHistoryAdd(get_history())\n",
    "\n",
    "#- second way to create a callback runnable function--\n",
    "def ChatUserInputAdd(input_dict: dict, config: RunnableConfig) -> dict:\n",
    "    print_ww(f\"ChatUserAdd::input_dict:{input_dict}::config={config}\") #- if we do dict at start of chain -- {'input': {'input': 'what is the weather like in Seattle WA?', 'chat_history':\n",
    "    get_history().add_user_message(input_dict['input']) \n",
    "    return input_dict # return the text as is\n",
    "\n",
    "chat_user_add = RunnableLambda(ChatUserInputAdd)\n",
    "\n",
    "\n",
    "history_chain = (\n",
    "    #- Expected a Runnable, callable or dict. If we use a dict here make sure every element is a runnable. And further access is via 'input'.'input'\n",
    "    # { # make sure all variable in the prompt template are in this dict\n",
    "    #     \"input\": RunnablePassthrough(),\n",
    "    # }\n",
    "    RunnablePassthrough() # passes in the full dict as is -- since we have the variables defined in the INVOKE call itself\n",
    "    | chat_user_add\n",
    "    | prompt_with_history\n",
    "    | chatbedrock_llm\n",
    "    | chat_add\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "print_ww(history_chain.invoke( # here the variable matches the chat prompt template\n",
    "    {\"input\": \"what is the weather like in Seattle WA?\", \"chat_history\": get_history().messages}, \n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    ")\n",
    "\n",
    "print(f\"\\n\\n chat_history after invocation is -- >{get_history()}\")\n",
    "\n",
    "#- ask a follow on question\n",
    "print_ww(history_chain.invoke(\n",
    "    {\"input\": \"How is it in winters?\", \"chat_history\": get_history().messages}, \n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now use the In-built helper methods to continue \n",
    "\n",
    "1. We can see that the auto chain will add user and also the AI messages automatically at appropriate places\n",
    "2. Key needs to be the same as what we have in the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in\n",
      "Seattle. From what I know, it's often overcast and drizzly there, with plenty of precipitation\n",
      "throughout the year. The summers can be quite pleasant, but the winters are chilly and wet with lots\n",
      "of rain and grey skies. Batten down the hatches if ye be sailing into Seattle during storm season!\n",
      "That's about all I can tell ye without getting too landlubber-ish. Arrrr!\n",
      "\n",
      "INPUT_SCHEMA::{'title': 'RunnableWithChatHistoryInput', 'type': 'array', 'items': {'$ref':\n",
      "'#/definitions/BaseMessage'}, 'definitions': {'BaseMessage': {'title': 'BaseMessage', 'description':\n",
      "'Base abstract message class.\\n\\nMessages are the inputs and outputs of ChatModels.', 'type':\n",
      "'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type':\n",
      "'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs':\n",
      "{'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response\n",
      "Metadata', 'type': 'object'}, 'type': {'title': 'Type', 'type': 'string'}, 'name': {'title': 'Name',\n",
      "'type': 'string'}, 'id': {'title': 'Id', 'type': 'string'}}, 'required': ['content', 'type']}}}\n",
      "\n",
      "CHAIN:SCHEMA::{'title': 'RunnableWithMessageHistory', 'description': 'Runnable that manages chat\n",
      "message history for another Runnable.\\n\\nA chat message history is a sequence of messages that\n",
      "represent a conversation.\\n\\nRunnableWithMessageHistory wraps another Runnable and manages the chat\n",
      "message\\nhistory for it; it is responsible for reading and updating the chat\n",
      "message\\nhistory.\\n\\nThe formats supported for the inputs and outputs of the wrapped Runnable\\nare\n",
      "described below.\\n\\nRunnableWithMessageHistory must always be called with a config that\n",
      "contains\\nthe appropriate parameters for the chat message history factory.\\n\\nBy default, the\n",
      "Runnable is expected to take a single configuration parameter\\ncalled `session_id` which is a\n",
      "string. This parameter is used to create a new\\nor look up an existing chat message history that\n",
      "matches the given session_id.\\n\\nIn this case, the invocation would look like\n",
      "this:\\n\\n`with_history.invoke(..., config={\"configurable\": {\"session_id\": \"bar\"}})`\\n; e.g.,\n",
      "``{\"configurable\": {\"session_id\": \"<SESSION_ID>\"}}``.\\n\\nThe configuration can be customized by\n",
      "passing in a list of\\n``ConfigurableFieldSpec`` objects to the ``history_factory_config`` parameter\n",
      "(see\\nexample below).\\n\\nIn the examples, we will use a chat message history with an in-\n",
      "memory\\nimplementation to make it easy to experiment and see the results.\\n\\nFor production use\n",
      "cases, you will want to use a persistent implementation\\nof chat message history, such as\n",
      "``RedisChatMessageHistory``.\\n\\nParameters:\\n    get_session_history: Function that returns a new\n",
      "BaseChatMessageHistory.\\n        This function should either take a single positional argument\\n\n",
      "`session_id` of type string and return a corresponding\\n        chat message history instance.\\n\n",
      "input_messages_key: Must be specified if the base runnable accepts a dict\\n        as input. The key\n",
      "in the input dict that contains the messages.\\n    output_messages_key: Must be specified if the\n",
      "base Runnable returns a dict\\n        as output. The key in the output dict that contains the\n",
      "messages.\\n    history_messages_key: Must be specified if the base runnable accepts a dict\\n\n",
      "as input and expects a separate key for historical messages.\\n    history_factory_config: Configure\n",
      "fields that should be passed to the\\n        chat history factory. See ``ConfigurableFieldSpec`` for\n",
      "more details.\\n\\nExample: Chat message history with an in-memory implementation for testing.\\n\\n..\n",
      "code-block:: python\\n\\n    from operator import itemgetter\\n    from typing import List\\n\\n    from\n",
      "langchain_openai.chat_models import ChatOpenAI\\n\\n    from langchain_core.chat_history import\n",
      "BaseChatMessageHistory\\n    from langchain_core.documents import Document\\n    from\n",
      "langchain_core.messages import BaseMessage, AIMessage\\n    from langchain_core.prompts import\n",
      "ChatPromptTemplate, MessagesPlaceholder\\n    from langchain_core.pydantic_v1 import BaseModel,\n",
      "Field\\n    from langchain_core.runnables import (\\n        RunnableLambda,\\n\n",
      "ConfigurableFieldSpec,\\n        RunnablePassthrough,\\n    )\\n    from\n",
      "langchain_core.runnables.history import RunnableWithMessageHistory\\n\\n\\n    class\n",
      "InMemoryHistory(BaseChatMessageHistory, BaseModel):\\n        \"\"\"In memory implementation of chat\n",
      "message history.\"\"\"\\n\\n        messages: List[BaseMessage] = Field(default_factory=list)\\n\\n\n",
      "def add_messages(self, messages: List[BaseMessage]) -> None:\\n            \"\"\"Add a list of messages\n",
      "to the store\"\"\"\\n            self.messages.extend(messages)\\n\\n        def clear(self) -> None:\\n\n",
      "self.messages = []\\n\\n    # Here we use a global variable to store the chat message history.\\n    #\n",
      "This will make it easier to inspect it to see the underlying results.\\n    store = {}\\n\\n    def\n",
      "get_by_session_id(session_id: str) -> BaseChatMessageHistory:\\n        if session_id not in store:\\n\n",
      "store[session_id] = InMemoryHistory()\\n        return store[session_id]\\n\\n\\n    history =\n",
      "get_by_session_id(\"1\")\\n    history.add_message(AIMessage(content=\"hello\"))\\n    print(store)  #\n",
      "noqa: T201\\n\\n\\nExample where the wrapped Runnable takes a dictionary input:\\n\\n    .. code-block::\n",
      "python\\n\\n        from typing import Optional\\n\\n        from langchain_community.chat_models import\n",
      "ChatAnthropic\\n        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\n",
      "from langchain_core.runnables.history import RunnableWithMessageHistory\\n\\n\\n        prompt =\n",
      "ChatPromptTemplate.from_messages([\\n            (\"system\", \"You\\'re an assistant who\\'s good at\n",
      "{ability}\"),\\n            MessagesPlaceholder(variable_name=\"history\"),\\n            (\"human\",\n",
      "\"{question}\"),\\n        ])\\n\\n        chain = prompt | ChatAnthropic(model=\"claude-2\")\\n\\n\n",
      "chain_with_history = RunnableWithMessageHistory(\\n            chain,\\n            # Uses the\n",
      "get_by_session_id function defined in the example\\n            # above.\\n\n",
      "get_by_session_id,\\n            input_messages_key=\"question\",\\n\n",
      "history_messages_key=\"history\",\\n        )\\n\\n        print(chain_with_history.invoke(  # noqa:\n",
      "T201\\n            {\"ability\": \"math\", \"question\": \"What does cosine mean?\"},\\n\n",
      "config={\"configurable\": {\"session_id\": \"foo\"}}\\n        ))\\n\\n        # Uses the store defined in\n",
      "the example above.\\n        print(store)  # noqa: T201\\n\\n        print(chain_with_history.invoke(\n",
      "# noqa: T201\\n            {\"ability\": \"math\", \"question\": \"What\\'s its inverse\"},\\n\n",
      "config={\"configurable\": {\"session_id\": \"foo\"}}\\n        ))\\n\\n        print(store)  # noqa:\n",
      "T201\\n\\n\\nExample where the session factory takes two keys, user_id and conversation id):\\n\\n    ..\n",
      "code-block:: python\\n\\n        store = {}\\n\\n        def get_session_history(\\n            user_id:\n",
      "str, conversation_id: str\\n        ) -> BaseChatMessageHistory:\\n            if (user_id,\n",
      "conversation_id) not in store:\\n                store[(user_id, conversation_id)] =\n",
      "InMemoryHistory()\\n            return store[(user_id, conversation_id)]\\n\\n        prompt =\n",
      "ChatPromptTemplate.from_messages([\\n            (\"system\", \"You\\'re an assistant who\\'s good at\n",
      "{ability}\"),\\n            MessagesPlaceholder(variable_name=\"history\"),\\n            (\"human\",\n",
      "\"{question}\"),\\n        ])\\n\\n        chain = prompt | ChatAnthropic(model=\"claude-2\")\\n\\n\n",
      "with_message_history = RunnableWithMessageHistory(\\n            chain,\\n\n",
      "get_session_history=get_session_history,\\n            input_messages_key=\"question\",\\n\n",
      "history_messages_key=\"history\",\\n            history_factory_config=[\\n\n",
      "ConfigurableFieldSpec(\\n                    id=\"user_id\",\\n                    annotation=str,\\n\n",
      "name=\"User ID\",\\n                    description=\"Unique identifier for the user.\",\\n\n",
      "default=\"\",\\n                    is_shared=True,\\n                ),\\n\n",
      "ConfigurableFieldSpec(\\n                    id=\"conversation_id\",\\n\n",
      "annotation=str,\\n                    name=\"Conversation ID\",\\n\n",
      "description=\"Unique identifier for the conversation.\",\\n                    default=\"\",\\n\n",
      "is_shared=True,\\n                ),\\n            ],\\n        )\\n\\n\n",
      "with_message_history.invoke(\\n            {\"ability\": \"math\", \"question\": \"What does cosine\n",
      "mean?\"},\\n            config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\\n\n",
      ")', 'type': 'object', 'properties': {'name': {'title': 'Name', 'type': 'string'}, 'bound': {'title':\n",
      "'Bound', 'allOf': [{'type': 'array', 'items': [{}, {}]}]}, 'kwargs': {'title': 'Kwargs', 'type':\n",
      "'object'}, 'config': {'$ref': '#/definitions/RunnableConfig'}, 'custom_input_type': {'title':\n",
      "'Custom Input Type'}, 'custom_output_type': {'title': 'Custom Output Type'}, 'input_messages_key':\n",
      "{'title': 'Input Messages Key', 'type': 'string'}, 'output_messages_key': {'title': 'Output Messages\n",
      "Key', 'type': 'string'}, 'history_messages_key': {'title': 'History Messages Key', 'type':\n",
      "'string'}, 'history_factory_config': {'title': 'History Factory Config', 'type': 'array', 'items':\n",
      "{'type': 'array', 'items': [{'title': 'Id', 'type': 'string'}, {'title': 'Annotation'}, {'title':\n",
      "'Name', 'type': 'string'}, {'title': 'Description', 'type': 'string'}, {'title': 'Default'},\n",
      "{'title': 'Is Shared', 'type': 'boolean'}, {'title': 'Dependencies', 'type': 'array', 'items':\n",
      "{'type': 'string'}}], 'minItems': 7, 'maxItems': 7}}}, 'required': ['bound',\n",
      "'history_factory_config'], 'definitions': {'RunnableConfig': {'title': 'RunnableConfig', 'type':\n",
      "'object', 'properties': {'tags': {'title': 'Tags', 'type': 'array', 'items': {'type': 'string'}},\n",
      "'metadata': {'title': 'Metadata', 'type': 'object'}, 'callbacks': {'title': 'Callbacks', 'anyOf':\n",
      "[{'type': 'array', 'items': {}}, {}]}, 'run_name': {'title': 'Run Name', 'type': 'string'},\n",
      "'max_concurrency': {'title': 'Max Concurrency', 'type': 'integer'}, 'recursion_limit': {'title':\n",
      "'Recursion Limit', 'type': 'integer'}, 'configurable': {'title': 'Configurable', 'type': 'object'},\n",
      "'run_id': {'title': 'Run Id', 'type': 'string', 'format': 'uuid'}}}}}\n",
      "\n",
      "OUPUT_SCHEMA::__root__=None\n",
      "\n",
      "\n",
      " Now we run The example below shows how to use a different chat history for each session.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "chain = prompt | chatbedrock_llm | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print_ww(wrapped_chain.invoke({\"input\": \"what is the weather like in Seattle WA?\"}))\n",
    "\n",
    "\n",
    "print_ww(f\"\\nINPUT_SCHEMA::{wrapped_chain.input_schema.schema()}\")\n",
    "print_ww(f\"\\nCHAIN:SCHEMA::{wrapped_chain.schema()}\")\n",
    "print_ww(f\"\\nOUPUT_SCHEMA::{wrapped_chain.output_schema()}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n Now we run The example below shows how to use a different chat history for each session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: what is the weather like in Seattle WA?\n",
      "AI: Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle. From what I know, it's often overcast and drizzly there, with plenty of precipitation throughout the year. The summers can be quite pleasant, but the winters are chilly and wet with lots of rain and grey skies. Batten down the hatches if ye be sailing into Seattle during storm season! That's about all I can tell ye without getting too landlubber-ish. Arrrr!\n"
     ]
    }
   ],
   "source": [
    "print(history)\n",
    "# history.add_ai_message\n",
    "# history.add_user_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the multiple session id's with in memory conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy matey! As a pirate, I don't spend much time on land, but I've heard a few tales about the\n",
      "weather in Seattle. From what I know, it rains a lot there - they don't call it the Emerald City for\n",
      "nothin'! The summers can be quite pleasant though, with mild temperatures. And I've been told the\n",
      "winters are chilly and gray, with plenty of wind and drizzle comin' off the Puget Sound. The sea\n",
      "dogs who've made port there say to always have your slicker handy, no matter the season. Arrrr!\n",
      "\n",
      "\n",
      " now ask another question and we will see the History conversation was maintained\n",
      "Ahoy, landlubber! The damp weather in Seattle be a boon for any self-respectin' pirate. The near-\n",
      "constant drizzle keeps the city shrouded in a misty veil, perfect for sneakin' about unnoticed. The\n",
      "mild summers mean ye don't have to sweat like a scurvy dog under the blazin' sun. And the gray\n",
      "winters provide excellent cover for nighttime raids and plunderin'. Plus, all that rain makes the\n",
      "streets nice and slick for a quick getaway after relievin' some poor landlubber of their valuables.\n",
      "A pirate couldn't ask for better conditions for their ill-begotten activities! Arrrr!\n",
      "\n",
      "\n",
      " now check the history\n",
      "Human: what is the weather like in Seattle WA?\n",
      "AI: Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle. From what I know, it's often overcast and drizzly there, with plenty of precipitation throughout the year. The summers can be quite pleasant, but the winters are chilly and wet with lots of rain and grey skies. Batten down the hatches if ye be sailing into Seattle during storm season! That's about all I can tell ye without getting too landlubber-ish. Arrrr!\n"
     ]
    }
   ],
   "source": [
    "### This below LEVARAGES the In-memory with multiple sessions and session id\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    #print(session_id)\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain = prompt | chatbedrock_llm | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print_ww(wrapped_chain.invoke(\n",
    "    {\"input\": \"what is the weather like in Seattle WA\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "))\n",
    "\n",
    "print(\"\\n\\n now ask another question and we will see the History conversation was maintained\")\n",
    "print_ww(wrapped_chain.invoke(\n",
    "    {\"input\": \"Ok what are benefits of this weather in 100 words?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "))\n",
    "\n",
    "print(\"\\n\\n now check the history\")\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we do a Conversation Chat Chain with History and add a Retriever to that convo\n",
    "\n",
    "\n",
    "[Docs links]('https://python.langchain.com/v0.2/docs/versions/migrating_chains/conversation_retrieval_chain/')\n",
    "\n",
    "**Chat History needs to be a list since this is message api so alternate with human and user**\n",
    "\n",
    "1. The ConversationalRetrievalChain was an all-in one way that combined retrieval-augmented generation with chat history, allowing you to \"chat with\" your documents.\n",
    "\n",
    "2. Advantages of switching to the LCEL implementation are similar to the RetrievalQA section above:\n",
    "\n",
    "3. Clearer internals. The ConversationalRetrievalChain chain hides an entire question rephrasing step which dereferences the initial query against the chat history.\n",
    "4. This means the class contains two sets of configurable prompts, LLMs, etc.\n",
    "5. More easily return source documents.\n",
    "6. Support for runnable methods like streaming and async operations.\n",
    "\n",
    "**Below are the key classes to be used**\n",
    "\n",
    "1. We create a QA Chain using the qa_chain as `create_stuff_documents_chain(chatbedrock_llm, qa_prompt)`\n",
    "2. Then we create the Retrieval History chain using the `create_retrieval_chain(history_aware_retriever, qa_chain)`\n",
    "3. Retriever is wrapped in as `create_history_aware_retriever`\n",
    "4. `{context}` goes as System prompts which goes into the Prompt templates\n",
    "5. `Chat History` goes in the Prompt templates like \"placeholder\", \"{chat_history}\")\n",
    "\n",
    "The LCEL implementation exposes the internals of what's happening around retrieving, formatting documents, and passing them through a prompt to the LLM, but it is more verbose. You can customize and wrap this composition logic in a helper function, or use the higher-level `create_retrieval_chain` and `create_stuff_documents_chain` helper method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS as VectorStore\n",
    "\n",
    "In order to be able to use embeddings for search, we need a store that can efficiently perform vector similarity searches. In this notebook we use FAISS, which is an in memory store. For permanently store vectors, one can use pgVector, Pinecone or Chroma.\n",
    "\n",
    "The langchain VectorStore API's are available [here](https://python.langchain.com/en/harrison-docs-refactor-3-24/reference/modules/vectorstore.html)\n",
    "\n",
    "To know more about the FAISS vector store please refer to this [document](https://arxiv.org/pdf/1702.08734.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titan embeddings Model\n",
    "\n",
    "Embeddings are a way to represent words, phrases or any other discrete items as vectors in a continuous vector space. This allows machine learning models to perform mathematical operations on these representations and capture semantic relationships between them.\n",
    "\n",
    "Embeddings are for example used for the RAG [document search capability](https://labelbox.com/blog/how-vector-similarity-search-works/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garygrewal/virtualenv/trainenv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `BedrockEmbeddings` was deprecated in LangChain 0.2.11 and will be removed in 0.4.0. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import BedrockEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv to rag_data/Amazon_SageMaker_FAQs.csv\n",
      "Number of documents=153\n",
      "Number of documents after split and chunking=154\n",
      "vectorstore_faiss_aws: number of elements in the index=154::\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "\n",
    "s3_path = \"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv\"\n",
    "!aws s3 cp $s3_path ./rag_data/Amazon_SageMaker_FAQs.csv\n",
    "\n",
    "loader = CSVLoader(\"./rag_data/Amazon_SageMaker_FAQs.csv\") # --- > 219 docs with 400 chars, each row consists in a question column and an answer column\n",
    "documents_aws = loader.load() #\n",
    "print(f\"Number of documents={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Number of documents after split and chunking={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "\n",
    "    \n",
    "vectorstore_faiss_aws = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "     embedding = br_embeddings\n",
    ")\n",
    "\n",
    "print(f\"vectorstore_faiss_aws: number of elements in the index={vectorstore_faiss_aws.index.ntotal}::\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we do the simple Retrieval QA chain -- No chat history but with retriver\n",
    "[Docs link]('https://python.langchain.com/v0.2/docs/versions/migrating_chains/retrieval_qa/')\n",
    "\n",
    "Key points\n",
    "1. The chain in QA uses the variable as the first value, can be input or question  and so the prompt template for the Human query has to have the `Question` or `input` as the variable\n",
    "2. This chain will re formulate the question, call the retriver and then answer the question\n",
    "3. Our prompt template removes any answer where retriver is not needed and so no answer is obtained\n",
    "4. Context goes into the system prompts section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\"), HumanMessage(content='test_input')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ").invoke({'input': 'test_input', 'chat_history' : chat_history_messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'BedrockEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11df84a50>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_faiss_aws.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided does not contain enough information to fully explain what autonomous agents are\n",
      "in relation to reinforcement learning. However, based on the brief mention, I can say that in\n",
      "reinforcement learning, an autonomous agent is an entity that learns to make decisions and take\n",
      "actions within an environment in order to maximize some reward signal over time. Autonomous agents\n",
      "are a key concept in reinforcement learning, as the goal is for the agent to learn an optimal policy\n",
      "for behaving in the environment through trial-and-error interactions and feedback from the\n",
      "rewards/penalties received.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"), # expected by the qa chain as it sends in question as the variable\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    #print(docs)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vectorstore_faiss_aws.as_retriever() | format_docs, # can work even without the format\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | debug_inputs\n",
    "    | condense_question_prompt\n",
    "    | chatbedrock_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print_ww(qa_chain.invoke(input=\"What are autonomous agents?\")) # cannot be a dict object here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the same question to Meta Models\n",
    "**Note with the converse API we do not need to formulate or change any prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I don't have enough context to answer this question. The provided context only mentions autonomous\n",
      "vehicles and industrial control as examples of applications where reinforcement learning can be\n",
      "used, but it does not define what autonomous agents are.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"), # expected by the qa chain as it sends in question as the variable\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    #print(docs)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vectorstore_faiss_aws.as_retriever() | format_docs, # can work even without the format\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | condense_question_prompt\n",
    "    | chatbedrock_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print_ww(qa_chain.invoke(input=\"What are autonomous agents?\")) # cannot be a dict object here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we get a real answer as we invoke where retriever gives context\n",
    "\n",
    "Use the Helper method to create the Retiever QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What are the options for model explainability in SageMaker?', 'config': {'configurable':\n",
      "{'session_id': 'abc123'}}, 'context': [Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 11}, page_content='\\ufeffWhat is Amazon SageMaker?:\n",
      "How does Amazon SageMaker Clarify improve model explainability?\\nAmazon SageMaker is a fully managed\n",
      "service to prepare data and build, train, and deploy machine learning (ML) models for any use case\n",
      "with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Clarify is integrated\n",
      "with Amazon SageMaker Experiments to provide a feature importance graph detailing the importance of\n",
      "each input for your model’s overall decision-making process after the model has been trained. These\n",
      "details can help determine if a particular model input has more influence than it should on overall\n",
      "model behavior. SageMaker Clarify also makes explanations for individual predictions available via\n",
      "an API.'), Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 19},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: What does Amazon SageMaker Model Dashboard\n",
      "do?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy\n",
      "machine learning (ML) models for any use case with fully managed infrastructure, tools, and\n",
      "workflows.: Amazon SageMaker Model Dashboard gives you a comprehensive overview of deployed models\n",
      "and endpoints, letting you track resources and model behavior violations through one pane. It allows\n",
      "you to monitor model behavior in four dimensions, including data and model quality, and bias and\n",
      "feature attribution drift through its integration with Amazon SageMaker Model Monitor and Amazon\n",
      "SageMaker Clarify. SageMaker Model Dashboard also provides an integrated experience to set up and\n",
      "receive alerts for missing and inactive model monitoring jobs, and deviations in model behavior for\n",
      "model quality, data quality, bias drift, and feature attribution drift. You can further inspect\n",
      "individual models and analyze factors impacting model performance over time. Then, you can follow up\n",
      "with ML practitioners to take corrective measures.'), Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9}, page_content='\\ufeffWhat is Amazon SageMaker?:\n",
      "How can I check for imbalances in my model?\\nAmazon SageMaker is a fully managed service to prepare\n",
      "data and build, train, and deploy machine learning (ML) models for any use case with fully managed\n",
      "infrastructure, tools, and workflows.: Amazon SageMaker Clarify\\xa0helps improve model transparency\n",
      "by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances\n",
      "during data preparation, after training, and ongoing over time, and also includes tools to help\n",
      "explain ML models and their predictions. Findings can be shared through explainability reports.'),\n",
      "Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 118},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: Why should I use\\xa0Amazon SageMaker Inference\n",
      "Recommender?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and\n",
      "deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and\n",
      "workflows.: You should use SageMaker Inference Recommender if you need recommendations for the right\n",
      "endpoint configuration to improve performance and reduce costs. Previously, data scientists who\n",
      "wanted to deploy their models had to run manual benchmarks to select the right endpoint\n",
      "configuration. They had to first select the right ML instance type out of the 70+ available instance\n",
      "types based on the resource requirements of their models and sample payloads, and then optimize the\n",
      "model to account for differing hardware. Then, they had to conduct extensive load tests to validate\n",
      "that latency and throughput requirements are met and that the costs are low. SageMaker Inference\n",
      "Recommender eliminates this complexity by making it easy for you to: 1) get started in minutes with\n",
      "an instance recommendation; 2) conduct load tests across instance types to get recommendations on\n",
      "your endpoint configuration within hours; and 3) automatically tune container and model server\n",
      "parameters as well as perform model optimizations for a given instance type.')], 'answer':\n",
      "\"According to the provided context, Amazon SageMaker offers the following options for improving\n",
      "model explainability:\\n\\n1. Amazon SageMaker Clarify: It provides a feature importance graph\n",
      "detailing the importance of each input for the model's overall decision-making process after\n",
      "training. It also makes explanations for individual predictions available via an API. This helps\n",
      "determine if a particular model input has more influence than it should on the overall model\n",
      "behavior.\\n\\n2. Amazon SageMaker Clarify also helps detect statistical bias across the entire ML\n",
      "workflow - during data preparation, after training, and ongoing monitoring over time. It includes\n",
      "tools to explain ML models and their predictions, and findings can be shared through explainability\n",
      "reports.\\n\\nThe context mentions that SageMaker Clarify is integrated with SageMaker Experiments and\n",
      "SageMaker Model Monitor to provide these model explainability capabilities. However, it does not\n",
      "provide any other specific options for model explainability in SageMaker.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "qa_chain = create_stuff_documents_chain(chatbedrock_llm, condense_question_prompt)\n",
    "\n",
    "convo_qa_chain = create_retrieval_chain(vectorstore_faiss_aws.as_retriever(), qa_chain)\n",
    "\n",
    "print_ww(convo_qa_chain.invoke(\n",
    "    {'input':\"What are the options for model explainability in SageMaker?\", \n",
    "      'config':{\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "    })) # cannot be a dict object here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we create Chat Conversation which has history and retrieval context\n",
    "So we use the HISTORY AWARE Retriever and create a chain\n",
    "\n",
    "1. We create a stuff chain\n",
    "2. Then we pass it to the create retrieval chain method -- we could have used the LCEL as well to create the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "contextualized_question_system_template = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualized_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualized_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chatbedrock_llm, vectorstore_faiss_aws.as_retriever(), contextualized_question_prompt\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{contex}\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"Explain this  {input}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(chatbedrock_llm, qa_prompt)\n",
    "\n",
    "convo_qa_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
    "\n",
    "convo_qa_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are the options for model explainability in SageMaker?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you\n",
      "can.'), HumanMessage(content='Human: what is the weather like in Seattle WA?'),\n",
      "AIMessage(content=\"AI: Ahoy matey! As a pirate, I don't spend much time on land, but I've heard\n",
      "tales of the weather in Seattle.\"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"\\n    You are an assistant for question-answering tasks. ONLY Use\n",
      "the following pieces of retrieved context to answer the question.\\n    If the answer is not in the\n",
      "context below , just say you do not have enough context. \\n    If you don't know the answer, just\n",
      "say that you don't know. \\n    Use three sentences maximum and keep the answer concise.\\n\n",
      "Context: this is a test context \\n    \"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"You are an assistant for question-answering tasks. Use the\n",
      "following pieces of retrieved context to answer the question. If you don't know the answer, say that\n",
      "you don't know. Use three sentences maximum and keep the answer concise.\\n\\nthis is a test\n",
      "context\"), HumanMessage(content='Human: what is the weather like in Seattle WA?'),\n",
      "AIMessage(content=\"AI: Ahoy matey! As a pirate, I don't spend much time on land, but I've heard\n",
      "tales of the weather in Seattle.\"), HumanMessage(content='Explain this  test_input.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat_history_messages = [\n",
    "        HumanMessage(\"Human: what is the weather like in Seattle WA?\"), # - normal string converts it to a Human message always but we need ai/human pairs\n",
    "        AIMessage(\"AI: Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages( # can create either as System Message Object or as TUPLE -- system, message\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"), # this assumes the messages are in list of messages format and this becomes MessagePlaceholder object\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- variable chat_history should be a list of base messages, got test_chat_history of type <class 'str'>\n",
    "#- this gets converted as a LIST of messages -- with each of the TUPLE or Object being executed with the variables when invoked\n",
    "print_ww(prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages}))\n",
    "\n",
    "# -- condense question prompt with CONTEXT\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- missing variables {'context'}. chat history will get ignored - variables are passed in as keys in the dict\n",
    "print(\"\\n\")\n",
    "print_ww(condense_question_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))\n",
    "\n",
    "# - Chat prompt template with Place holders\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{contex}\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"Explain this  {input}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print_ww(qa_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto add the history to the Chat with Retriever\n",
    "\n",
    "Wrap with Runnable Chat History with Session id and run the chat conversation\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/context_aware_history_retriever.png)\n",
    "\n",
    "borrowed from https://github.com/langchain-ai/langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "contextualized_question_system_template = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualized_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualized_question_system_template),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chatbedrock_llm, vectorstore_faiss_aws.as_retriever(), contextualized_question_prompt\n",
    ")\n",
    "\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If the answer is not present in the context, just say you do not have enough context to answer. \\\n",
    "If the input is not present in the context, just say you do not have enough context to answer. \\\n",
    "If the question is not present in the context, just say you do not have enough context to answer. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "question_answer_chain = create_stuff_documents_chain(chatbedrock_llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "#- Wrap the rag_chain with RunnableWithMessageHistory to automatically handle chat history:\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    #print(session_id)\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain_with_history.invoke(\n",
    "    {\"input\": \"What kind of bias can SageMaker detect?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a follow on question\n",
    "\n",
    "1. The phrase `it` will be converted based on the chat history\n",
    "2. Retriever gets invoked to get relevant content based on chat history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "print_ww(follow_up_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"Will it help?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "print_ww(follow_up_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now ask a random question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"Give me a few tips on how to plant a  new garden.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "follow_up_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the semantic search works:\n",
    "1. First we calculate the embeddings vector for the query, and\n",
    "2. then we use this vector to do a similarity search on the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = br_embeddings.embed_query(\"R in SageMaker\")\n",
    "print(v[0:10])\n",
    "results = vectorstore_faiss_aws.similarity_search_by_vector(v, k=4)\n",
    "for r in results:\n",
    "    print_ww(r.page_content)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory\n",
    "In any chatbot we will need a QA Chain with various options which are customized by the use case. But in a chatbot we will always need to keep the history of the conversation so the model can take it into consideration to provide the answer. In this example we use the [ConversationalRetrievalChain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db) from LangChain, together with a ConversationBufferMemory to keep the history of the conversation.\n",
    "\n",
    "Source: https://python.langchain.com/docs/modules/chains/popular/chat_vector_db\n",
    "\n",
    "Set `verbose` to `True` to see all the what is going on behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "print_ww(CONDENSE_QUESTION_PROMPT.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters used for ConversationRetrievalChain\n",
    "* **retriever**: We used `VectorStoreRetriever`, which is backed by a `VectorStore`. To retrieve text, there are two search types you can choose: `\"similarity\"` or `\"mmr\"`. `search_type=\"similarity\"` uses similarity search in the retriever object where it selects text chunk vectors that are most similar to the question vector.\n",
    "\n",
    "* **memory**: Memory Chain to store the history \n",
    "\n",
    "* **condense_question_prompt**: Given a question from the user, we use the previous conversation and that question to make up a standalone question\n",
    "\n",
    "* **chain_type**: If the chat history is long and doesn't fit the context you use this parameter and the options are `stuff`, `refine`, `map_reduce`, `map-rerank`\n",
    "\n",
    "If the question asked is outside the scope of context, then the model will reply it doesn't know the answer\n",
    "\n",
    "**Note**: if you are curious how the chain works, uncomment the `verbose=True` line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do some prompt engineering\n",
    "\n",
    "You can \"tune\" your prompt to get more or less verbose answers. For example, try to change the number of sentences, or remove that instruction all-together. You might also need to change the number of `max_tokens` (eg 1000 or 2000) to get the full answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this demo we used Claude V3 sonnet LLM to create conversational interface with following patterns:\n",
    "\n",
    "1. Chatbot (Basic - without context)\n",
    "\n",
    "2. Chatbot using prompt template(Langchain)\n",
    "\n",
    "3. Chatbot with personas\n",
    "\n",
    "4. Chatbot with context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "trainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
