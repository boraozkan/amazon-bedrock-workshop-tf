{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Interface - Medical Clinic\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "In this notebook, we will build a chatbot using the Foundation Models (FMs) in Amazon Bedrock. For our use-case we use Claude V3 Sonnet as our foundation models.  For more details refer to [Documentation](https://aws.amazon.com/bedrock/claude/). The ideal balance between intelligence and speed—particularly for enterprise workloads. It excels at complex reasoning, nuanced content creation, scientific queries, math, and coding. Data teams can use Sonnet for RAG, as well as search and retrieval across vast amounts of information while sales teams can leverage Sonnet for product recommendations, forecasting, and targeted marketing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers.Chatbots uses natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. They can be accessed through various channels such as websites, social media platforms, and messaging apps.\n",
    "\n",
    "\n",
    "## Chatbot using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/chatbot_bedrock.png)\n",
    "\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "1. **Chatbot (Basic)** - Zero Shot chatbot with a FM model\n",
    "2. **Chatbot using prompt** - template(Langchain) - Chatbot with some context provided in the prompt template\n",
    "3. **Chatbot with persona** - Chatbot with defined roles. i.e. Career Coach and Human interactions\n",
    "4. **Contextual-aware chatbot** - Passing in context through an external file by generating embeddings.\n",
    "\n",
    "## Langchain framework for building Chatbot with Amazon Bedrock\n",
    "In Conversational interfaces such as chatbots, it is highly important to remember previous interactions, both at a short term but also at a long term level.\n",
    "\n",
    "LangChain provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular and useful regardless of how they are used. Secondly, LangChain provides easy ways to incorporate these utilities into chains.\n",
    "It allows us to easily define and interact with different types of abstractions, which make it easy to build powerful chatbots.\n",
    "\n",
    "## Building Chatbot with Context - Key Elements\n",
    "\n",
    "The first process in a building a contextual-aware chatbot is to **generate embeddings** for the context. Typically, you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using Titan Embeddings model for this\n",
    "\n",
    "![Embeddings](./images/embeddings_lang.png)\n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "![Chatbot](./images/chatbot_lang.png)\n",
    "\n",
    "## Architecture [Context Aware Chatbot]\n",
    "![4](./images/context-aware-chatbot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've run the [Bedrock boto3 setup notebook](../00_Prerequisites/bedrock_basics.ipynb) notebook. ⚠️ ⚠️ ⚠️ Then run these installs below\n",
    "\n",
    "**please note**\n",
    "\n",
    "for we are tracking an annoying warning when using the RunnableWithMessageHistory [Runnable History Issue]('https://github.com/langchain-ai/langchain-aws/issues/150'). Please ignore the warning mesages for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-community==0.2.12\n",
    "# %pip install -U --no-cache-dir  \\\n",
    "#     \"langchain>=0.2.12\" \\\n",
    "#     sqlalchemy -U \\\n",
    "#     \"faiss-cpu>=1.7,<2\" \\\n",
    "#     \"pypdf>=3.8,<4\" \\\n",
    "#     pinecone-client>=5.0.1 \\\n",
    "#     tiktoken>=0.7.0 \\\n",
    "#     \"ipywidgets>=7,<8\" \\\n",
    "#     matplotlib>=3.9.0 \\\n",
    "#     anthropic>=0.32.0 \\\n",
    "#     \"langchain-aws>=0.1.15\"\n",
    "# - boto3-1.34.162 botocore-1.34.162 langchain-0.2.14 langchain-aws-0.1.17 langchain-core-0.2.34 langchain-community-0.2.12\n",
    "#%pip install -U --no-cache-dir transformers\n",
    "#%pip install -U --no-cache-dir boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_bedrock_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if runtime:\n",
    "        service_name='bedrock-runtime'\n",
    "    else:\n",
    "        service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region='us-west-2' #os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "models_list = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region='us-west-2', #os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=False\n",
    ").list_foundation_models()\n",
    "\n",
    "#[models['modelId'] for models in models_list['modelSummaries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3.Session().client(\"s3\").list_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot (Basic - without context)\n",
    "\n",
    "We use [CoversationChain](https://python.langchain.com/en/latest/modules/models/llms/integrations/bedrock.html?highlight=ConversationChain#using-in-a-conversation-chain) from LangChain to start the conversation. We also use the [ConversationBufferMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer.html) for storing the messages. We can also get the history as a list of messages (this is very useful in a chat model).\n",
    "\n",
    "Chatbots needs to remember the previous interactions. Conversational memory allows us to do that. There are several ways that we can implement conversational memory. In the context of LangChain, they are all built on top of the ConversationChain.\n",
    "\n",
    "**Note:** The model outputs are non-deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In classical physics, energy is often thought of as being continuous, meaning it can take on any value within a certain range. For example, the energy of a rolling ball can be thought of as being anywhere from 0 to infinity, with any value in between being possible.\n",
      "\n",
      "In contrast, quantum mechanics introduces the concept of discrete energy levels or states. This means that energy is not continuous, but rather comes in specific, distinct packets or quanta. These quanta are separated by gaps, and\n",
      "--- Latency: 1358ms - Input tokens:58 - Output tokens:100 ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nQuantum mechanics is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, such as atoms and subatomic particles. It provides a new and different framework for understanding physical phenomena, and it has been incredibly successful in explaining a wide range of experimental results.\\n\\nThe core idea of quantum mechanics is that, at the atomic and subatomic level, particles do not have definite positions, velocities, or properties until they are measured. Instead, they exist in a state of super'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "modelId = 'meta.llama3-8b-instruct-v1:0'\n",
    "\n",
    "messages_list=[\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': \"What is quantum mechanics? \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'assistant', \n",
    "        \"content\":[{\n",
    "            'text': \"It is a branch of physics that describes how matter and energy interact with discrete energy values \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': \"Can you explain a bit more about discrete energies?\"\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "\n",
    "    \n",
    "response = boto3_bedrock.converse(\n",
    "    messages=messages_list, \n",
    "    modelId='meta.llama3-8b-instruct-v1:0',\n",
    "    inferenceConfig={\n",
    "        \"temperature\": 0.5,\n",
    "        \"maxTokens\": 100,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    ")\n",
    "response_body = response['output']['message']['content'][0]['text'] \\\n",
    "        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
    "        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
    "        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n'\n",
    "\n",
    "print(response_body)\n",
    "\n",
    "\n",
    "def invoke_meta_converse(prompt_str,boto3_bedrock ):\n",
    "    modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "    messages_list=[{ \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': prompt_str\n",
    "        }]\n",
    "    }]\n",
    "  \n",
    "    response = boto3_bedrock.converse(\n",
    "        messages=messages_list, \n",
    "        modelId=modelId,\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0.5,\n",
    "            \"maxTokens\": 100,\n",
    "            \"topP\": 0.9\n",
    "        }\n",
    "    )\n",
    "    response_body = response['output']['message']['content'][0]['text']\n",
    "    return response_body\n",
    "\n",
    "\n",
    "invoke_meta_converse(\"what is quantum mechanics\", boto3_bedrock)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to ChatBedrock\n",
    "\n",
    "**Supports the following**\n",
    "1. Multiple Models from Bedrock \n",
    "2. Converse API\n",
    "3. Ability to do tool binding\n",
    "4. Ability to plug with LangGraph flows\n",
    "\n",
    "### Ask the question Meta Llama models\n",
    "\n",
    "**please make sure you have the models enabled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nSeattle, Washington is known for its mild and wet climate, with significant rainfall throughout the year. Here's a breakdown of the typical weather patterns in Seattle:\\n\\n1. Rainfall: Seattle is famous for its rain, with an average annual rainfall of around 37 inches (94 cm). The rainiest months are November to March, with an average of 15-20 rainy days per month.\\n2. Temperature: Seattle's average temperature ranges from 35°F (2°C) in January (the coldest month) to 77°F (25°C) in July (the warmest month). The average temperature is around 50°F (10°C) throughout the year.\\n3. Sunshine: Seattle gets an average of 154 sunny days per year, with the sunniest months being July and August. However, the sun can be obscured by clouds and fog, reducing the amount of direct sunlight.\\n4. Fog: Seattle is known for its fog, especially during the winter months. The city can experience fog for several days at a time, especially in the mornings.\\n5. Wind: Seattle is known for its strong winds, especially during the winter months. The city can experience gusts of up to 40 mph (64 km/h) during storms.\\n6. Snow: Seattle rarely sees significant snowfall, with an average annual snowfall of around 6 inches (15 cm). The snowiest month is usually January, with an average of 1-2 inches (2.5-5 cm) of snow.\\n7. Summer: Seattle's summer months (June to August) are mild and pleasant, with average highs in the mid-70s to low 80s (23-27°C). However, the city can experience occasional heatwaves, with temperatures reaching up to 90°F (32°C) or more.\\n8. Winter: Seattle's winter months (December to February) are cool and wet, with average lows in the mid-30s to low 40s (2-6°C). The city can experience occasional cold snaps, with temperatures dropping below 20°F (-7°C) for short periods.\\n\\nOverall, Seattle's weather is characterized by mild temperatures, significant rainfall, and overcast skies. It's essential to pack layers and waterproof clothing when visiting the city, especially during the winter months.\", response_metadata={'ResponseMetadata': {'RequestId': '5ed1fcff-e69f-45f6-8f52-bd6891eec799', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 18:01:53 GMT', 'content-type': 'application/json', 'content-length': '2211', 'connection': 'keep-alive', 'x-amzn-requestid': '5ed1fcff-e69f-45f6-8f52-bd6891eec799'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 6161}}, id='run-20b5c336-f368-47b3-a6b6-1f5d6815dfa9-0', usage_metadata={'input_tokens': 22, 'output_tokens': 472, 'total_tokens': 494})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 200}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in Seattle WA\"\n",
    "    )\n",
    "]\n",
    "bedrock_llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to the converse api flag -- this class corectly formulates the messages correctly\n",
    "\n",
    "so we can directly use the string mesages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nSeattle, Washington is known for its mild and wet climate, with significant rainfall throughout the year. Here's a breakdown of the typical weather patterns in Seattle:\\n\\n1. Rainfall: Seattle is famous for its rain, with an average annual rainfall of around 37 inches (94 cm). The rainiest months are November to March, with an average of 15-20 rainy days per month.\\n2. Temperature: Seattle's average temperature ranges from 35°F (2°C) in January (the coldest month) to 77°F (25°C) in July (the warmest month). The average temperature is around 50°F (10°C) throughout the year.\\n3. Sunshine: Seattle gets an average of 154 sunny days per year, with the sunniest months being July and August. However, the sun can be obscured by clouds and fog, reducing the amount of direct sunlight.\\n4. Fog: Seattle is known for its fog, especially during the winter months. The city can experience fog for several days at a time, especially in the mornings.\\n5. Wind: Seattle is known for its windy conditions, especially during the winter months. The city can experience strong winds, especially in the Puget Sound area.\\n6. Snow: Seattle rarely sees significant snowfall, with an average annual snowfall of around 6 inches (15 cm). The snowiest month is usually January, with an average of 1-2 inches (2.5-5 cm) of snow.\\n7. Seasonal changes: Seattle's climate is characterized by distinct seasonal changes, with:\\n\\t* Spring (March to May): Mild temperatures, increasing sunshine, and occasional rain showers.\\n\\t* Summer (June to August): Warm temperatures, long days, and occasional heatwaves.\\n\\t* Autumn (September to November): Cool temperatures, decreasing sunshine, and increasing rainfall.\\n\\t* Winter (December to February): Cool temperatures, frequent rain, and occasional snow.\\n\\nKeep in mind that these are general weather patterns, and actual conditions can vary from year to year. It's always a good idea to check current weather forecasts and conditions before planning your trip to Seattle.\", response_metadata={'ResponseMetadata': {'RequestId': 'dddf070b-fd8a-4252-8a6e-56a8c170e45c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 18:02:05 GMT', 'content-type': 'application/json', 'content-length': '2192', 'connection': 'keep-alive', 'x-amzn-requestid': 'dddf070b-fd8a-4252-8a6e-56a8c170e45c'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 5542}}, id='run-23f765f4-be09-4d37-a43d-ee78c7f5f01d-0', usage_metadata={'input_tokens': 23, 'output_tokens': 436, 'total_tokens': 459})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_llm.invoke(\"what is the weather like in Seattle WA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask a follow on\n",
    "\n",
    "because we have not plugged in any History or context or api's the model wil not be able to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n\\nThe warmth of summers depends on the location and climate. In general, summer is the warmest season in many parts of the world, especially near the equator.\\n\\nIn tropical regions, such as near the equator, summers are often extremely hot and humid. Temperatures can soar above 90°F (32°C) and even reach as high as 100°F (38°C) or more in some areas.\\n\\nIn temperate regions, such as in the Northern Hemisphere, summers are usually warm but not as hot as in tropical regions. Temperatures can range from the mid-70s to the mid-80s Fahrenheit (23-30°C).\\n\\nIn some regions, such as in the Southern Hemisphere, summers can be quite mild, especially in areas with a Mediterranean climate. Temperatures may range from the mid-60s to the mid-70s Fahrenheit (18-24°C).\\n\\nSome examples of warm summer temperatures in different parts of the world include:\\n\\n* In the United States, temperatures in the summer can range from 80°F (27°C) in the Northeast to 100°F (38°C) in the Southwest.\\n* In Europe, temperatures in the summer can range from 65°F (18°C) in the UK to 90°F (32°C) in southern Europe.\\n* In Australia, temperatures in the summer can range from 75°F (24°C) in the southeast to 95°F (35°C) in the northwest.\\n* In Africa, temperatures in the summer can range from 80°F (27°C) in the north to 100°F (38°C) in the south.\\n\\nOverall, the warmth of summers can vary greatly depending on the location and climate.', response_metadata={'ResponseMetadata': {'RequestId': '908327ed-9cc1-43a3-84a9-8e05753c36b0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 18:02:10 GMT', 'content-type': 'application/json', 'content-length': '1626', 'connection': 'keep-alive', 'x-amzn-requestid': '908327ed-9cc1-43a3-84a9-8e05753c36b0'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 4442}}, id='run-784dba7e-0a4a-4921-bcac-8ddea25539d9-0', usage_metadata={'input_tokens': 20, 'output_tokens': 345, 'total_tokens': 365})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_llm.invoke(\"is it warm in summers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nSeattle, Washington is known for its mild and wet climate, with significant rainfall throughout the year. Here's a breakdown of the typical weather patterns in Seattle:\\n\\n1. Rainfall: Seattle is famous for its rain, with an average annual rainfall of around 37 inches (94 cm). The rainiest months are November to March, with an average of 15-20 rainy days per month.\\n2. Temperature: Seattle's average temperature ranges from 35°F (2°C) in January (the coldest month) to 77°F (25°C) in July (the warmest month). The average temperature is around 50°F (10°C) throughout the year.\\n3. Sunshine: Seattle gets an average of 154 sunny days per year, with the sunniest months being July and August. However, the sun can be obscured by clouds and fog, reducing the amount of direct sunlight.\\n4. Fog: Seattle is known for its fog, especially during the winter months. The city can experience fog for several days at a time, especially in the mornings.\\n5. Wind: Seattle is known for its strong winds, especially during the winter months. The city can experience gusts of up to 40 mph (64 km/h) during storms.\\n6. Snow: Seattle rarely sees significant snowfall, with an average annual snowfall of around 6 inches (15 cm). The snowiest month is usually January, with an average of 1-2 inches (2.5-5 cm) of snow.\\n7. Summer: Seattle's summer months (June to August) are mild and pleasant, with average highs in the mid-70s to mid-80s (23-30°C). However, the city can experience occasional heatwaves, with temperatures reaching up to 90°F (32°C) or more.\\n8. Winter: Seattle's winter months (December to February) are cool and wet, with average lows in the mid-30s to mid-40s (2-7°C). The city can experience occasional cold snaps, with temperatures dropping below 20°F (-7°C) for short periods.\\n\\nOverall, Seattle's weather is characterized by mild temperatures, significant rainfall, and overcast skies. It's essential to pack layers and waterproof clothing when visiting the city, especially during the winter months.\", response_metadata={'ResponseMetadata': {'RequestId': '08b0642f-d4b4-4844-a8e0-ccbc20861ad7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 18:02:16 GMT', 'content-type': 'application/json', 'content-length': '2211', 'connection': 'keep-alive', 'x-amzn-requestid': '08b0642f-d4b4-4844-a8e0-ccbc20861ad7'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 6149}}, id='run-0e7056fe-7f34-4963-8bb1-22584d920255-0', usage_metadata={'input_tokens': 22, 'output_tokens': 472, 'total_tokens': 494})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in Seattle WA\"\n",
    "    )\n",
    "]\n",
    "bedrock_llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding prompt templates \n",
    "\n",
    "1. You can define prompts as a list of messages, all modesl expect SystemMessage, and then alternate with HumanMessage and AIMessage\n",
    "2. This means Context needs to be part of the System message \n",
    "3. Further the CHAT HISTORY needs to be right after the system message as a MessagePlaceholder which is a list of alternating [Human/AI]\n",
    "4. The Variables defined in the chat template need to be send into the chain as dict with the keys being the variable names\n",
    "5. You can define the template as a tuple with (\"system\", \"message\") or can be using the class SystemMessage \n",
    "6. Invoke creates a final resulting object of type <class 'langchain_core.prompt_values.ChatPromptValue'> with the variables substituted with their values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you\n",
      "can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy\n",
      "matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in\n",
      "Seattle.\"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"\\n    You are an assistant for question-answering tasks. ONLY Use\n",
      "the following pieces of retrieved context to answer the question.\\n    If the answer is not in the\n",
      "context below , just say you do not have enough context. \\n    If you don't know the answer, just\n",
      "say that you don't know. \\n    Use three sentences maximum and keep the answer concise.\\n\n",
      "Context: this is a test context \\n    \"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"You are an assistant for question-answering tasks. Use the\n",
      "following pieces of retrieved context to answer the question. If you don't know the answer, say that\n",
      "you don't know. Use three sentences maximum and keep the answer concise.\\n\\nthis is a test\n",
      "context\"), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy\n",
      "matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in\n",
      "Seattle.\"), HumanMessage(content='Explain this  test_input.')]\n",
      "\n",
      "\n",
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat_history_messages = [\n",
    "        HumanMessage(\"What is the weather like in Seattle WA?\"), # - normal string converts it to a Human message always but we need ai/human pairs\n",
    "        AIMessage(\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages( # can create either as System Message Object or as TUPLE -- system, message\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"), # this assumes the messages are in list of messages format and this becomes MessagePlaceholder object\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- variable chat_history should be a list of base messages, got test_chat_history of type <class 'str'>\n",
    "#- this gets converted as a LIST of messages -- with each of the TUPLE or Object being executed with the variables when invoked\n",
    "print_ww(prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages}))\n",
    "\n",
    "# -- condense question prompt with CONTEXT\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- missing variables {'context'}. chat history will get ignored - variables are passed in as keys in the dict\n",
    "print(\"\\n\")\n",
    "print_ww(condense_question_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))\n",
    "\n",
    "# - Chat prompt template with Place holders\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{contex}\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"Explain this  {input}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print_ww(qa_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(type(qa_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\"), HumanMessage(content='test_input')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ").invoke({'input': 'test_input', 'chat_history' : chat_history_messages})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Conversation chain \n",
    "\n",
    "**Uses the In memory Chat Message History**\n",
    "\n",
    "The above example uses the same history for all sessions. The example below shows how to use a different chat history for each session.\n",
    "\n",
    "**Note**\n",
    "1. `Chat History` is a variable is a place holder in the prompt template. which will have Human/Ai alternative messages\n",
    "2. Human query is the final question as `Input` variable\n",
    "3. config is the `{\"configurable\": {'session_id_variable':'value,....other keys}` These are passed into the any and all Runnable and wrappers of runnable\n",
    "4. `RunnableWithMessageHistory` is the class which we wrap the `chain` in to run with history. which is in [Docs link]('https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#')\n",
    "5. For production use cases, you will want to use a persistent implementation of chat message history, such as `RedisChatMessageHistory`.\n",
    "6. This class needs a DICT as a input\n",
    "7. chain has .input_schema.schema to get the json of how to pass in the input\n",
    "\n",
    "8. Configuration gets passed in as invoke({dict}, config={\"configurable\": {\"session_id\": \"abc123\"}}) and it gets converted to `RunnableConfig` which is passed into every invoke method. To access this we need to extend the Runnable class and access it\n",
    "9. The chain usually processes the inputs as a dict object\n",
    "\n",
    "\n",
    "Wrap the rag_chain with RunnableWithMessageHistory to automatically handle chat history:\n",
    "\n",
    "Any Chain wrapped with RunnableWithMessageHistory - will manage chat history variables appropriately, however the ChatTemplate should have the Placeholder for history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the same manually by configuring the chain with the chat history being Added and invoked automatically\n",
    "\n",
    "if we configue the chain manually not necessary all variables have to be invluded in the inputs. If those are being used or accessed then it will provide those\n",
    "\n",
    "1. For runnable we can either extend the runnable class\n",
    "2. Or we can define a method and create a runnable lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've never\n",
      "set foot in Seattle, Washington. But I've heard tales o' the Emerald City's weather from me mateys\n",
      "who've sailed those waters.\n",
      "\n",
      "From what I've gathered, Seattle's weather be as unpredictable as a barnacle on a ship's hull. It's\n",
      "known for bein' rainy and gray, with overcast skies most o' the time. The city gets a fair amount o'\n",
      "precipitation, with an average o' 226 days o' rain per year! That be a lot o' wet weather, matey!\n",
      "\n",
      "But don't ye worry, there be some sunshine to be had, too. The summer months o' June, July, and\n",
      "August be the driest, with an average o' 15-20 days o' sunshine. And in the winter, the days be\n",
      "shorter, but the sun still shines bright, even if it be through the clouds.\n",
      "\n",
      "So, if ye be plannin' a trip to Seattle, be prepared for some rain, but don't let it dampen yer\n",
      "spirits, matey! Just grab yer trusty umbrella and a good pair o' boots, and ye'll be ready to take\n",
      "on the Emerald City's weather like a swashbucklin' pirate!\n",
      "\n",
      "\n",
      " chat_history after invocation is -- >Human: what is the weather like in Seattle WA?\n",
      "AI: \n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've never set foot in Seattle, Washington. But I've heard tales o' the Emerald City's weather from me mateys who've sailed those waters.\n",
      "\n",
      "From what I've gathered, Seattle's weather be as unpredictable as a barnacle on a ship's hull. It's known for bein' rainy and gray, with overcast skies most o' the time. The city gets a fair amount o' precipitation, with an average o' 226 days o' rain per year! That be a lot o' wet weather, matey!\n",
      "\n",
      "But don't ye worry, there be some sunshine to be had, too. The summer months o' June, July, and August be the driest, with an average o' 15-20 days o' sunshine. And in the winter, the days be shorter, but the sun still shines bright, even if it be through the clouds.\n",
      "\n",
      "So, if ye be plannin' a trip to Seattle, be prepared for some rain, but don't let it dampen yer spirits, matey! Just grab yer trusty umbrella and a good pair o' boots, and ye'll be ready to take on the Emerald City's weather like a swashbucklin' pirate!\n",
      "\n",
      "\n",
      "Winters in Seattle, ye say? Well, matey, it be a different story altogether! Winter in Seattle be a\n",
      "chilly and wet affair, with the Pacific Northwest's famous rain comin' back in full force.\n",
      "\n",
      "From December to February, the average temperature be around 40°F (4°C), with lows often droppin' to\n",
      "around 35°F (2°C) or even colder. And don't be surprised if ye wake up to a layer o' frost on yer\n",
      "ship's... er, I mean, yer car's windshield!\n",
      "\n",
      "But the rain, oh the rain! It be a constant companion, matey. Seattle gets an average o' 17 inches\n",
      "(43 cm) o' rain in the winter months, with some days seein' as much as 2-3 inches (5-7.5 cm) o'\n",
      "precipitation! That be a lot o' wet weather, even for a pirate like meself!\n",
      "\n",
      "And don't even get me started on the wind, matey! Winter be the season o' strong gusts and howlin'\n",
      "gales, makin' it feel like the winds o' the high seas are blowin' right through the city!\n",
      "\n",
      "But, as a pirate, I be made o' sterner stuff, and I be tellin' ye, there be some advantages to\n",
      "winter in Seattle. The rain makes the city's parks and gardens look like a lush, green oasis, and\n",
      "the cooler weather be perfect for explorin' the city's many museums and indoor attractions.\n",
      "\n",
      "So, if ye be plannin' a trip to Seattle in the winter, just remember to pack yer rain gear, a good\n",
      "coat, and a sturdy pair o' boots, and ye'll be ready to take on the Emerald City's winter weather\n",
      "like a true pirate!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "prompt_with_history = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "# - add the history to the in-memory chat history\n",
    "class ChatHistoryAdd(Runnable):\n",
    "    def __init__(self, chat_history):\n",
    "        self.chat_history = chat_history\n",
    "\n",
    "    def invoke(self, input: str, config: RunnableConfig = None) -> str:\n",
    "        try:\n",
    "            #print_ww(f\"ChatHistoryAdd::config={config}::history_object={self.chat_history}::input={input}::\")\n",
    "            \n",
    "            self.chat_history.add_ai_message(input.content)\n",
    "            return input\n",
    "        except Exception as e:\n",
    "            return f\"Error processing input: {str(e)}\"\n",
    "\n",
    "# Usage\n",
    "chat_add = ChatHistoryAdd(get_history())\n",
    "\n",
    "#- second way to create a callback runnable function--\n",
    "def ChatUserInputAdd(input_dict: dict, config: RunnableConfig) -> dict:\n",
    "    #print_ww(f\"ChatUserAdd::input_dict:{input_dict}::config={config}\") #- if we do dict at start of chain -- {'input': {'input': 'what is the weather like in Seattle WA?', 'chat_history':\n",
    "    get_history().add_user_message(input_dict['input']) \n",
    "    return input_dict # return the text as is\n",
    "\n",
    "chat_user_add = RunnableLambda(ChatUserInputAdd)\n",
    "\n",
    "\n",
    "history_chain = (\n",
    "    #- Expected a Runnable, callable or dict. If we use a dict here make sure every element is a runnable. And further access is via 'input'.'input'\n",
    "    # { # make sure all variable in the prompt template are in this dict\n",
    "    #     \"input\": RunnablePassthrough(),\n",
    "    #     \"chat_history\": get_history().messages\n",
    "    # }\n",
    "    RunnablePassthrough() # passes in the full dict as is -- since we have the variables defined in the INVOKE call itself\n",
    "    | chat_user_add\n",
    "    | prompt_with_history\n",
    "    | chatbedrock_llm\n",
    "    | chat_add\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "print_ww(history_chain.invoke( # here the variable matches the chat prompt template\n",
    "    {\"input\": \"what is the weather like in Seattle WA?\", \"chat_history\": get_history().messages}, \n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    ")\n",
    "\n",
    "print(f\"\\n\\n chat_history after invocation is -- >{get_history()}\")\n",
    "\n",
    "#- ask a follow on question\n",
    "print_ww(history_chain.invoke(\n",
    "    {\"input\": \"How is it in winters?\", \"chat_history\": get_history().messages}, \n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate way of invoking \n",
    "\n",
    "1. Here  only use input is sent in as a string\n",
    "2. The chain tales care of the History of chats addition to the whole prompt\n",
    "3. We create a new Chain -- `but we are re-using the same History Object` and hence it has the previous conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history::input_dict:what is it like in autumn?::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x11a232650>, 'recursion_limit': 25, 'configurable': {'session_id': 'abc123'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAutumn in Seattle, matey! It be a grand time o' year, indeed! The Pacific Northwest's autumn season be a time o' transition, when the summer's warmth gives way to the winter's chill. And Seattle, being the Emerald City, be a sight to behold during this time o' year.\\n\\nIn autumn, the days be gettin' shorter, with the sun risin' later and setin' earlier. But the weather be mild, with average highs in the mid-50s to low 60s Fahrenheit (13-18°C). It be a perfect time to get out and about, takin' in the sights and sounds o' the city.\\n\\nThe rain, which be a constant companion in Seattle, starts to pick up in October and November, but it be a gentle, misty rain that adds to the autumnal atmosphere. And the leaves, oh the leaves! The trees in Seattle's parks and gardens be ablaze with color, turnin' shades o' gold, orange, and red. It be a pirate's delight, matey!\\n\\nBut autumn in Seattle be more than just the weather and the scenery. It be a time o' harvest and celebration, with the city's many farmers' markets and food festivals fillin' the air with the smells o' fresh produce and baked goods. And the coffee, oh the coffee! Seattle be the birthplace o' the coffee revolution, and autumn be the perfect time to cozy up with a warm cup o' joe and watch the leaves fall.\\n\\nSo, if ye be plannin' a trip to Seattle in the autumn, be sure to pack yer layers, yer rain gear, and yer sense o' adventure. And don't forget to take in the sights and sounds o' the city, matey!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#- second way to create a callback runnable function--\n",
    "def get_chat_history(input_dict: dict, config: RunnableConfig) -> dict:\n",
    "    print(f\"get_chat_history::input_dict:{input_dict}::config={config}\") #- if we do dict at start of chain -- {'input': {'input': 'what is the weather like in Seattle WA?', 'chat_history':\n",
    "    return get_history().messages # return the text as is\n",
    "\n",
    "chat_history_get = RunnableLambda(get_chat_history)\n",
    "\n",
    "history_chain = (\n",
    "    #- Expected a Runnable, callable or dict. If we use a dict here make sure every element is a runnable. And further access is via 'input'.'input'\n",
    "    { # make sure all variable in the prompt template are in this dict\n",
    "        \"input\": RunnablePassthrough(),\n",
    "        \"chat_history\": chat_history_get\n",
    "    }\n",
    "    | chat_user_add\n",
    "    | prompt_with_history\n",
    "    | chatbedrock_llm\n",
    "    | chat_add\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "history_chain.invoke( # here the variable matches the chat prompt template\n",
    "    \"what is it like in autumn?\", \n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now use the In-built helper methods to continue \n",
    "\n",
    "1. We can see that the auto chain will add user and also the AI messages automatically at appropriate places\n",
    "2. Key needs to be the same as what we have in the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arrr, shiver me timbers! Seattle, ye say? Well, matey, I've had me share o' adventures on the high\n",
      "seas, but I've never set foot in that damp and drizzly place. But I've heard tell from me mateys\n",
      "who've sailed those waters that Seattle's weather be as unpredictable as a barnacle on a ship's\n",
      "hull!\n",
      "\n",
      "From what I've gathered, Seattle's got a reputation for bein' a soggy place, with rain comin' down\n",
      "like a stormy sea on most days o' the year. The clouds be gray and thick, like a pirate's beard\n",
      "after a long voyage at sea. And don't even get me started on the wind, matey! It be as fierce as a\n",
      "sea monster, blowin' in from the Pacific and makin' ye want to tie yerself to the mast!\n",
      "\n",
      "But, I've also heard that when the sun does come out, it be as bright as a chest overflowin' with\n",
      "gold doubloons! So, if ye be lookin' for a bit o' sunshine, ye might want to keep yer eye on the\n",
      "forecast, matey!\n",
      "\n",
      "So, there ye have it, me take on the weather in Seattle, WA. Now, if ye'll excuse me, I've got to\n",
      "get back to me ship and me trusty parrot, Polly. We've got a date with the high seas, and I don't\n",
      "want to be late!\n",
      "\n",
      "INPUT_SCHEMA::{'title': 'RunnableWithChatHistoryInput', 'type': 'array', 'items': {'$ref':\n",
      "'#/definitions/BaseMessage'}, 'definitions': {'BaseMessage': {'title': 'BaseMessage', 'description':\n",
      "'Base abstract message class.\\n\\nMessages are the inputs and outputs of ChatModels.', 'type':\n",
      "'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type':\n",
      "'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs':\n",
      "{'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response\n",
      "Metadata', 'type': 'object'}, 'type': {'title': 'Type', 'type': 'string'}, 'name': {'title': 'Name',\n",
      "'type': 'string'}, 'id': {'title': 'Id', 'type': 'string'}}, 'required': ['content', 'type']}}}\n",
      "\n",
      "CHAIN:SCHEMA::{'title': 'RunnableWithMessageHistory', 'description': 'Runnable that manages chat\n",
      "message history for another Runnable.\\n\\nA chat message history is a sequence of messages that\n",
      "represent a conversation.\\n\\nRunnableWithMessageHistory wraps another Runnable and manages the chat\n",
      "message\\nhistory for it; it is responsible for reading and updating the chat\n",
      "message\\nhistory.\\n\\nThe formats supported for the inputs and outputs of the wrapped Runnable\\nare\n",
      "described below.\\n\\nRunnableWithMessageHistory must always be called with a config that\n",
      "contains\\nthe appropriate parameters for the chat message history factory.\\n\\nBy default, the\n",
      "Runnable is expected to take a single configuration parameter\\ncalled `session_id` which is a\n",
      "string. This parameter is used to create a new\\nor look up an existing chat message history that\n",
      "matches the given session_id.\\n\\nIn this case, the invocation would look like\n",
      "this:\\n\\n`with_history.invoke(..., config={\"configurable\": {\"session_id\": \"bar\"}})`\\n; e.g.,\n",
      "``{\"configurable\": {\"session_id\": \"<SESSION_ID>\"}}``.\\n\\nThe configuration can be customized by\n",
      "passing in a list of\\n``ConfigurableFieldSpec`` objects to the ``history_factory_config`` parameter\n",
      "(see\\nexample below).\\n\\nIn the examples, we will use a chat message history with an in-\n",
      "memory\\nimplementation to make it easy to experiment and see the results.\\n\\nFor production use\n",
      "cases, you will want to use a persistent implementation\\nof chat message history, such as\n",
      "``RedisChatMessageHistory``.\\n\\nParameters:\\n    get_session_history: Function that returns a new\n",
      "BaseChatMessageHistory.\\n        This function should either take a single positional argument\\n\n",
      "`session_id` of type string and return a corresponding\\n        chat message history instance.\\n\n",
      "input_messages_key: Must be specified if the base runnable accepts a dict\\n        as input. The key\n",
      "in the input dict that contains the messages.\\n    output_messages_key: Must be specified if the\n",
      "base Runnable returns a dict\\n        as output. The key in the output dict that contains the\n",
      "messages.\\n    history_messages_key: Must be specified if the base runnable accepts a dict\\n\n",
      "as input and expects a separate key for historical messages.\\n    history_factory_config: Configure\n",
      "fields that should be passed to the\\n        chat history factory. See ``ConfigurableFieldSpec`` for\n",
      "more details.\\n\\nExample: Chat message history with an in-memory implementation for testing.\\n\\n..\n",
      "code-block:: python\\n\\n    from operator import itemgetter\\n    from typing import List\\n\\n    from\n",
      "langchain_openai.chat_models import ChatOpenAI\\n\\n    from langchain_core.chat_history import\n",
      "BaseChatMessageHistory\\n    from langchain_core.documents import Document\\n    from\n",
      "langchain_core.messages import BaseMessage, AIMessage\\n    from langchain_core.prompts import\n",
      "ChatPromptTemplate, MessagesPlaceholder\\n    from langchain_core.pydantic_v1 import BaseModel,\n",
      "Field\\n    from langchain_core.runnables import (\\n        RunnableLambda,\\n\n",
      "ConfigurableFieldSpec,\\n        RunnablePassthrough,\\n    )\\n    from\n",
      "langchain_core.runnables.history import RunnableWithMessageHistory\\n\\n\\n    class\n",
      "InMemoryHistory(BaseChatMessageHistory, BaseModel):\\n        \"\"\"In memory implementation of chat\n",
      "message history.\"\"\"\\n\\n        messages: List[BaseMessage] = Field(default_factory=list)\\n\\n\n",
      "def add_messages(self, messages: List[BaseMessage]) -> None:\\n            \"\"\"Add a list of messages\n",
      "to the store\"\"\"\\n            self.messages.extend(messages)\\n\\n        def clear(self) -> None:\\n\n",
      "self.messages = []\\n\\n    # Here we use a global variable to store the chat message history.\\n    #\n",
      "This will make it easier to inspect it to see the underlying results.\\n    store = {}\\n\\n    def\n",
      "get_by_session_id(session_id: str) -> BaseChatMessageHistory:\\n        if session_id not in store:\\n\n",
      "store[session_id] = InMemoryHistory()\\n        return store[session_id]\\n\\n\\n    history =\n",
      "get_by_session_id(\"1\")\\n    history.add_message(AIMessage(content=\"hello\"))\\n    print(store)  #\n",
      "noqa: T201\\n\\n\\nExample where the wrapped Runnable takes a dictionary input:\\n\\n    .. code-block::\n",
      "python\\n\\n        from typing import Optional\\n\\n        from langchain_community.chat_models import\n",
      "ChatAnthropic\\n        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\n",
      "from langchain_core.runnables.history import RunnableWithMessageHistory\\n\\n\\n        prompt =\n",
      "ChatPromptTemplate.from_messages([\\n            (\"system\", \"You\\'re an assistant who\\'s good at\n",
      "{ability}\"),\\n            MessagesPlaceholder(variable_name=\"history\"),\\n            (\"human\",\n",
      "\"{question}\"),\\n        ])\\n\\n        chain = prompt | ChatAnthropic(model=\"claude-2\")\\n\\n\n",
      "chain_with_history = RunnableWithMessageHistory(\\n            chain,\\n            # Uses the\n",
      "get_by_session_id function defined in the example\\n            # above.\\n\n",
      "get_by_session_id,\\n            input_messages_key=\"question\",\\n\n",
      "history_messages_key=\"history\",\\n        )\\n\\n        print(chain_with_history.invoke(  # noqa:\n",
      "T201\\n            {\"ability\": \"math\", \"question\": \"What does cosine mean?\"},\\n\n",
      "config={\"configurable\": {\"session_id\": \"foo\"}}\\n        ))\\n\\n        # Uses the store defined in\n",
      "the example above.\\n        print(store)  # noqa: T201\\n\\n        print(chain_with_history.invoke(\n",
      "# noqa: T201\\n            {\"ability\": \"math\", \"question\": \"What\\'s its inverse\"},\\n\n",
      "config={\"configurable\": {\"session_id\": \"foo\"}}\\n        ))\\n\\n        print(store)  # noqa:\n",
      "T201\\n\\n\\nExample where the session factory takes two keys, user_id and conversation id):\\n\\n    ..\n",
      "code-block:: python\\n\\n        store = {}\\n\\n        def get_session_history(\\n            user_id:\n",
      "str, conversation_id: str\\n        ) -> BaseChatMessageHistory:\\n            if (user_id,\n",
      "conversation_id) not in store:\\n                store[(user_id, conversation_id)] =\n",
      "InMemoryHistory()\\n            return store[(user_id, conversation_id)]\\n\\n        prompt =\n",
      "ChatPromptTemplate.from_messages([\\n            (\"system\", \"You\\'re an assistant who\\'s good at\n",
      "{ability}\"),\\n            MessagesPlaceholder(variable_name=\"history\"),\\n            (\"human\",\n",
      "\"{question}\"),\\n        ])\\n\\n        chain = prompt | ChatAnthropic(model=\"claude-2\")\\n\\n\n",
      "with_message_history = RunnableWithMessageHistory(\\n            chain,\\n\n",
      "get_session_history=get_session_history,\\n            input_messages_key=\"question\",\\n\n",
      "history_messages_key=\"history\",\\n            history_factory_config=[\\n\n",
      "ConfigurableFieldSpec(\\n                    id=\"user_id\",\\n                    annotation=str,\\n\n",
      "name=\"User ID\",\\n                    description=\"Unique identifier for the user.\",\\n\n",
      "default=\"\",\\n                    is_shared=True,\\n                ),\\n\n",
      "ConfigurableFieldSpec(\\n                    id=\"conversation_id\",\\n\n",
      "annotation=str,\\n                    name=\"Conversation ID\",\\n\n",
      "description=\"Unique identifier for the conversation.\",\\n                    default=\"\",\\n\n",
      "is_shared=True,\\n                ),\\n            ],\\n        )\\n\\n\n",
      "with_message_history.invoke(\\n            {\"ability\": \"math\", \"question\": \"What does cosine\n",
      "mean?\"},\\n            config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\\n\n",
      ")', 'type': 'object', 'properties': {'name': {'title': 'Name', 'type': 'string'}, 'bound': {'title':\n",
      "'Bound', 'allOf': [{'type': 'array', 'items': [{}, {}]}]}, 'kwargs': {'title': 'Kwargs', 'type':\n",
      "'object'}, 'config': {'$ref': '#/definitions/RunnableConfig'}, 'custom_input_type': {'title':\n",
      "'Custom Input Type'}, 'custom_output_type': {'title': 'Custom Output Type'}, 'input_messages_key':\n",
      "{'title': 'Input Messages Key', 'type': 'string'}, 'output_messages_key': {'title': 'Output Messages\n",
      "Key', 'type': 'string'}, 'history_messages_key': {'title': 'History Messages Key', 'type':\n",
      "'string'}, 'history_factory_config': {'title': 'History Factory Config', 'type': 'array', 'items':\n",
      "{'type': 'array', 'items': [{'title': 'Id', 'type': 'string'}, {'title': 'Annotation'}, {'title':\n",
      "'Name', 'type': 'string'}, {'title': 'Description', 'type': 'string'}, {'title': 'Default'},\n",
      "{'title': 'Is Shared', 'type': 'boolean'}, {'title': 'Dependencies', 'type': 'array', 'items':\n",
      "{'type': 'string'}}], 'minItems': 7, 'maxItems': 7}}}, 'required': ['bound',\n",
      "'history_factory_config'], 'definitions': {'RunnableConfig': {'title': 'RunnableConfig', 'type':\n",
      "'object', 'properties': {'tags': {'title': 'Tags', 'type': 'array', 'items': {'type': 'string'}},\n",
      "'metadata': {'title': 'Metadata', 'type': 'object'}, 'callbacks': {'title': 'Callbacks', 'anyOf':\n",
      "[{'type': 'array', 'items': {}}, {}]}, 'run_name': {'title': 'Run Name', 'type': 'string'},\n",
      "'max_concurrency': {'title': 'Max Concurrency', 'type': 'integer'}, 'recursion_limit': {'title':\n",
      "'Recursion Limit', 'type': 'integer'}, 'configurable': {'title': 'Configurable', 'type': 'object'},\n",
      "'run_id': {'title': 'Run Id', 'type': 'string', 'format': 'uuid'}}}}}\n",
      "\n",
      "OUPUT_SCHEMA::__root__=None\n",
      "\n",
      "\n",
      " Now we run The example below shows how to use a different chat history for each session.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "chain = prompt | chatbedrock_llm | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print_ww(wrapped_chain.invoke({\"input\": \"what is the weather like in Seattle WA?\"}))\n",
    "\n",
    "\n",
    "print_ww(f\"\\nINPUT_SCHEMA::{wrapped_chain.input_schema.schema()}\")\n",
    "print_ww(f\"\\nCHAIN:SCHEMA::{wrapped_chain.schema()}\")\n",
    "print_ww(f\"\\nOUPUT_SCHEMA::{wrapped_chain.output_schema()}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n Now we run The example below shows how to use a different chat history for each session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: what is the weather like in Seattle WA?\n",
      "AI: \n",
      "\n",
      "Arrr, shiver me timbers! Seattle, ye say? Well, matey, I've had me share o' adventures on the high seas, but I've never set foot in that damp and drizzly place. But I've heard tell from me mateys who've sailed those waters that Seattle's weather be as unpredictable as a barnacle on a ship's hull!\n",
      "\n",
      "From what I've gathered, Seattle's got a reputation for bein' a soggy place, with rain comin' down like a stormy sea on most days o' the year. The clouds be gray and thick, like a pirate's beard after a long voyage at sea. And don't even get me started on the wind, matey! It be as fierce as a sea monster, blowin' in from the Pacific and makin' ye want to tie yerself to the mast!\n",
      "\n",
      "But, I've also heard that when the sun does come out, it be as bright as a chest overflowin' with gold doubloons! So, if ye be lookin' for a bit o' sunshine, ye might want to keep yer eye on the forecast, matey!\n",
      "\n",
      "So, there ye have it, me take on the weather in Seattle, WA. Now, if ye'll excuse me, I've got to get back to me ship and me trusty parrot, Polly. We've got a date with the high seas, and I don't want to be late!\n"
     ]
    }
   ],
   "source": [
    "print(history)\n",
    "# history.add_ai_message\n",
    "# history.add_user_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the multiple session id's with in memory conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I be more familiar with the high seas than the landlubbers'\n",
      "weather forecasts. But, I've heard tell of Seattle, Washington bein' a damp and drizzly place,\n",
      "especially in the winter months. They call it the \"Emerald City\" due to its lush greenery, but I\n",
      "reckon it's more like the \"Grey City\" with all the overcast skies!\n",
      "\n",
      "In the summer, the weather be mild and pleasant, with temperatures rangein' from 65 to 85 degrees\n",
      "Fahrenheit (18 to 30 degrees Celsius). But don't ye be thinkin' it's all sunshine and rainbows,\n",
      "matey! The Pacific Northwest be known for its rain, and Seattle gets its fair share o'\n",
      "precipitation, even in the summer. So, pack yer waterproof gear and a good sense o' humor!\n",
      "\n",
      "In the winter, it be a different story altogether. The temperatures drop, and the rain turns to snow\n",
      "and ice. It be a good idea to keep yer wits about ye and yer sea legs steady, or ye might find\n",
      "yerself walkin' the plank into a puddle o' slush!\n",
      "\n",
      "So, there ye have it, me hearty! That be the weather in Seattle, Washington, as seen through the\n",
      "eyes o' a pirate. Now, if ye'll excuse me, I have to go find me a nice, warm spot to dry me boots!\n",
      "\n",
      "\n",
      " now ask another question and we will see the History conversation was maintained\n",
      "\n",
      "\n",
      "Shiver me timbers! The weather in Seattle may be grey and drizzly, but it has its advantages, matey!\n",
      "The mild temperatures and abundant rainfall make for lush greenery and vibrant flora, perfect for a\n",
      "pirate's hideout. The overcast skies also reduce the risk o' scurvy, keepin' me and me crew healthy\n",
      "and strong. And let's not forget the misty mornings, which add a touch o' mystery to our\n",
      "swashbucklin' adventures! Plus, the rain keeps the landlubbers indoors, makin' it easier for us\n",
      "pirates to sneak about and plunder their booty! Arrr!\n",
      "\n",
      "\n",
      " now check the history\n",
      "Human: what is the weather like in Seattle WA?\n",
      "AI: \n",
      "\n",
      "Arrr, shiver me timbers! Seattle, ye say? Well, matey, I've had me share o' adventures on the high seas, but I've never set foot in that damp and drizzly place. But I've heard tell from me mateys who've sailed those waters that Seattle's weather be as unpredictable as a barnacle on a ship's hull!\n",
      "\n",
      "From what I've gathered, Seattle's got a reputation for bein' a soggy place, with rain comin' down like a stormy sea on most days o' the year. The clouds be gray and thick, like a pirate's beard after a long voyage at sea. And don't even get me started on the wind, matey! It be as fierce as a sea monster, blowin' in from the Pacific and makin' ye want to tie yerself to the mast!\n",
      "\n",
      "But, I've also heard that when the sun does come out, it be as bright as a chest overflowin' with gold doubloons! So, if ye be lookin' for a bit o' sunshine, ye might want to keep yer eye on the forecast, matey!\n",
      "\n",
      "So, there ye have it, me take on the weather in Seattle, WA. Now, if ye'll excuse me, I've got to get back to me ship and me trusty parrot, Polly. We've got a date with the high seas, and I don't want to be late!\n"
     ]
    }
   ],
   "source": [
    "### This below LEVARAGES the In-memory with multiple sessions and session id\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    #print(session_id)\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain = prompt | chatbedrock_llm | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print_ww(wrapped_chain.invoke(\n",
    "    {\"input\": \"what is the weather like in Seattle WA\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "))\n",
    "\n",
    "print(\"\\n\\n now ask another question and we will see the History conversation was maintained\")\n",
    "print_ww(wrapped_chain.invoke(\n",
    "    {\"input\": \"Ok what are benefits of this weather in 100 words?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "))\n",
    "\n",
    "print(\"\\n\\n now check the history\")\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we do a Conversation Chat Chain with History and add a Retriever to that convo\n",
    "\n",
    "\n",
    "[Docs links]('https://python.langchain.com/v0.2/docs/versions/migrating_chains/conversation_retrieval_chain/')\n",
    "\n",
    "**Chat History needs to be a list since this is message api so alternate with human and user**\n",
    "\n",
    "1. The ConversationalRetrievalChain was an all-in one way that combined retrieval-augmented generation with chat history, allowing you to \"chat with\" your documents.\n",
    "\n",
    "2. Advantages of switching to the LCEL implementation are similar to the RetrievalQA section above:\n",
    "\n",
    "3. Clearer internals. The ConversationalRetrievalChain chain hides an entire question rephrasing step which dereferences the initial query against the chat history.\n",
    "4. This means the class contains two sets of configurable prompts, LLMs, etc.\n",
    "5. More easily return source documents.\n",
    "6. Support for runnable methods like streaming and async operations.\n",
    "\n",
    "**Below are the key classes to be used**\n",
    "\n",
    "1. We create a QA Chain using the qa_chain as `create_stuff_documents_chain(chatbedrock_llm, qa_prompt)`\n",
    "2. Then we create the Retrieval History chain using the `create_retrieval_chain(history_aware_retriever, qa_chain)`\n",
    "3. Retriever is wrapped in as `create_history_aware_retriever`\n",
    "4. `{context}` goes as System prompts which goes into the Prompt templates\n",
    "5. `Chat History` goes in the Prompt templates like \"placeholder\", \"{chat_history}\")\n",
    "\n",
    "The LCEL implementation exposes the internals of what's happening around retrieving, formatting documents, and passing them through a prompt to the LLM, but it is more verbose. You can customize and wrap this composition logic in a helper function, or use the higher-level `create_retrieval_chain` and `create_stuff_documents_chain` helper method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS as VectorStore\n",
    "\n",
    "In order to be able to use embeddings for search, we need a store that can efficiently perform vector similarity searches. In this notebook we use FAISS, which is an in memory store. For permanently store vectors, one can use pgVector, Pinecone or Chroma.\n",
    "\n",
    "The langchain VectorStore API's are available [here](https://python.langchain.com/en/harrison-docs-refactor-3-24/reference/modules/vectorstore.html)\n",
    "\n",
    "To know more about the FAISS vector store please refer to this [document](https://arxiv.org/pdf/1702.08734.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titan embeddings Model\n",
    "\n",
    "Embeddings are a way to represent words, phrases or any other discrete items as vectors in a continuous vector space. This allows machine learning models to perform mathematical operations on these representations and capture semantic relationships between them.\n",
    "\n",
    "Embeddings are for example used for the RAG [document search capability](https://labelbox.com/blog/how-vector-similarity-search-works/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garygrewal/virtualenv/trainenv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `BedrockEmbeddings` was deprecated in LangChain 0.2.11 and will be removed in 0.4.0. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import BedrockEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv to rag_data/Amazon_SageMaker_FAQs.csv\n",
      "Number of documents=153\n",
      "Number of documents after split and chunking=154\n",
      "vectorstore_faiss_aws: number of elements in the index=154::\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "\n",
    "s3_path = \"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv\"\n",
    "!aws s3 cp $s3_path ./rag_data/Amazon_SageMaker_FAQs.csv\n",
    "\n",
    "loader = CSVLoader(\"./rag_data/Amazon_SageMaker_FAQs.csv\") # --- > 219 docs with 400 chars, each row consists in a question column and an answer column\n",
    "documents_aws = loader.load() #\n",
    "print(f\"Number of documents={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Number of documents after split and chunking={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "\n",
    "    \n",
    "vectorstore_faiss_aws = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "     embedding = br_embeddings\n",
    ")\n",
    "\n",
    "print(f\"vectorstore_faiss_aws: number of elements in the index={vectorstore_faiss_aws.index.ntotal}::\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we do the simple Retrieval QA chain -- No chat history but with retriver\n",
    "[Docs link]('https://python.langchain.com/v0.2/docs/versions/migrating_chains/retrieval_qa/')\n",
    "\n",
    "Key points\n",
    "1. The chain in QA uses the variable as the first value, can be input or question  and so the prompt template for the Human query has to have the `Question` or `input` as the variable\n",
    "2. This chain will re formulate the question, call the retriver and then answer the question\n",
    "3. Our prompt template removes any answer where retriver is not needed and so no answer is obtained\n",
    "4. Context goes into the system prompts section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\"), HumanMessage(content='test_input')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ").invoke({'input': 'test_input', 'chat_history' : chat_history_messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'BedrockEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x119a1ddd0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_faiss_aws.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The retriever invoke is called with the user input \n",
    "\n",
    "1. That will fetch the context and then add that as a string to the inputs \n",
    "2. The chain will use that as `context` based on the variable in the chain so we have the correct context\n",
    "3. This same process could have been done with the memory as well if we wanted to send a string as input\n",
    "\n",
    "The input is a string because we convert it to a dict as the very first step on the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I don't have enough context to answer this question. The provided context only mentions autonomous\n",
      "vehicles and HVAC as examples of applications where reinforcement learning can be used, but it does\n",
      "not define what autonomous agents are.\n",
      "\n",
      "\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine\n",
      "learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"), # expected by the qa chain as it sends in question as the variable\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    #print(docs)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#- second way to create a callback runnable function--\n",
    "def debug_inputs(input_dict: dict, config: RunnableConfig) -> dict:\n",
    "    #print_ww(f\"debug_inputs::input_dict:{type(input_dict)}::value::{input_dict}::config={config}\") #- if we do dict at start of chain -- {'input': {'input': 'what is the weather like in Seattle WA?', 'chat_history':\n",
    "    return input_dict # return the text as is\n",
    "\n",
    "chat_user_debug = RunnableLambda(debug_inputs)\n",
    "\n",
    "# The chain \n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vectorstore_faiss_aws.as_retriever() | format_docs, # can work even without the format\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | chat_user_debug\n",
    "    | condense_question_prompt\n",
    "    | chatbedrock_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print_ww(qa_chain.invoke(input=\"What are autonomous agents?\")) # cannot be a dict object here because we create the dict from string as first step\n",
    "\n",
    "print_ww(qa_chain.invoke(input=\"What is SageMaker used for?\")) # cannot be a dict object here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate way of creating the Chain with retriever and ask a valid question - No History of chat \n",
    "\n",
    "1. Now we get a real answer as we invoke where retriever gives context\n",
    "\n",
    "2. Use the Helper method to create the Retiever QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', 'config', 'context', 'answer'])\n",
      "\n",
      " return values\n",
      "\n",
      "{'input': 'What are the options for model explainability in SageMaker?', 'config': {'configurable':\n",
      "{'session_id': 'abc123'}}, 'context': [Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 11}, page_content='\\ufeffWhat is Amazon SageMaker?:\n",
      "How does Amazon SageMaker Clarify improve model explainability?\\nAmazon SageMaker is a fully managed\n",
      "service to prepare data and build, train, and deploy machine learning (ML) models for any use case\n",
      "with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Clarify is integrated\n",
      "with Amazon SageMaker Experiments to provide a feature importance graph detailing the importance of\n",
      "each input for your model’s overall decision-making process after the model has been trained. These\n",
      "details can help determine if a particular model input has more influence than it should on overall\n",
      "model behavior. SageMaker Clarify also makes explanations for individual predictions available via\n",
      "an API.'), Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 19},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: What does Amazon SageMaker Model Dashboard\n",
      "do?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy\n",
      "machine learning (ML) models for any use case with fully managed infrastructure, tools, and\n",
      "workflows.: Amazon SageMaker Model Dashboard gives you a comprehensive overview of deployed models\n",
      "and endpoints, letting you track resources and model behavior violations through one pane. It allows\n",
      "you to monitor model behavior in four dimensions, including data and model quality, and bias and\n",
      "feature attribution drift through its integration with Amazon SageMaker Model Monitor and Amazon\n",
      "SageMaker Clarify. SageMaker Model Dashboard also provides an integrated experience to set up and\n",
      "receive alerts for missing and inactive model monitoring jobs, and deviations in model behavior for\n",
      "model quality, data quality, bias drift, and feature attribution drift. You can further inspect\n",
      "individual models and analyze factors impacting model performance over time. Then, you can follow up\n",
      "with ML practitioners to take corrective measures.'), Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9}, page_content='\\ufeffWhat is Amazon SageMaker?:\n",
      "How can I check for imbalances in my model?\\nAmazon SageMaker is a fully managed service to prepare\n",
      "data and build, train, and deploy machine learning (ML) models for any use case with fully managed\n",
      "infrastructure, tools, and workflows.: Amazon SageMaker Clarify\\xa0helps improve model transparency\n",
      "by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances\n",
      "during data preparation, after training, and ongoing over time, and also includes tools to help\n",
      "explain ML models and their predictions. Findings can be shared through explainability reports.'),\n",
      "Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 118},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: Why should I use\\xa0Amazon SageMaker Inference\n",
      "Recommender?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and\n",
      "deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and\n",
      "workflows.: You should use SageMaker Inference Recommender if you need recommendations for the right\n",
      "endpoint configuration to improve performance and reduce costs. Previously, data scientists who\n",
      "wanted to deploy their models had to run manual benchmarks to select the right endpoint\n",
      "configuration. They had to first select the right ML instance type out of the 70+ available instance\n",
      "types based on the resource requirements of their models and sample payloads, and then optimize the\n",
      "model to account for differing hardware. Then, they had to conduct extensive load tests to validate\n",
      "that latency and throughput requirements are met and that the costs are low. SageMaker Inference\n",
      "Recommender eliminates this complexity by making it easy for you to: 1) get started in minutes with\n",
      "an instance recommendation; 2) conduct load tests across instance types to get recommendations on\n",
      "your endpoint configuration within hours; and 3) automatically tune container and model server\n",
      "parameters as well as perform model optimizations for a given instance type.')], 'answer':\n",
      "\"\\n\\nAccording to the provided context, Amazon SageMaker Clarify provides model explainability\n",
      "by:\\n\\n* Providing a feature importance graph detailing the importance of each input for your\n",
      "model's overall decision-making process\\n* Making explanations for individual predictions available\n",
      "via an API\\n* Detecting statistical bias across the entire ML workflow\\n* Checking for imbalances\n",
      "during data preparation, after training, and ongoing over time\\n* Including tools to help explain ML\n",
      "models and their predictions\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "qa_chain = create_stuff_documents_chain(chatbedrock_llm, condense_question_prompt)\n",
    "\n",
    "convo_qa_chain = create_retrieval_chain(vectorstore_faiss_aws.as_retriever(), qa_chain)\n",
    "\n",
    "# - view the keys\n",
    "\n",
    "print_ww(convo_qa_chain.invoke(\n",
    "    {'input':\"What are the options for model explainability in SageMaker?\", \n",
    "      'config':{\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "    }).keys()) # cannot be a dict object here)\n",
    "\n",
    "# view the actual output\n",
    "print(\"\\n return values\\n\")\n",
    "print_ww(convo_qa_chain.invoke(\n",
    "    {'input':\"What are the options for model explainability in SageMaker?\", \n",
    "      'config':{\"configurable\": {\"session_id\": \"abc123\"}}, # this param is not used in this chain\n",
    "    })) # cannot be a dict object here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'BedrockEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x119a1ddd0>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"\\n    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\\n    If the answer is not in the context below , just say you do not have enough context. \\n    If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise.\\n    Context: {context} \\n    \")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "            | ChatBedrock(client=<botocore.client.BedrockRuntime object at 0x1141585d0>, model_id='meta.llama3-8b-instruct-v1:0', model_kwargs={'temperature': 0.0, 'top_p': 0.5, 'max_tokens_to_sample': 2000}, beta_use_converse_api=True)\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we create Chat Conversation which has history and retrieval context - First just history chain and  with advanced option of re writing the context and query\n",
    "So we use the HISTORY AWARE Retriever and create a chain\n",
    "\n",
    "1. We create a stuff chain\n",
    "2. Then we pass it to the create retrieval chain method -- we could have used the LCEL as well to create the chain\n",
    "3. If we need advanced history calling with advanced options of first check if the question has been answered before using an LLM call then use `create_history_aware_retriever`\n",
    "\n",
    "**However to create the actual history we need to wrap with RunnableWithHistory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bound=RunnableAssign(mapper={\n",
      "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not\n",
      "x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
      "           | VectorStoreRetriever(tags=['FAISS', 'BedrockEmbeddings'],\n",
      "vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x119a1ddd0>))],\n",
      "default=ChatPromptTemplate(input_variables=['input'],\n",
      "messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a\n",
      "chat history and the latest user question which might reference context in the chat history,\n",
      "formulate a standalone question which can be understood without the chat history. Do NOT answer the\n",
      "question, just reformulate it if needed and otherwise return it as is.')),\n",
      "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
      "           | ChatBedrock(client=<botocore.client.BedrockRuntime object at 0x1141585d0>,\n",
      "model_id='meta.llama3-8b-instruct-v1:0', model_kwargs={'temperature': 0.0, 'top_p': 0.5,\n",
      "'max_tokens_to_sample': 2000}, beta_use_converse_api=True)\n",
      "           | StrOutputParser()\n",
      "           | VectorStoreRetriever(tags=['FAISS', 'BedrockEmbeddings'],\n",
      "vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x119a1ddd0>)),\n",
      "config={'run_name': 'retrieve_documents'})\n",
      "})\n",
      "| RunnableAssign(mapper={\n",
      "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "              context: RunnableLambda(format_docs)\n",
      "            }), config={'run_name': 'format_inputs'})\n",
      "            | ChatPromptTemplate(input_variables=['context', 'input'],\n",
      "optional_variables=['chat_history'], input_types={'chat_history':\n",
      "typing.List[typing.Union[langchain_core.messages.ai.AIMessage,\n",
      "langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage,\n",
      "langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage,\n",
      "langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []},\n",
      "messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'],\n",
      "template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved\n",
      "context to answer the question. If you don't know the answer, say that you don't know. Use three\n",
      "sentences maximum and keep the answer concise.\\n\\n{context}\")),\n",
      "MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
      "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='Explain this\n",
      "{input}.'))])\n",
      "            | ChatBedrock(client=<botocore.client.BedrockRuntime object at 0x1141585d0>,\n",
      "model_id='meta.llama3-8b-instruct-v1:0', model_kwargs={'temperature': 0.0, 'top_p': 0.5,\n",
      "'max_tokens_to_sample': 2000}, beta_use_converse_api=True)\n",
      "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
      "  }) config={'run_name': 'retrieval_chain'}::\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the options for model explainability in SageMaker?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 11}, page_content='\\ufeffWhat is Amazon SageMaker?: How does Amazon SageMaker Clarify improve model explainability?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Clarify is integrated with Amazon SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available via an API.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 19}, page_content='\\ufeffWhat is Amazon SageMaker?: What does Amazon SageMaker Model Dashboard do?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Model Dashboard gives you a comprehensive overview of deployed models and endpoints, letting you track resources and model behavior violations through one pane. It allows you to monitor model behavior in four dimensions, including data and model quality, and bias and feature attribution drift through its integration with Amazon SageMaker Model Monitor and Amazon SageMaker Clarify. SageMaker Model Dashboard also provides an integrated experience to set up and receive alerts for missing and inactive model monitoring jobs, and deviations in model behavior for model quality, data quality, bias drift, and feature attribution drift. You can further inspect individual models and analyze factors impacting model performance over time. Then, you can follow up with ML practitioners to take corrective measures.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9}, page_content='\\ufeffWhat is Amazon SageMaker?: How can I check for imbalances in my model?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Clarify\\xa0helps improve model transparency by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time, and also includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 118}, page_content='\\ufeffWhat is Amazon SageMaker?: Why should I use\\xa0Amazon SageMaker Inference Recommender?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: You should use SageMaker Inference Recommender if you need recommendations for the right endpoint configuration to improve performance and reduce costs. Previously, data scientists who wanted to deploy their models had to run manual benchmarks to select the right endpoint configuration. They had to first select the right ML instance type out of the 70+ available instance types based on the resource requirements of their models and sample payloads, and then optimize the model to account for differing hardware. Then, they had to conduct extensive load tests to validate that latency and throughput requirements are met and that the costs are low. SageMaker Inference Recommender eliminates this complexity by making it easy for you to: 1) get started in minutes with an instance recommendation; 2) conduct load tests across instance types to get recommendations on your endpoint configuration within hours; and 3) automatically tune container and model server parameters as well as perform model optimizations for a given instance type.')],\n",
       " 'answer': \"\\n\\nAmazon SageMaker provides model explainability through Amazon SageMaker Clarify, which offers feature importance graphs and individual prediction explanations. These features help determine the importance of each input for a model's overall decision-making process and provide insights into individual predictions.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "### This below LEVARAGES the In-memory with multiple sessions and session id\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    #print(session_id)\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "contextualized_question_system_template = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualized_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualized_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chatbedrock_llm, vectorstore_faiss_aws.as_retriever(), contextualized_question_prompt\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"Explain this  {input}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(chatbedrock_llm, qa_prompt)\n",
    "\n",
    "convo_qa_chain = create_retrieval_chain(\n",
    "    history_aware_retriever, \n",
    "    #vectorstore_faiss_aws.as_retriever(),\n",
    "    qa_chain\n",
    ")\n",
    "\n",
    "print_ww(f\"\\n{convo_qa_chain}::\\n\")\n",
    "\n",
    "convo_qa_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are the options for model explainability in SageMaker?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto add the history to the Chat with Retriever\n",
    "\n",
    "Wrap with Runnable Chat History with Session id and run the chat conversation\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/context_aware_history_retriever.png)\n",
    "\n",
    "borrowed from https://github.com/langchain-ai/langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "### This below LEVARAGES the In-memory with multiple sessions and session id\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    #print(session_id)\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "contextualized_question_system_template = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualized_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualized_question_system_template),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#- we will not ue this below\n",
    "# history_aware_retriever = create_history_aware_retriever(\n",
    "#     chatbedrock_llm, vectorstore_faiss_aws.as_retriever(), contextualized_question_prompt\n",
    "# )\n",
    "\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If the answer is not present in the context, just say you do not have enough context to answer. \\\n",
    "If the input is not present in the context, just say you do not have enough context to answer. \\\n",
    "If the question is not present in the context, just say you do not have enough context to answer. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "question_answer_chain = create_stuff_documents_chain(chatbedrock_llm, qa_prompt)\n",
    "\n",
    "#rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) # - this works but adds a call to the LLM for context \n",
    "rag_chain = create_retrieval_chain(vectorstore_faiss_aws.as_retriever(), question_answer_chain) # - this works but adds a call to the LLM for context \n",
    "\n",
    "#- Wrap the rag_chain with RunnableWithMessageHistory to automatically handle chat history:\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What kind of bias can SageMaker detect?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 10}, page_content=\"\\ufeffWhat is Amazon SageMaker?: What kind of bias does Amazon SageMaker Clarify detect?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You need to choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9}, page_content='\\ufeffWhat is Amazon SageMaker?: How can I check for imbalances in my model?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Clarify\\xa0helps improve model transparency by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time, and also includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 118}, page_content='\\ufeffWhat is Amazon SageMaker?: Why should I use\\xa0Amazon SageMaker Inference Recommender?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: You should use SageMaker Inference Recommender if you need recommendations for the right endpoint configuration to improve performance and reduce costs. Previously, data scientists who wanted to deploy their models had to run manual benchmarks to select the right endpoint configuration. They had to first select the right ML instance type out of the 70+ available instance types based on the resource requirements of their models and sample payloads, and then optimize the model to account for differing hardware. Then, they had to conduct extensive load tests to validate that latency and throughput requirements are met and that the costs are low. SageMaker Inference Recommender eliminates this complexity by making it easy for you to: 1) get started in minutes with an instance recommendation; 2) conduct load tests across instance types to get recommendations on your endpoint configuration within hours; and 3) automatically tune container and model server parameters as well as perform model optimizations for a given instance type.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 117}, page_content='\\ufeffWhat is Amazon SageMaker?: What is Amazon SageMaker Inference Recommender?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Inference Recommender\\xa0is a new capability of Amazon SageMaker that reduces the time required to get ML models in production by automating performance benchmarking and tuning model performance across SageMaker ML instances. You can now use SageMaker Inference Recommender to deploy your model to an endpoint that delivers the best performance and minimizes cost. You can get started with SageMaker Inference Recommender in minutes while selecting an instance type and get recommendations for optimal endpoint configurations within hours, eliminating weeks of manual testing and tuning time. With SageMaker Inference Recommender, you pay only for the SageMaker ML instances used during load testing, and there are no additional charges.')],\n",
       " 'answer': '\\n\\nAmazon SageMaker Clarify detects statistical bias across the entire ML workflow, including imbalances during data preparation, after training, and ongoing over time.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain_with_history.invoke(\n",
    "    {\"input\": \"What kind of bias can SageMaker detect?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a follow on question\n",
    "\n",
    "1. The phrase `it` will be converted based on the chat history\n",
    "2. Retriever gets invoked to get relevant content based on chat history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What are common ways of implementing this?', 'chat_history': [HumanMessage(content='What\n",
      "kind of bias can SageMaker detect?'), AIMessage(content='\\n\\nAmazon SageMaker Clarify detects\n",
      "statistical bias across the entire ML workflow, including imbalances during data preparation, after\n",
      "training, and ongoing over time.')], 'context': [Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 105}, page_content='\\ufeffWhat is Amazon SageMaker?:\n",
      "When should I use reinforcement learning?\\nAmazon SageMaker is a fully managed service to prepare\n",
      "data and build, train, and deploy machine learning (ML) models for any use case with fully managed\n",
      "infrastructure, tools, and workflows.: While the goal of supervised learning techniques is to find\n",
      "the right answer based on the patterns in the training data, the goal of unsupervised learning\n",
      "techniques is to find similarities and differences between data points. In contrast, the goal of\n",
      "reinforcement learning (RL) techniques is to learn how to achieve a desired outcome even when it is\n",
      "not clear how to accomplish that outcome. As a result, RL is more suited to enabling intelligent\n",
      "applications where an agent can make autonomous decisions such as robotics, autonomous vehicles,\n",
      "HVAC, industrial control, and more.'), Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 50}, page_content='\\ufeffWhat is Amazon SageMaker?:\n",
      "How can I reproduce a feature from a given moment in time?\\nAmazon SageMaker is a fully managed\n",
      "service to prepare data and build, train, and deploy machine learning (ML) models for any use case\n",
      "with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Feature Store maintains\n",
      "time stamps for all features at every instance of time. This helps you retrieve features at any\n",
      "period of time for business or compliance requirements. You can easily explain model features and\n",
      "their values from when they were first created to the present time by reproducing the model from a\n",
      "given moment in time.'), Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row':\n",
      "107}, page_content='\\ufeffWhat is Amazon SageMaker?: Do I need to write my own RL agent algorithms\n",
      "to train RL models?\\nAmazon SageMaker is a fully managed service to prepare data and build, train,\n",
      "and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools,\n",
      "and workflows.: No, Amazon SageMaker RL includes RL toolkits such as Coach and Ray RLLib that offer\n",
      "implementations of RL agent algorithms such as DQN, PPO, A3C, and many more.'),\n",
      "Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 25},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: What solutions come pre-built with Amazon SageMaker\n",
      "JumpStart?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy\n",
      "machine learning (ML) models for any use case with fully managed infrastructure, tools, and\n",
      "workflows.: SageMaker JumpStart includes solutions that are preconfigured with all necessary AWS\n",
      "services to launch a solution into production. Solutions are fully customizable so you can easily\n",
      "modify them to fit your specific use case and dataset. You can use solutions for over 15 use cases\n",
      "including demand forecasting, fraud detection, and predictive maintenance, and readily deploy\n",
      "solutions with just a few clicks. For more information about all solutions available, visit the\n",
      "SageMaker\\xa0getting started page.')], 'answer': \"\\n\\nI don't have enough context to answer. The\n",
      "provided context does not mention specific ways of implementing bias detection in Amazon\n",
      "SageMaker.\"}\n"
     ]
    }
   ],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"What are common ways of implementing this?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "print_ww(follow_up_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Will it help?', 'chat_history': [HumanMessage(content='What kind of bias can SageMaker\n",
      "detect?'), AIMessage(content='\\n\\nAmazon SageMaker Clarify detects statistical bias across the\n",
      "entire ML workflow, including imbalances during data preparation, after training, and ongoing over\n",
      "time.'), HumanMessage(content='What are common ways of implementing this?'),\n",
      "AIMessage(content=\"\\n\\nI don't have enough context to answer. The provided context does not mention\n",
      "specific ways of implementing bias detection in Amazon SageMaker.\")], 'context':\n",
      "[Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 101},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: How do I decide to use Amazon SageMaker Autopilot or\n",
      "Automatic Model Tuning?\\nAmazon SageMaker is a fully managed service to prepare data and build,\n",
      "train, and deploy machine learning (ML) models for any use case with fully managed infrastructure,\n",
      "tools, and workflows.: Amazon SageMaker Autopilot automates everything in a typical ML workflow,\n",
      "including feature preprocessing, algorithm selection, and hyperparameter tuning, while specifically\n",
      "focusing on classification and regression use cases. Automatic Model Tuning, on the other hand, is\n",
      "designed to tune any model, no matter whether it is based on built-in algorithms, deep learning\n",
      "frameworks, or custom containers. In exchange for the flexibility, you have to manually pick the\n",
      "specific algorithm, hyperparameters to tune, and corresponding search ranges.'),\n",
      "Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 105},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: When should I use reinforcement learning?\\nAmazon\n",
      "SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning\n",
      "(ML) models for any use case with fully managed infrastructure, tools, and workflows.: While the\n",
      "goal of supervised learning techniques is to find the right answer based on the patterns in the\n",
      "training data, the goal of unsupervised learning techniques is to find similarities and differences\n",
      "between data points. In contrast, the goal of reinforcement learning (RL) techniques is to learn how\n",
      "to achieve a desired outcome even when it is not clear how to accomplish that outcome. As a result,\n",
      "RL is more suited to enabling intelligent applications where an agent can make autonomous decisions\n",
      "such as robotics, autonomous vehicles, HVAC, industrial control, and more.'),\n",
      "Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: How can I check for imbalances in my model?\\nAmazon\n",
      "SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning\n",
      "(ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon\n",
      "SageMaker Clarify\\xa0helps improve model transparency by detecting statistical bias across the\n",
      "entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training,\n",
      "and ongoing over time, and also includes tools to help explain ML models and their predictions.\n",
      "Findings can be shared through explainability reports.'), Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 10}, page_content=\"\\ufeffWhat is Amazon SageMaker?:\n",
      "What kind of bias does Amazon SageMaker Clarify detect?\\nAmazon SageMaker is a fully managed service\n",
      "to prepare data and build, train, and deploy machine learning (ML) models for any use case with\n",
      "fully managed infrastructure, tools, and workflows.: Measuring bias in ML models is a first step to\n",
      "mitigating bias. Bias may be measured before training and after training, as well as for inference\n",
      "for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even\n",
      "considering simple notions of fairness leads to many different measures applicable in various\n",
      "contexts. You need to choose bias notions and metrics that are valid for the application and the\n",
      "situation under investigation. SageMaker currently supports the computation of different bias\n",
      "metrics for training data (as part of SageMaker data preparation), for the trained model (as part of\n",
      "Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker\n",
      "Model Monitor). For example, before training, we provide metrics for checking whether the training\n",
      "data is representative (that is, whether one group is underrepresented) and whether there are\n",
      "differences in the label distribution across groups. After training or during deployment, metrics\n",
      "can be helpful to measure whether (and by how much) the performance of the model differs across\n",
      "groups. For example, start by comparing the error rates (how likely a model's prediction is to\n",
      "differ from the true label) or break further down into precision (how likely a positive prediction\n",
      "is to be correct) and recall (how likely the model will correctly label a positive example).\")],\n",
      "'answer': \"\\n\\nI don't have enough context to answer. The provided context does not mention the\n",
      "specific situation or problem you are trying to solve, so it's unclear whether Amazon SageMaker\n",
      "Clarify will help or not.\"}\n"
     ]
    }
   ],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"Will it help?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "print_ww(follow_up_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now ask a random question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"Give me a few tips on how to plant a  new garden.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "follow_up_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the semantic search works:\n",
    "1. First we calculate the embeddings vector for the query, and\n",
    "2. then we use this vector to do a similarity search on the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = br_embeddings.embed_query(\"R in SageMaker\")\n",
    "print(v[0:10])\n",
    "results = vectorstore_faiss_aws.similarity_search_by_vector(v, k=4)\n",
    "for r in results:\n",
    "    print_ww(r.page_content)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory\n",
    "In any chatbot we will need a QA Chain with various options which are customized by the use case. But in a chatbot we will always need to keep the history of the conversation so the model can take it into consideration to provide the answer. In this example we use the [ConversationalRetrievalChain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db) from LangChain, together with a ConversationBufferMemory to keep the history of the conversation.\n",
    "\n",
    "Source: https://python.langchain.com/docs/modules/chains/popular/chat_vector_db\n",
    "\n",
    "Set `verbose` to `True` to see all the what is going on behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "print_ww(CONDENSE_QUESTION_PROMPT.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters used for ConversationRetrievalChain\n",
    "* **retriever**: We used `VectorStoreRetriever`, which is backed by a `VectorStore`. To retrieve text, there are two search types you can choose: `\"similarity\"` or `\"mmr\"`. `search_type=\"similarity\"` uses similarity search in the retriever object where it selects text chunk vectors that are most similar to the question vector.\n",
    "\n",
    "* **memory**: Memory Chain to store the history \n",
    "\n",
    "* **condense_question_prompt**: Given a question from the user, we use the previous conversation and that question to make up a standalone question\n",
    "\n",
    "* **chain_type**: If the chat history is long and doesn't fit the context you use this parameter and the options are `stuff`, `refine`, `map_reduce`, `map-rerank`\n",
    "\n",
    "If the question asked is outside the scope of context, then the model will reply it doesn't know the answer\n",
    "\n",
    "**Note**: if you are curious how the chain works, uncomment the `verbose=True` line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do some prompt engineering\n",
    "\n",
    "You can \"tune\" your prompt to get more or less verbose answers. For example, try to change the number of sentences, or remove that instruction all-together. You might also need to change the number of `max_tokens` (eg 1000 or 2000) to get the full answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this demo we used Claude V3 sonnet LLM to create conversational interface with following patterns:\n",
    "\n",
    "1. Chatbot (Basic - without context)\n",
    "\n",
    "2. Chatbot using prompt template(Langchain)\n",
    "\n",
    "3. Chatbot with personas\n",
    "\n",
    "4. Chatbot with context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "trainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
