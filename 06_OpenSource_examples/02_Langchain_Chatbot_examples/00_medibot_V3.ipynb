{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Interface - Medical Clinic\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "In this notebook, we will build a chatbot using the Foundation Models (FMs) in Amazon Bedrock. For our use-case we use Claude V3 Sonnet as our foundation models.  For more details refer to [Documentation](https://aws.amazon.com/bedrock/claude/). The ideal balance between intelligence and speed—particularly for enterprise workloads. It excels at complex reasoning, nuanced content creation, scientific queries, math, and coding. Data teams can use Sonnet for RAG, as well as search and retrieval across vast amounts of information while sales teams can leverage Sonnet for product recommendations, forecasting, and targeted marketing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers.Chatbots uses natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. They can be accessed through various channels such as websites, social media platforms, and messaging apps.\n",
    "\n",
    "\n",
    "## Chatbot using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/chatbot_bedrock.png)\n",
    "\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "1. **Chatbot (Basic)** - Zero Shot chatbot with a FM model\n",
    "2. **Chatbot using prompt** - template(Langchain) - Chatbot with some context provided in the prompt template\n",
    "3. **Chatbot with persona** - Chatbot with defined roles. i.e. Career Coach and Human interactions\n",
    "4. **Contextual-aware chatbot** - Passing in context through an external file by generating embeddings.\n",
    "\n",
    "## Langchain framework for building Chatbot with Amazon Bedrock\n",
    "In Conversational interfaces such as chatbots, it is highly important to remember previous interactions, both at a short term but also at a long term level.\n",
    "\n",
    "LangChain provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular and useful regardless of how they are used. Secondly, LangChain provides easy ways to incorporate these utilities into chains.\n",
    "It allows us to easily define and interact with different types of abstractions, which make it easy to build powerful chatbots.\n",
    "\n",
    "## Building Chatbot with Context - Key Elements\n",
    "\n",
    "The first process in a building a contextual-aware chatbot is to **generate embeddings** for the context. Typically, you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using Titan Embeddings model for this\n",
    "\n",
    "![Embeddings](./images/embeddings_lang.png)\n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "![Chatbot](./images/chatbot_lang.png)\n",
    "\n",
    "## Architecture [Context Aware Chatbot]\n",
    "![4](./images/context-aware-chatbot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've run the [Bedrock boto3 setup notebook](../00_Prerequisites/bedrock_basics.ipynb) notebook. ⚠️ ⚠️ ⚠️ Then run these installs below\n",
    "\n",
    "**please note**\n",
    "\n",
    "for we are tracking an annoying warning when using the RunnableWithMessageHistory [Runnable History Issue]('https://github.com/langchain-ai/langchain-aws/issues/150'). Please ignore the warning mesages for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-community==0.2.12\n",
    "# %pip install -U --no-cache-dir  \\\n",
    "#     \"langchain>=0.2.12\" \\\n",
    "#     sqlalchemy -U \\\n",
    "#     \"faiss-cpu>=1.7,<2\" \\\n",
    "#     \"pypdf>=3.8,<4\" \\\n",
    "#     pinecone-client>=5.0.1 \\\n",
    "#     tiktoken>=0.7.0 \\\n",
    "#     \"ipywidgets>=7,<8\" \\\n",
    "#     matplotlib>=3.9.0 \\\n",
    "#     anthropic>=0.32.0 \\\n",
    "#     \"langchain-aws>=0.1.15\"\n",
    "# - boto3-1.34.162 botocore-1.34.162 langchain-0.2.14 langchain-aws-0.1.17 langchain-core-0.2.34 langchain-community-0.2.12\n",
    "#%pip install -U --no-cache-dir transformers\n",
    "#%pip install -U --no-cache-dir boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_bedrock_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if runtime:\n",
    "        service_name='bedrock-runtime'\n",
    "    else:\n",
    "        service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region='us-west-2' #os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "models_list = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region='us-west-2', #os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=False\n",
    ").list_foundation_models()\n",
    "\n",
    "#[models['modelId'] for models in models_list['modelSummaries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3.Session().client(\"s3\").list_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot (Basic - without context)\n",
    "\n",
    "We use [CoversationChain](https://python.langchain.com/en/latest/modules/models/llms/integrations/bedrock.html?highlight=ConversationChain#using-in-a-conversation-chain) from LangChain to start the conversation. We also use the [ConversationBufferMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer.html) for storing the messages. We can also get the history as a list of messages (this is very useful in a chat model).\n",
    "\n",
    "Chatbots needs to remember the previous interactions. Conversational memory allows us to do that. There are several ways that we can implement conversational memory. In the context of LangChain, they are all built on top of the ConversationChain.\n",
    "\n",
    "**Note:** The model outputs are non-deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In classical physics, energy is thought of as a continuous spectrum, meaning that it can take on any value within a certain range. However, in quantum mechanics, energy is quantized, meaning that it comes in discrete packets or \"quanta\".\n",
      "\n",
      "Think of it like a piano keyboard. In classical physics, the sound of a piano is like a continuous wave of sound that can take on any pitch or frequency. But in quantum mechanics, the sound of a piano is like a series of discrete notes\n",
      "--- Latency: 1425ms - Input tokens:58 - Output tokens:100 ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nQuantum mechanics is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, such as atoms and subatomic particles. It provides a new and different framework for understanding physical phenomena, and it has been incredibly successful in explaining a wide range of experimental results.\\n\\nAt the heart of quantum mechanics is the concept of wave-particle duality. In classical physics, particles such as electrons and photons were thought to have definite positions and trajectories. However, in quantum mechanics, these'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "modelId = 'meta.llama3-8b-instruct-v1:0'\n",
    "\n",
    "messages_list=[\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': \"What is quantum mechanics? \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'assistant', \n",
    "        \"content\":[{\n",
    "            'text': \"It is a branch of physics that describes how matter and energy interact with discrete energy values \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': \"Can you explain a bit more about discrete energies?\"\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "\n",
    "    \n",
    "response = boto3_bedrock.converse(\n",
    "    messages=messages_list, \n",
    "    modelId='meta.llama3-8b-instruct-v1:0',\n",
    "    inferenceConfig={\n",
    "        \"temperature\": 0.5,\n",
    "        \"maxTokens\": 100,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    ")\n",
    "response_body = response['output']['message']['content'][0]['text'] \\\n",
    "        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
    "        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
    "        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n'\n",
    "\n",
    "print(response_body)\n",
    "\n",
    "\n",
    "def invoke_meta_converse(prompt_str,boto3_bedrock ):\n",
    "    modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "    messages_list=[{ \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': prompt_str\n",
    "        }]\n",
    "    }]\n",
    "  \n",
    "    response = boto3_bedrock.converse(\n",
    "        messages=messages_list, \n",
    "        modelId=modelId,\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0.5,\n",
    "            \"maxTokens\": 100,\n",
    "            \"topP\": 0.9\n",
    "        }\n",
    "    )\n",
    "    response_body = response['output']['message']['content'][0]['text']\n",
    "    return response_body\n",
    "\n",
    "\n",
    "invoke_meta_converse(\"what is quantum mechanics\", boto3_bedrock)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to ChatBedrock\n",
    "\n",
    "**Supports the following**\n",
    "1. Multiple Models from Bedrock \n",
    "2. Converse API\n",
    "3. Ability to do tool binding\n",
    "4. Ability to plug with LangGraph flows\n",
    "\n",
    "### Ask the question Meta Llama models\n",
    "\n",
    "**please make sure you have the models enabled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nSeattle, Washington is known for its mild and wet climate, with significant rainfall throughout the year. Here's a breakdown of the typical weather patterns in Seattle:\\n\\n1. Rainfall: Seattle is famous for its rain, with an average annual rainfall of around 37 inches (94 cm). The rainiest months are November to March, with an average of 15-20 rainy days per month.\\n2. Temperature: Seattle's average temperature ranges from 35°F (2°C) in January (the coldest month) to 77°F (25°C) in July (the warmest month). The average temperature is around 50°F (10°C) throughout the year.\\n3. Sunshine: Seattle gets an average of 154 sunny days per year, with the sunniest months being July and August. However, the sun can be obscured by clouds and fog, reducing the amount of direct sunlight.\\n4. Fog: Seattle is known for its fog, especially during the winter months. The city can experience fog for several days at a time, especially in the mornings.\\n5. Wind: Seattle is known for its strong winds, especially during the winter months. The city can experience gusts of up to 40 mph (64 km/h) during storms.\\n6. Snow: Seattle rarely sees significant snowfall, with an average annual snowfall of around 6 inches (15 cm). The snowiest month is usually January, with an average of 1-2 inches (2.5-5 cm) of snow.\\n7. Summer: Seattle's summer months (June to August) are mild and pleasant, with average highs in the mid-70s to low 80s (23-27°C). However, the city can experience occasional heatwaves, with temperatures reaching up to 90°F (32°C) or more.\\n8. Winter: Seattle's winter months (December to February) are cool and wet, with average lows in the mid-30s to low 40s (2-6°C). The city can experience occasional cold snaps, with temperatures dropping below 20°F (-7°C) for short periods.\\n\\nOverall, Seattle's weather is characterized by mild temperatures, significant rainfall, and overcast skies. It's essential to pack layers and waterproof clothing when visiting the city, especially during the winter months.\", response_metadata={'ResponseMetadata': {'RequestId': '8e5351d3-0210-44be-94f1-9fae38a9e639', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 05:05:20 GMT', 'content-type': 'application/json', 'content-length': '2211', 'connection': 'keep-alive', 'x-amzn-requestid': '8e5351d3-0210-44be-94f1-9fae38a9e639'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 6161}}, id='run-88dfa1cd-c003-4d95-8ac8-0dc28cf7b31f-0', usage_metadata={'input_tokens': 22, 'output_tokens': 472, 'total_tokens': 494})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 200}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in Seattle WA\"\n",
    "    )\n",
    "]\n",
    "bedrock_llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to the converse api flag -- this class corectly formulates the messages correctly\n",
    "\n",
    "so we can directly use the string mesages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nSeattle, Washington is known for its mild and wet climate, with significant rainfall throughout the year. Here's a breakdown of the typical weather patterns in Seattle:\\n\\n1. Rainfall: Seattle is famous for its rain, with an average annual rainfall of around 37 inches (94 cm). The rainiest months are November to March, with an average of 15-20 rainy days per month.\\n2. Temperature: Seattle's average temperature ranges from 35°F (2°C) in January (the coldest month) to 77°F (25°C) in July (the warmest month). The average temperature is around 50°F (10°C) throughout the year.\\n3. Sunshine: Seattle gets an average of 154 sunny days per year, with the sunniest months being July and August. However, the sun can be obscured by clouds and fog, reducing the amount of direct sunlight.\\n4. Fog: Seattle is known for its fog, especially during the winter months. The city can experience fog for several days at a time, especially in the mornings.\\n5. Wind: Seattle is known for its strong winds, especially during the winter months. The city can experience gusts of up to 40 mph (64 km/h) during storms.\\n6. Snow: Seattle rarely sees significant snowfall, with an average annual snowfall of around 6 inches (15 cm). The snowiest month is usually January, with an average of 1-2 inches (2.5-5 cm) of snow.\\n7. Seasonal changes:\\n\\t* Spring (March to May): Mild temperatures, with average highs in the mid-50s to low 60s (13°C-18°C). Rainfall is still common, but the days are getting longer.\\n\\t* Summer (June to August): Warmest months, with average highs in the mid-70s to low 80s (23°C-27°C). The days are long, with up to 16 hours of daylight.\\n\\t* Autumn (September to November): Mild temperatures, with average highs in the mid-50s to low 60s (13°C-18°C). Rainfall increases, and the days get shorter.\\n\\t* Winter (December to February): Coldest months, with average lows in the mid-30s to low 40s (2°C-6°C). Rainfall is common, and the days are short, with as few as 8 hours of daylight.\\n\\nKeep in mind that these are general weather patterns, and actual conditions can vary from year to year. It's always a good idea to check current weather forecasts and conditions before planning your trip to Seattle.\", response_metadata={'ResponseMetadata': {'RequestId': 'd4b03c9f-ca04-4e31-88d0-f850aa5edeaa', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 05:05:28 GMT', 'content-type': 'application/json', 'content-length': '2413', 'connection': 'keep-alive', 'x-amzn-requestid': 'd4b03c9f-ca04-4e31-88d0-f850aa5edeaa'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 7086}}, id='run-edb1177d-a78e-4ecd-9771-304eb6191171-0', usage_metadata={'input_tokens': 23, 'output_tokens': 543, 'total_tokens': 566})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_llm.invoke(\"what is the weather like in Seattle WA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask a follow on\n",
    "\n",
    "because we have not plugged in any History or context or api's the model wil not be able to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n\\nThe warmth of summers depends on the location and climate. In general, summer is the warmest season in many parts of the world, especially near the equator.\\n\\nIn tropical regions, such as near the equator, summers are often extremely hot and humid. Temperatures can soar above 90°F (32°C) and even reach as high as 100°F (38°C) or more in some areas.\\n\\nIn temperate regions, such as in the Northern Hemisphere, summers are usually warm but not as hot as in tropical regions. Temperatures can range from the mid-70s to the mid-80s Fahrenheit (23-30°C).\\n\\nIn some regions, such as in the Southern Hemisphere, summers can be quite mild, especially in areas with a Mediterranean climate. Temperatures may range from the mid-60s to the mid-70s Fahrenheit (18-24°C).\\n\\nSome examples of warm summer temperatures in different parts of the world include:\\n\\n* In the United States, temperatures in the summer can range from 80°F (27°C) in the Northeast to 100°F (38°C) in the Southwest.\\n* In Europe, temperatures in the summer can range from 65°F (18°C) in the UK to 90°F (32°C) in Spain and Italy.\\n* In Australia, temperatures in the summer can range from 75°F (24°C) in the southeast to 95°F (35°C) in the northwest.\\n* In Africa, temperatures in the summer can range from 80°F (27°C) in the north to 100°F (38°C) in the south.\\n\\nOverall, the warmth of summers can vary greatly depending on the location and climate.', response_metadata={'ResponseMetadata': {'RequestId': '207d2e02-9ca6-4718-933f-0245ebd574b0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 05:05:33 GMT', 'content-type': 'application/json', 'content-length': '1626', 'connection': 'keep-alive', 'x-amzn-requestid': '207d2e02-9ca6-4718-933f-0245ebd574b0'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 4453}}, id='run-e1b607ba-a6f5-4e47-8ecb-4e7d1f04b379-0', usage_metadata={'input_tokens': 20, 'output_tokens': 346, 'total_tokens': 366})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_llm.invoke(\"is it warm in summers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nSeattle, Washington is known for its mild and wet climate, with significant rainfall throughout the year. Here's a breakdown of the typical weather patterns in Seattle:\\n\\n1. Rainfall: Seattle is famous for its rain, with an average annual rainfall of around 37 inches (94 cm). The rainiest months are November to March, with an average of 15-20 rainy days per month.\\n2. Temperature: Seattle's average temperature ranges from 35°F (2°C) in January (the coldest month) to 77°F (25°C) in July (the warmest month). The average temperature is around 50°F (10°C) throughout the year.\\n3. Sunshine: Seattle gets an average of 154 sunny days per year, with the sunniest months being July and August. However, the sun can be obscured by clouds and fog, reducing the amount of direct sunlight.\\n4. Fog: Seattle is known for its fog, especially during the winter months. The city can experience fog for several days at a time, especially in the mornings.\\n5. Wind: Seattle is known for its strong winds, especially during the winter months. The city can experience gusts of up to 40 mph (64 km/h) during storms.\\n6. Snow: Seattle rarely sees significant snowfall, with an average annual snowfall of around 6 inches (15 cm). The snowiest month is usually January, with an average of 1-2 inches (2.5-5 cm) of snow.\\n7. Summer: Seattle's summer months (June to August) are mild and pleasant, with average highs in the mid-70s to mid-80s (23-30°C). However, the city can experience occasional heatwaves, with temperatures reaching up to 90°F (32°C) or more.\\n8. Winter: Seattle's winter months (December to February) are cool and wet, with average lows in the mid-30s to mid-40s (2-7°C). The city can experience occasional cold snaps, with temperatures dropping below 20°F (-7°C) for short periods.\\n\\nOverall, Seattle's weather is characterized by mild temperatures, significant rainfall, and overcast skies. It's essential to pack layers and waterproof clothing when visiting the city, especially during the winter months.\", response_metadata={'ResponseMetadata': {'RequestId': '424b2e68-af9c-46ea-a6a3-dcfda65cadec', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 05:06:15 GMT', 'content-type': 'application/json', 'content-length': '2211', 'connection': 'keep-alive', 'x-amzn-requestid': '424b2e68-af9c-46ea-a6a3-dcfda65cadec'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 6074}}, id='run-325c4eaa-52fe-477e-9fad-6b940f7e2c65-0', usage_metadata={'input_tokens': 22, 'output_tokens': 472, 'total_tokens': 494})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in Seattle WA\"\n",
    "    )\n",
    "]\n",
    "bedrock_llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding prompt templates \n",
    "\n",
    "1. You can define prompts as a list of messages, all modesl expect SystemMessage, and then alternate with HumanMessage and AIMessage\n",
    "2. This means Context needs to be part of the System message \n",
    "3. Further the CHAT HISTORY needs to be right after the system message as a MessagePlaceholder which is a list of alternating [Human/AI]\n",
    "4. The Variables defined in the chat template need to be send into the chain as dict with the keys being the variable names\n",
    "5. You can define the template as a tuple with (\"system\", \"message\") or can be using the class SystemMessage \n",
    "6. Invoke creates a final resulting object of type <class 'langchain_core.prompt_values.ChatPromptValue'> with the variables substituted with their values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you\n",
      "can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy\n",
      "matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in\n",
      "Seattle.\"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"\\n    You are an assistant for question-answering tasks. ONLY Use\n",
      "the following pieces of retrieved context to answer the question.\\n    If the answer is not in the\n",
      "context below , just say you do not have enough context. \\n    If you don't know the answer, just\n",
      "say that you don't know. \\n    Use three sentences maximum and keep the answer concise.\\n\n",
      "Context: this is a test context \\n    \"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"You are an assistant for question-answering tasks. Use the\n",
      "following pieces of retrieved context to answer the question. If you don't know the answer, say that\n",
      "you don't know. Use three sentences maximum and keep the answer concise.\\n\\nthis is a test\n",
      "context\"), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy\n",
      "matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in\n",
      "Seattle.\"), HumanMessage(content='Explain this  test_input.')]\n",
      "\n",
      "\n",
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat_history_messages = [\n",
    "        HumanMessage(\"What is the weather like in Seattle WA?\"), # - normal string converts it to a Human message always but we need ai/human pairs\n",
    "        AIMessage(\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages( # can create either as System Message Object or as TUPLE -- system, message\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"), # this assumes the messages are in list of messages format and this becomes MessagePlaceholder object\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- variable chat_history should be a list of base messages, got test_chat_history of type <class 'str'>\n",
    "#- this gets converted as a LIST of messages -- with each of the TUPLE or Object being executed with the variables when invoked\n",
    "print_ww(prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages}))\n",
    "\n",
    "# -- condense question prompt with CONTEXT\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- missing variables {'context'}. chat history will get ignored - variables are passed in as keys in the dict\n",
    "print(\"\\n\")\n",
    "print_ww(condense_question_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))\n",
    "\n",
    "# - Chat prompt template with Place holders\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{contex}\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"Explain this  {input}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print_ww(qa_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(type(qa_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\"), HumanMessage(content='test_input')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ").invoke({'input': 'test_input', 'chat_history' : chat_history_messages})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Conversation chain \n",
    "\n",
    "**Uses the In memory Chat Message History**\n",
    "\n",
    "The above example uses the same history for all sessions. The example below shows how to use a different chat history for each session.\n",
    "\n",
    "**Note**\n",
    "1. `Chat History` is a variable is a place holder in the prompt template. which will have Human/Ai alternative messages\n",
    "2. Human query is the final question as `Input` variable\n",
    "3. config is the `{\"configurable\": {'session_id_variable':'value,....other keys}` These are passed into the any and all Runnable and wrappers of runnable\n",
    "4. `RunnableWithMessageHistory` is the class which we wrap the `chain` in to run with history. which is in [Docs link]('https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#')\n",
    "5. For production use cases, you will want to use a persistent implementation of chat message history, such as `RedisChatMessageHistory`.\n",
    "6. This class needs a DICT as a input\n",
    "7. chain has .input_schema.schema to get the json of how to pass in the input\n",
    "\n",
    "8. Configuration gets passed in as invoke({dict}, config={\"configurable\": {\"session_id\": \"abc123\"}}) and it gets converted to `RunnableConfig` which is passed into every invoke method. To access this we need to extend the Runnable class and access it\n",
    "9. The chain usually processes the inputs as a dict object\n",
    "\n",
    "\n",
    "Wrap the rag_chain with RunnableWithMessageHistory to automatically handle chat history:\n",
    "\n",
    "Any Chain wrapped with RunnableWithMessageHistory - will manage chat history variables appropriately, however the ChatTemplate should have the Placeholder for history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the same manually by configuring the chain with the chat history being Added and invoked automatically\n",
    "\n",
    "if we configue the chain manually not necessary all variables have to be invluded in the inputs. If those are being used or accessed then it will provide those\n",
    "\n",
    "1. For runnable we can either extend the runnable class\n",
    "2. Or we can define a method and create a runnable lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatUserAdd::input_dict:{'input': 'what is the weather like in Seattle WA?', 'chat_history':\n",
      "[]}::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks':\n",
      "<langchain_core.callbacks.manager.CallbackManager object at 0x113eda350>, 'recursion_limit': 25,\n",
      "'configurable': {'session_id': 'abc123'}}\n",
      "ChatHistoryAdd::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks':\n",
      "<langchain_core.callbacks.manager.CallbackManager object at 0x116b3bd10>, 'recursion_limit': 25,\n",
      "'configurable': {'session_id': 'abc123'}}::history_object=Human: what is the weather like in Seattle\n",
      "WA?::input=content=\"\\n\\nArrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven\n",
      "seas, but I've never set foot in Seattle, Washington. But I've heard tales o' the Emerald City's\n",
      "weather from me mateys who've sailed those waters.\\n\\nFrom what I've gathered, Seattle's weather be\n",
      "as unpredictable as a barnacle on a ship's hull. It's known for bein' rainy and gray, with overcast\n",
      "skies most o' the time. The city gets a fair amount o' precipitation, with an average o' 226 days o'\n",
      "rain per year! That be a lot o' wet weather, matey!\\n\\nBut don't ye worry, there be some sunshine to\n",
      "be had, too. The summer months o' June, July, and August be the driest, with an average o' 15-20\n",
      "days o' sunshine. And in the winter, the days be shorter, but the sun still shines bright, even if\n",
      "it be through the clouds.\\n\\nSo, if ye be plannin' a trip to Seattle, be prepared for a mix o' rain\n",
      "and shine. And don't ferget yer waterproof boots and a good umbrella, or ye might be walkin' the\n",
      "plank!\" response_metadata={'ResponseMetadata': {'RequestId': 'f2a563a3-a551-444d-ba96-d9745d147301',\n",
      "'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 05:08:25 GMT', 'content-type':\n",
      "'application/json', 'content-length': '1163', 'connection': 'keep-alive', 'x-amzn-requestid':\n",
      "'f2a563a3-a551-444d-ba96-d9745d147301'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics':\n",
      "{'latencyMs': 3648}} id='run-6ed82f10-ad91-4ca1-9acc-6e271c969540-0' usage_metadata={'input_tokens':\n",
      "46, 'output_tokens': 260, 'total_tokens': 306}::\n",
      "\n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've never\n",
      "set foot in Seattle, Washington. But I've heard tales o' the Emerald City's weather from me mateys\n",
      "who've sailed those waters.\n",
      "\n",
      "From what I've gathered, Seattle's weather be as unpredictable as a barnacle on a ship's hull. It's\n",
      "known for bein' rainy and gray, with overcast skies most o' the time. The city gets a fair amount o'\n",
      "precipitation, with an average o' 226 days o' rain per year! That be a lot o' wet weather, matey!\n",
      "\n",
      "But don't ye worry, there be some sunshine to be had, too. The summer months o' June, July, and\n",
      "August be the driest, with an average o' 15-20 days o' sunshine. And in the winter, the days be\n",
      "shorter, but the sun still shines bright, even if it be through the clouds.\n",
      "\n",
      "So, if ye be plannin' a trip to Seattle, be prepared for a mix o' rain and shine. And don't ferget\n",
      "yer waterproof boots and a good umbrella, or ye might be walkin' the plank!\n",
      "\n",
      "\n",
      " chat_history after invocation is -- >Human: what is the weather like in Seattle WA?\n",
      "AI: \n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've never set foot in Seattle, Washington. But I've heard tales o' the Emerald City's weather from me mateys who've sailed those waters.\n",
      "\n",
      "From what I've gathered, Seattle's weather be as unpredictable as a barnacle on a ship's hull. It's known for bein' rainy and gray, with overcast skies most o' the time. The city gets a fair amount o' precipitation, with an average o' 226 days o' rain per year! That be a lot o' wet weather, matey!\n",
      "\n",
      "But don't ye worry, there be some sunshine to be had, too. The summer months o' June, July, and August be the driest, with an average o' 15-20 days o' sunshine. And in the winter, the days be shorter, but the sun still shines bright, even if it be through the clouds.\n",
      "\n",
      "So, if ye be plannin' a trip to Seattle, be prepared for a mix o' rain and shine. And don't ferget yer waterproof boots and a good umbrella, or ye might be walkin' the plank!\n",
      "ChatUserAdd::input_dict:{'input': 'How is it in winters?', 'chat_history':\n",
      "[HumanMessage(content='what is the weather like in Seattle WA?'), AIMessage(content=\"\\n\\nArrr,\n",
      "shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've never set foot\n",
      "in Seattle, Washington. But I've heard tales o' the Emerald City's weather from me mateys who've\n",
      "sailed those waters.\\n\\nFrom what I've gathered, Seattle's weather be as unpredictable as a barnacle\n",
      "on a ship's hull. It's known for bein' rainy and gray, with overcast skies most o' the time. The\n",
      "city gets a fair amount o' precipitation, with an average o' 226 days o' rain per year! That be a\n",
      "lot o' wet weather, matey!\\n\\nBut don't ye worry, there be some sunshine to be had, too. The summer\n",
      "months o' June, July, and August be the driest, with an average o' 15-20 days o' sunshine. And in\n",
      "the winter, the days be shorter, but the sun still shines bright, even if it be through the\n",
      "clouds.\\n\\nSo, if ye be plannin' a trip to Seattle, be prepared for a mix o' rain and shine. And\n",
      "don't ferget yer waterproof boots and a good umbrella, or ye might be walkin' the\n",
      "plank!\")]}::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks':\n",
      "<langchain_core.callbacks.manager.CallbackManager object at 0x116b935d0>, 'recursion_limit': 25,\n",
      "'configurable': {'session_id': 'abc123'}}\n",
      "ChatHistoryAdd::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks':\n",
      "<langchain_core.callbacks.manager.CallbackManager object at 0x113ede990>, 'recursion_limit': 25,\n",
      "'configurable': {'session_id': 'abc123'}}::history_object=Human: what is the weather like in Seattle\n",
      "WA?\n",
      "AI:\n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've never\n",
      "set foot in Seattle, Washington. But I've heard tales o' the Emerald City's weather from me mateys\n",
      "who've sailed those waters.\n",
      "\n",
      "From what I've gathered, Seattle's weather be as unpredictable as a barnacle on a ship's hull. It's\n",
      "known for bein' rainy and gray, with overcast skies most o' the time. The city gets a fair amount o'\n",
      "precipitation, with an average o' 226 days o' rain per year! That be a lot o' wet weather, matey!\n",
      "\n",
      "But don't ye worry, there be some sunshine to be had, too. The summer months o' June, July, and\n",
      "August be the driest, with an average o' 15-20 days o' sunshine. And in the winter, the days be\n",
      "shorter, but the sun still shines bright, even if it be through the clouds.\n",
      "\n",
      "So, if ye be plannin' a trip to Seattle, be prepared for a mix o' rain and shine. And don't ferget\n",
      "yer waterproof boots and a good umbrella, or ye might be walkin' the plank!\n",
      "Human: How is it in winters?::input=content=\"\\n\\nWinters in Seattle, ye say? Well, matey, it be a\n",
      "different tale altogether! Winter in Seattle be a chilly and wet affair, with the Pacific\n",
      "Northwest's famous rain comin' back in full force.\\n\\nFrom December to February, the average\n",
      "temperature be around 40°F (4°C), with lows often droppin' to around 35°F (2°C) or even colder. And\n",
      "don't be surprised if ye see snowflakes fallin' from the sky, matey! Seattle gets an average o' 6-8\n",
      "inches (15-20 cm) o' snow per year, with most o' it fallin' in January and February.\\n\\nBut even\n",
      "with the cold and wet weather, there be some advantages to winter in Seattle. The city's famous\n",
      "coffee culture be in full swing, with cozy cafes and coffee shops to warm ye up on a chilly day. And\n",
      "the holiday season be a magical time in Seattle, with festive lights and decorations adornin' the\n",
      "streets and buildings.\\n\\nJust be prepared for the rain, matey! It be a good idea to pack waterproof\n",
      "gear, including a sturdy umbrella and waterproof boots. And if ye be plannin' on explorin' the great\n",
      "outdoors, be sure to bundle up warm and bring a waterproof jacket to keep ye dry.\\n\\nSo, if ye be\n",
      "lookin' for a winter adventure in Seattle, just remember to pack yer rain gear and a sense o'\n",
      "adventure, and ye'll be ready to take on the Emerald City's winter weather!\"\n",
      "response_metadata={'ResponseMetadata': {'RequestId': '75be1006-85a1-4529-9199-7a844fdc9357',\n",
      "'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 05:08:30 GMT', 'content-type':\n",
      "'application/json', 'content-length': '1486', 'connection': 'keep-alive', 'x-amzn-requestid':\n",
      "'75be1006-85a1-4529-9199-7a844fdc9357'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics':\n",
      "{'latencyMs': 4341}} id='run-859ae201-f942-4d3c-92f9-f2f44437be95-0' usage_metadata={'input_tokens':\n",
      "317, 'output_tokens': 313, 'total_tokens': 630}::\n",
      "\n",
      "\n",
      "Winters in Seattle, ye say? Well, matey, it be a different tale altogether! Winter in Seattle be a\n",
      "chilly and wet affair, with the Pacific Northwest's famous rain comin' back in full force.\n",
      "\n",
      "From December to February, the average temperature be around 40°F (4°C), with lows often droppin' to\n",
      "around 35°F (2°C) or even colder. And don't be surprised if ye see snowflakes fallin' from the sky,\n",
      "matey! Seattle gets an average o' 6-8 inches (15-20 cm) o' snow per year, with most o' it fallin' in\n",
      "January and February.\n",
      "\n",
      "But even with the cold and wet weather, there be some advantages to winter in Seattle. The city's\n",
      "famous coffee culture be in full swing, with cozy cafes and coffee shops to warm ye up on a chilly\n",
      "day. And the holiday season be a magical time in Seattle, with festive lights and decorations\n",
      "adornin' the streets and buildings.\n",
      "\n",
      "Just be prepared for the rain, matey! It be a good idea to pack waterproof gear, including a sturdy\n",
      "umbrella and waterproof boots. And if ye be plannin' on explorin' the great outdoors, be sure to\n",
      "bundle up warm and bring a waterproof jacket to keep ye dry.\n",
      "\n",
      "So, if ye be lookin' for a winter adventure in Seattle, just remember to pack yer rain gear and a\n",
      "sense o' adventure, and ye'll be ready to take on the Emerald City's winter weather!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "prompt_with_history = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "# - add the history to the in-memory chat history\n",
    "class ChatHistoryAdd(Runnable):\n",
    "    def __init__(self, chat_history):\n",
    "        self.chat_history = chat_history\n",
    "\n",
    "    def invoke(self, input: str, config: RunnableConfig = None) -> str:\n",
    "        try:\n",
    "            print_ww(f\"ChatHistoryAdd::config={config}::history_object={self.chat_history}::input={input}::\")\n",
    "            \n",
    "            self.chat_history.add_ai_message(input.content)\n",
    "            return input\n",
    "        except Exception as e:\n",
    "            return f\"Error processing input: {str(e)}\"\n",
    "\n",
    "# Usage\n",
    "chat_add = ChatHistoryAdd(get_history())\n",
    "\n",
    "#- second way to create a callback runnable function--\n",
    "def ChatUserInputAdd(input_dict: dict, config: RunnableConfig) -> dict:\n",
    "    print_ww(f\"ChatUserAdd::input_dict:{input_dict}::config={config}\") #- if we do dict at start of chain -- {'input': {'input': 'what is the weather like in Seattle WA?', 'chat_history':\n",
    "    get_history().add_user_message(input_dict['input']) \n",
    "    return input_dict # return the text as is\n",
    "\n",
    "chat_user_add = RunnableLambda(ChatUserInputAdd)\n",
    "\n",
    "\n",
    "history_chain = (\n",
    "    #- Expected a Runnable, callable or dict. If we use a dict here make sure every element is a runnable. And further access is via 'input'.'input'\n",
    "    # { # make sure all variable in the prompt template are in this dict\n",
    "    #     \"input\": RunnablePassthrough(),\n",
    "    #     \"chat_history\": get_history().messages\n",
    "    # }\n",
    "    RunnablePassthrough() # passes in the full dict as is -- since we have the variables defined in the INVOKE call itself\n",
    "    | chat_user_add\n",
    "    | prompt_with_history\n",
    "    | chatbedrock_llm\n",
    "    | chat_add\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "print_ww(history_chain.invoke( # here the variable matches the chat prompt template\n",
    "    {\"input\": \"what is the weather like in Seattle WA?\", \"chat_history\": get_history().messages}, \n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    ")\n",
    "\n",
    "print(f\"\\n\\n chat_history after invocation is -- >{get_history()}\")\n",
    "\n",
    "#- ask a follow on question\n",
    "print_ww(history_chain.invoke(\n",
    "    {\"input\": \"How is it in winters?\", \"chat_history\": get_history().messages}, \n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate way of invoking \n",
    "\n",
    "1. Here  only use input is sent in as a string\n",
    "2. The chain tales care of the History of chats addition to the whole prompt\n",
    "3. We create a new Chain -- `but we are re-using the same History Object` and hence it has the previous conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history::input_dict:what is it like in autumn?::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x11211ec10>, 'recursion_limit': 25, 'configurable': {'session_id': 'abc123'}}\n",
      "ChatUserAdd::input_dict:{'input': 'what is it like in autumn?', 'chat_history':\n",
      "[HumanMessage(content='what is the weather like in Seattle WA?'), AIMessage(content=\"\\n\\nArrr,\n",
      "shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've never set foot\n",
      "in Seattle, Washington. But I've heard tales o' the Emerald City's weather from me mateys who've\n",
      "sailed those waters.\\n\\nFrom what I've gathered, Seattle's weather be as unpredictable as a barnacle\n",
      "on a ship's hull. It's known for bein' rainy and gray, with overcast skies most o' the time. The\n",
      "city gets a fair amount o' precipitation, with an average o' 226 days o' rain per year! That be a\n",
      "lot o' wet weather, matey!\\n\\nBut don't ye worry, there be some sunshine to be had, too. The summer\n",
      "months o' June, July, and August be the driest, with an average o' 15-20 days o' sunshine. And in\n",
      "the winter, the days be shorter, but the sun still shines bright, even if it be through the\n",
      "clouds.\\n\\nSo, if ye be plannin' a trip to Seattle, be prepared for a mix o' rain and shine. And\n",
      "don't ferget yer waterproof boots and a good umbrella, or ye might be walkin' the plank!\"),\n",
      "HumanMessage(content='How is it in winters?'), AIMessage(content=\"\\n\\nWinters in Seattle, ye say?\n",
      "Well, matey, it be a different tale altogether! Winter in Seattle be a chilly and wet affair, with\n",
      "the Pacific Northwest's famous rain comin' back in full force.\\n\\nFrom December to February, the\n",
      "average temperature be around 40°F (4°C), with lows often droppin' to around 35°F (2°C) or even\n",
      "colder. And don't be surprised if ye see snowflakes fallin' from the sky, matey! Seattle gets an\n",
      "average o' 6-8 inches (15-20 cm) o' snow per year, with most o' it fallin' in January and\n",
      "February.\\n\\nBut even with the cold and wet weather, there be some advantages to winter in Seattle.\n",
      "The city's famous coffee culture be in full swing, with cozy cafes and coffee shops to warm ye up on\n",
      "a chilly day. And the holiday season be a magical time in Seattle, with festive lights and\n",
      "decorations adornin' the streets and buildings.\\n\\nJust be prepared for the rain, matey! It be a\n",
      "good idea to pack waterproof gear, including a sturdy umbrella and waterproof boots. And if ye be\n",
      "plannin' on explorin' the great outdoors, be sure to bundle up warm and bring a waterproof jacket to\n",
      "keep ye dry.\\n\\nSo, if ye be lookin' for a winter adventure in Seattle, just remember to pack yer\n",
      "rain gear and a sense o' adventure, and ye'll be ready to take on the Emerald City's winter\n",
      "weather!\")]}::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks':\n",
      "<langchain_core.callbacks.manager.CallbackManager object at 0x116bed550>, 'recursion_limit': 25,\n",
      "'configurable': {'session_id': 'abc123'}}\n",
      "ChatHistoryAdd::config={'tags': [], 'metadata': {'session_id': 'abc123'}, 'callbacks':\n",
      "<langchain_core.callbacks.manager.CallbackManager object at 0x116a7add0>, 'recursion_limit': 25,\n",
      "'configurable': {'session_id': 'abc123'}}::history_object=Human: what is the weather like in Seattle\n",
      "WA?\n",
      "AI:\n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I've had me share o' sailin' the seven seas, but I've never\n",
      "set foot in Seattle, Washington. But I've heard tales o' the Emerald City's weather from me mateys\n",
      "who've sailed those waters.\n",
      "\n",
      "From what I've gathered, Seattle's weather be as unpredictable as a barnacle on a ship's hull. It's\n",
      "known for bein' rainy and gray, with overcast skies most o' the time. The city gets a fair amount o'\n",
      "precipitation, with an average o' 226 days o' rain per year! That be a lot o' wet weather, matey!\n",
      "\n",
      "But don't ye worry, there be some sunshine to be had, too. The summer months o' June, July, and\n",
      "August be the driest, with an average o' 15-20 days o' sunshine. And in the winter, the days be\n",
      "shorter, but the sun still shines bright, even if it be through the clouds.\n",
      "\n",
      "So, if ye be plannin' a trip to Seattle, be prepared for a mix o' rain and shine. And don't ferget\n",
      "yer waterproof boots and a good umbrella, or ye might be walkin' the plank!\n",
      "Human: How is it in winters?\n",
      "AI:\n",
      "\n",
      "Winters in Seattle, ye say? Well, matey, it be a different tale altogether! Winter in Seattle be a\n",
      "chilly and wet affair, with the Pacific Northwest's famous rain comin' back in full force.\n",
      "\n",
      "From December to February, the average temperature be around 40°F (4°C), with lows often droppin' to\n",
      "around 35°F (2°C) or even colder. And don't be surprised if ye see snowflakes fallin' from the sky,\n",
      "matey! Seattle gets an average o' 6-8 inches (15-20 cm) o' snow per year, with most o' it fallin' in\n",
      "January and February.\n",
      "\n",
      "But even with the cold and wet weather, there be some advantages to winter in Seattle. The city's\n",
      "famous coffee culture be in full swing, with cozy cafes and coffee shops to warm ye up on a chilly\n",
      "day. And the holiday season be a magical time in Seattle, with festive lights and decorations\n",
      "adornin' the streets and buildings.\n",
      "\n",
      "Just be prepared for the rain, matey! It be a good idea to pack waterproof gear, including a sturdy\n",
      "umbrella and waterproof boots. And if ye be plannin' on explorin' the great outdoors, be sure to\n",
      "bundle up warm and bring a waterproof jacket to keep ye dry.\n",
      "\n",
      "So, if ye be lookin' for a winter adventure in Seattle, just remember to pack yer rain gear and a\n",
      "sense o' adventure, and ye'll be ready to take on the Emerald City's winter weather!\n",
      "Human: what is it like in autumn?::input=content=\"\\n\\nAutumn in Seattle, matey! It be a grand time\n",
      "o' year, with the Pacific Northwest's famous rain holdin' off just long enough to enjoy the fall\n",
      "colors.\\n\\nFrom September to November, the average temperature be around 50-60°F (10-15°C), with the\n",
      "days gettin' shorter and the nights gettin' cooler. The fall foliage be a sight to behold, with the\n",
      "deciduous trees turnin' brilliant shades o' gold, orange, and red. The city's parks and gardens be\n",
      "filled with the sweet scent o' fallen leaves and the sound o' children playin' in the crisp autumn\n",
      "air.\\n\\nBut don't be fooled, matey! Autumn in Seattle be a time o' transition, and the weather can\n",
      "be as unpredictable as a barnacle on a ship's hull. One day it be sunny and mild, and the next it be\n",
      "rainy and gray. So, be sure to pack yer waterproof gear and a sense o' adventure!\\n\\nOne o' the best\n",
      "things about autumn in Seattle be the festivals and events that take place. The city hosts a variety\n",
      "o' fall festivals, including the Seattle Fall Festival, the Pumpkin Patch Festival, and the\n",
      "Halloween Party. And don't miss the opportunity to try some o' the city's famous fall foods, like\n",
      "apple cider donuts, pumpkin pie, and hot chocolate.\\n\\nSo, if ye be lookin' for a grand time in\n",
      "Seattle, come visit in the autumn! Just be prepared for the weather to be as unpredictable as a\n",
      "pirate's treasure map, and ye'll be ready for a swashbucklin' good time!\"\n",
      "response_metadata={'ResponseMetadata': {'RequestId': 'c804069f-73d4-4d09-9bbe-cd10ddcd14a7',\n",
      "'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 24 Aug 2024 05:08:35 GMT', 'content-type':\n",
      "'application/json', 'content-length': '1588', 'connection': 'keep-alive', 'x-amzn-requestid':\n",
      "'c804069f-73d4-4d09-9bbe-cd10ddcd14a7'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics':\n",
      "{'latencyMs': 4827}} id='run-68e2fa7f-35e7-4fed-8e68-2a7bc46455a9-0' usage_metadata={'input_tokens':\n",
      "646, 'output_tokens': 333, 'total_tokens': 979}::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAutumn in Seattle, matey! It be a grand time o' year, with the Pacific Northwest's famous rain holdin' off just long enough to enjoy the fall colors.\\n\\nFrom September to November, the average temperature be around 50-60°F (10-15°C), with the days gettin' shorter and the nights gettin' cooler. The fall foliage be a sight to behold, with the deciduous trees turnin' brilliant shades o' gold, orange, and red. The city's parks and gardens be filled with the sweet scent o' fallen leaves and the sound o' children playin' in the crisp autumn air.\\n\\nBut don't be fooled, matey! Autumn in Seattle be a time o' transition, and the weather can be as unpredictable as a barnacle on a ship's hull. One day it be sunny and mild, and the next it be rainy and gray. So, be sure to pack yer waterproof gear and a sense o' adventure!\\n\\nOne o' the best things about autumn in Seattle be the festivals and events that take place. The city hosts a variety o' fall festivals, including the Seattle Fall Festival, the Pumpkin Patch Festival, and the Halloween Party. And don't miss the opportunity to try some o' the city's famous fall foods, like apple cider donuts, pumpkin pie, and hot chocolate.\\n\\nSo, if ye be lookin' for a grand time in Seattle, come visit in the autumn! Just be prepared for the weather to be as unpredictable as a pirate's treasure map, and ye'll be ready for a swashbucklin' good time!\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#- second way to create a callback runnable function--\n",
    "def get_chat_history(input_dict: dict, config: RunnableConfig) -> dict:\n",
    "    print(f\"get_chat_history::input_dict:{input_dict}::config={config}\") #- if we do dict at start of chain -- {'input': {'input': 'what is the weather like in Seattle WA?', 'chat_history':\n",
    "    return get_history().messages # return the text as is\n",
    "\n",
    "chat_history_get = RunnableLambda(get_chat_history)\n",
    "\n",
    "history_chain = (\n",
    "    #- Expected a Runnable, callable or dict. If we use a dict here make sure every element is a runnable. And further access is via 'input'.'input'\n",
    "    { # make sure all variable in the prompt template are in this dict\n",
    "        \"input\": RunnablePassthrough(),\n",
    "        \"chat_history\": chat_history_get\n",
    "    }\n",
    "    | chat_user_add\n",
    "    | prompt_with_history\n",
    "    | chatbedrock_llm\n",
    "    | chat_add\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "history_chain.invoke( # here the variable matches the chat prompt template\n",
    "    \"what is it like in autumn?\", \n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now use the In-built helper methods to continue \n",
    "\n",
    "1. We can see that the auto chain will add user and also the AI messages automatically at appropriate places\n",
    "2. Key needs to be the same as what we have in the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arrr, shiver me timbers! Seattle, ye say? Well, matey, I've had me share o' adventures on the high\n",
      "seas, but I've never set foot in that damp and drizzly place. But I've heard tell from me mateys\n",
      "who've sailed those waters that Seattle's weather be as unpredictable as a barnacle on a ship's\n",
      "hull!\n",
      "\n",
      "From what I've gathered, Seattle's got a reputation for bein' a soggy place, with rain comin' down\n",
      "like a stormy sea on most days o' the year. The clouds be gray and thick, like a pirate's beard\n",
      "after a long voyage at sea. And don't even get me started on the wind, matey! It be as fierce as a\n",
      "sea monster, blowin' in from the Pacific and makin' ye want to tie yerself to the mast!\n",
      "\n",
      "But, I've also heard that when the sun does come out, it be as bright as a chest overflowin' with\n",
      "gold doubloons! So, if ye be lookin' for a bit o' sunshine, ye might want to keep yer eye on the\n",
      "forecast, matey!\n",
      "\n",
      "So, there ye have it, me take on the weather in Seattle, WA. Now, if ye'll excuse me, I've got to\n",
      "get back to me ship and me trusty parrot, Polly. We've got a date with the high seas, and I don't\n",
      "want to be late!\n",
      "\n",
      "INPUT_SCHEMA::{'title': 'RunnableWithChatHistoryInput', 'type': 'array', 'items': {'$ref':\n",
      "'#/definitions/BaseMessage'}, 'definitions': {'BaseMessage': {'title': 'BaseMessage', 'description':\n",
      "'Base abstract message class.\\n\\nMessages are the inputs and outputs of ChatModels.', 'type':\n",
      "'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type':\n",
      "'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs':\n",
      "{'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response\n",
      "Metadata', 'type': 'object'}, 'type': {'title': 'Type', 'type': 'string'}, 'name': {'title': 'Name',\n",
      "'type': 'string'}, 'id': {'title': 'Id', 'type': 'string'}}, 'required': ['content', 'type']}}}\n",
      "\n",
      "CHAIN:SCHEMA::{'title': 'RunnableWithMessageHistory', 'description': 'Runnable that manages chat\n",
      "message history for another Runnable.\\n\\nA chat message history is a sequence of messages that\n",
      "represent a conversation.\\n\\nRunnableWithMessageHistory wraps another Runnable and manages the chat\n",
      "message\\nhistory for it; it is responsible for reading and updating the chat\n",
      "message\\nhistory.\\n\\nThe formats supported for the inputs and outputs of the wrapped Runnable\\nare\n",
      "described below.\\n\\nRunnableWithMessageHistory must always be called with a config that\n",
      "contains\\nthe appropriate parameters for the chat message history factory.\\n\\nBy default, the\n",
      "Runnable is expected to take a single configuration parameter\\ncalled `session_id` which is a\n",
      "string. This parameter is used to create a new\\nor look up an existing chat message history that\n",
      "matches the given session_id.\\n\\nIn this case, the invocation would look like\n",
      "this:\\n\\n`with_history.invoke(..., config={\"configurable\": {\"session_id\": \"bar\"}})`\\n; e.g.,\n",
      "``{\"configurable\": {\"session_id\": \"<SESSION_ID>\"}}``.\\n\\nThe configuration can be customized by\n",
      "passing in a list of\\n``ConfigurableFieldSpec`` objects to the ``history_factory_config`` parameter\n",
      "(see\\nexample below).\\n\\nIn the examples, we will use a chat message history with an in-\n",
      "memory\\nimplementation to make it easy to experiment and see the results.\\n\\nFor production use\n",
      "cases, you will want to use a persistent implementation\\nof chat message history, such as\n",
      "``RedisChatMessageHistory``.\\n\\nParameters:\\n    get_session_history: Function that returns a new\n",
      "BaseChatMessageHistory.\\n        This function should either take a single positional argument\\n\n",
      "`session_id` of type string and return a corresponding\\n        chat message history instance.\\n\n",
      "input_messages_key: Must be specified if the base runnable accepts a dict\\n        as input. The key\n",
      "in the input dict that contains the messages.\\n    output_messages_key: Must be specified if the\n",
      "base Runnable returns a dict\\n        as output. The key in the output dict that contains the\n",
      "messages.\\n    history_messages_key: Must be specified if the base runnable accepts a dict\\n\n",
      "as input and expects a separate key for historical messages.\\n    history_factory_config: Configure\n",
      "fields that should be passed to the\\n        chat history factory. See ``ConfigurableFieldSpec`` for\n",
      "more details.\\n\\nExample: Chat message history with an in-memory implementation for testing.\\n\\n..\n",
      "code-block:: python\\n\\n    from operator import itemgetter\\n    from typing import List\\n\\n    from\n",
      "langchain_openai.chat_models import ChatOpenAI\\n\\n    from langchain_core.chat_history import\n",
      "BaseChatMessageHistory\\n    from langchain_core.documents import Document\\n    from\n",
      "langchain_core.messages import BaseMessage, AIMessage\\n    from langchain_core.prompts import\n",
      "ChatPromptTemplate, MessagesPlaceholder\\n    from langchain_core.pydantic_v1 import BaseModel,\n",
      "Field\\n    from langchain_core.runnables import (\\n        RunnableLambda,\\n\n",
      "ConfigurableFieldSpec,\\n        RunnablePassthrough,\\n    )\\n    from\n",
      "langchain_core.runnables.history import RunnableWithMessageHistory\\n\\n\\n    class\n",
      "InMemoryHistory(BaseChatMessageHistory, BaseModel):\\n        \"\"\"In memory implementation of chat\n",
      "message history.\"\"\"\\n\\n        messages: List[BaseMessage] = Field(default_factory=list)\\n\\n\n",
      "def add_messages(self, messages: List[BaseMessage]) -> None:\\n            \"\"\"Add a list of messages\n",
      "to the store\"\"\"\\n            self.messages.extend(messages)\\n\\n        def clear(self) -> None:\\n\n",
      "self.messages = []\\n\\n    # Here we use a global variable to store the chat message history.\\n    #\n",
      "This will make it easier to inspect it to see the underlying results.\\n    store = {}\\n\\n    def\n",
      "get_by_session_id(session_id: str) -> BaseChatMessageHistory:\\n        if session_id not in store:\\n\n",
      "store[session_id] = InMemoryHistory()\\n        return store[session_id]\\n\\n\\n    history =\n",
      "get_by_session_id(\"1\")\\n    history.add_message(AIMessage(content=\"hello\"))\\n    print(store)  #\n",
      "noqa: T201\\n\\n\\nExample where the wrapped Runnable takes a dictionary input:\\n\\n    .. code-block::\n",
      "python\\n\\n        from typing import Optional\\n\\n        from langchain_community.chat_models import\n",
      "ChatAnthropic\\n        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\n",
      "from langchain_core.runnables.history import RunnableWithMessageHistory\\n\\n\\n        prompt =\n",
      "ChatPromptTemplate.from_messages([\\n            (\"system\", \"You\\'re an assistant who\\'s good at\n",
      "{ability}\"),\\n            MessagesPlaceholder(variable_name=\"history\"),\\n            (\"human\",\n",
      "\"{question}\"),\\n        ])\\n\\n        chain = prompt | ChatAnthropic(model=\"claude-2\")\\n\\n\n",
      "chain_with_history = RunnableWithMessageHistory(\\n            chain,\\n            # Uses the\n",
      "get_by_session_id function defined in the example\\n            # above.\\n\n",
      "get_by_session_id,\\n            input_messages_key=\"question\",\\n\n",
      "history_messages_key=\"history\",\\n        )\\n\\n        print(chain_with_history.invoke(  # noqa:\n",
      "T201\\n            {\"ability\": \"math\", \"question\": \"What does cosine mean?\"},\\n\n",
      "config={\"configurable\": {\"session_id\": \"foo\"}}\\n        ))\\n\\n        # Uses the store defined in\n",
      "the example above.\\n        print(store)  # noqa: T201\\n\\n        print(chain_with_history.invoke(\n",
      "# noqa: T201\\n            {\"ability\": \"math\", \"question\": \"What\\'s its inverse\"},\\n\n",
      "config={\"configurable\": {\"session_id\": \"foo\"}}\\n        ))\\n\\n        print(store)  # noqa:\n",
      "T201\\n\\n\\nExample where the session factory takes two keys, user_id and conversation id):\\n\\n    ..\n",
      "code-block:: python\\n\\n        store = {}\\n\\n        def get_session_history(\\n            user_id:\n",
      "str, conversation_id: str\\n        ) -> BaseChatMessageHistory:\\n            if (user_id,\n",
      "conversation_id) not in store:\\n                store[(user_id, conversation_id)] =\n",
      "InMemoryHistory()\\n            return store[(user_id, conversation_id)]\\n\\n        prompt =\n",
      "ChatPromptTemplate.from_messages([\\n            (\"system\", \"You\\'re an assistant who\\'s good at\n",
      "{ability}\"),\\n            MessagesPlaceholder(variable_name=\"history\"),\\n            (\"human\",\n",
      "\"{question}\"),\\n        ])\\n\\n        chain = prompt | ChatAnthropic(model=\"claude-2\")\\n\\n\n",
      "with_message_history = RunnableWithMessageHistory(\\n            chain,\\n\n",
      "get_session_history=get_session_history,\\n            input_messages_key=\"question\",\\n\n",
      "history_messages_key=\"history\",\\n            history_factory_config=[\\n\n",
      "ConfigurableFieldSpec(\\n                    id=\"user_id\",\\n                    annotation=str,\\n\n",
      "name=\"User ID\",\\n                    description=\"Unique identifier for the user.\",\\n\n",
      "default=\"\",\\n                    is_shared=True,\\n                ),\\n\n",
      "ConfigurableFieldSpec(\\n                    id=\"conversation_id\",\\n\n",
      "annotation=str,\\n                    name=\"Conversation ID\",\\n\n",
      "description=\"Unique identifier for the conversation.\",\\n                    default=\"\",\\n\n",
      "is_shared=True,\\n                ),\\n            ],\\n        )\\n\\n\n",
      "with_message_history.invoke(\\n            {\"ability\": \"math\", \"question\": \"What does cosine\n",
      "mean?\"},\\n            config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\\n\n",
      ")', 'type': 'object', 'properties': {'name': {'title': 'Name', 'type': 'string'}, 'bound': {'title':\n",
      "'Bound', 'allOf': [{'type': 'array', 'items': [{}, {}]}]}, 'kwargs': {'title': 'Kwargs', 'type':\n",
      "'object'}, 'config': {'$ref': '#/definitions/RunnableConfig'}, 'custom_input_type': {'title':\n",
      "'Custom Input Type'}, 'custom_output_type': {'title': 'Custom Output Type'}, 'input_messages_key':\n",
      "{'title': 'Input Messages Key', 'type': 'string'}, 'output_messages_key': {'title': 'Output Messages\n",
      "Key', 'type': 'string'}, 'history_messages_key': {'title': 'History Messages Key', 'type':\n",
      "'string'}, 'history_factory_config': {'title': 'History Factory Config', 'type': 'array', 'items':\n",
      "{'type': 'array', 'items': [{'title': 'Id', 'type': 'string'}, {'title': 'Annotation'}, {'title':\n",
      "'Name', 'type': 'string'}, {'title': 'Description', 'type': 'string'}, {'title': 'Default'},\n",
      "{'title': 'Is Shared', 'type': 'boolean'}, {'title': 'Dependencies', 'type': 'array', 'items':\n",
      "{'type': 'string'}}], 'minItems': 7, 'maxItems': 7}}}, 'required': ['bound',\n",
      "'history_factory_config'], 'definitions': {'RunnableConfig': {'title': 'RunnableConfig', 'type':\n",
      "'object', 'properties': {'tags': {'title': 'Tags', 'type': 'array', 'items': {'type': 'string'}},\n",
      "'metadata': {'title': 'Metadata', 'type': 'object'}, 'callbacks': {'title': 'Callbacks', 'anyOf':\n",
      "[{'type': 'array', 'items': {}}, {}]}, 'run_name': {'title': 'Run Name', 'type': 'string'},\n",
      "'max_concurrency': {'title': 'Max Concurrency', 'type': 'integer'}, 'recursion_limit': {'title':\n",
      "'Recursion Limit', 'type': 'integer'}, 'configurable': {'title': 'Configurable', 'type': 'object'},\n",
      "'run_id': {'title': 'Run Id', 'type': 'string', 'format': 'uuid'}}}}}\n",
      "\n",
      "OUPUT_SCHEMA::__root__=None\n",
      "\n",
      "\n",
      " Now we run The example below shows how to use a different chat history for each session.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "chain = prompt | chatbedrock_llm | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print_ww(wrapped_chain.invoke({\"input\": \"what is the weather like in Seattle WA?\"}))\n",
    "\n",
    "\n",
    "print_ww(f\"\\nINPUT_SCHEMA::{wrapped_chain.input_schema.schema()}\")\n",
    "print_ww(f\"\\nCHAIN:SCHEMA::{wrapped_chain.schema()}\")\n",
    "print_ww(f\"\\nOUPUT_SCHEMA::{wrapped_chain.output_schema()}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n Now we run The example below shows how to use a different chat history for each session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: what is the weather like in Seattle WA?\n",
      "AI: \n",
      "\n",
      "Arrr, shiver me timbers! Seattle, ye say? Well, matey, I've had me share o' adventures on the high seas, but I've never set foot in that damp and drizzly place. But I've heard tell from me mateys who've sailed those waters that Seattle's weather be as unpredictable as a barnacle on a ship's hull!\n",
      "\n",
      "From what I've gathered, Seattle's got a reputation for bein' a soggy place, with rain comin' down like a stormy sea on most days o' the year. The clouds be gray and thick, like a pirate's beard after a long voyage at sea. And don't even get me started on the wind, matey! It be as fierce as a sea monster, blowin' in from the Pacific and makin' ye want to tie yerself to the mast!\n",
      "\n",
      "But, I've also heard that when the sun does come out, it be as bright as a chest overflowin' with gold doubloons! So, if ye be lookin' for a bit o' sunshine, ye might want to keep yer eye on the forecast, matey!\n",
      "\n",
      "So, there ye have it, me take on the weather in Seattle, WA. Now, if ye'll excuse me, I've got to get back to me ship and me trusty parrot, Polly. We've got a date with the high seas, and I don't want to be late!\n"
     ]
    }
   ],
   "source": [
    "print(history)\n",
    "# history.add_ai_message\n",
    "# history.add_user_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the multiple session id's with in memory conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I be more familiar with the high seas than the landlubbers'\n",
      "weather forecasts. But, I've heard tell of Seattle, Washington bein' a place with a reputation for\n",
      "bein' as gray as a barnacle's backside. They call it the \"Emerald City,\" but I reckon it's more like\n",
      "the \"Gray City\"!\n",
      "\n",
      "From what I've gathered, Seattle's weather be a mix o' rain, clouds, and a wee bit o' sunshine. It's\n",
      "like sailin' through a misty veil, with the sun peekin' out every now and then to give ye a glimpse\n",
      "o' the treasure hidden beneath the clouds. But don't ye worry, matey, the rain be a good thing! It\n",
      "keeps the scurvy at bay and makes the city's greenery grow like a mighty sea monster risin' from the\n",
      "depths!\n",
      "\n",
      "So, if ye be plannin' a visit to Seattle, be sure to pack yer waterproof boots and a sturdy\n",
      "umbrella, or ye might be walkin' the plank into a puddle o' trouble! And don't forget to bring yer\n",
      "sense o' adventure and a hearty appetite for seafood, or ye might be cursed to walk the streets with\n",
      "a belly full o' gruel!\n",
      "\n",
      "\n",
      " now ask another question and we will see the History conversation was maintained\n",
      "\n",
      "\n",
      "Shiver me timbers! The gray weather in Seattle be a blessing in disguise, matey! The constant rain\n",
      "keeps the city's greenery lush and vibrant, makin' it a pirate's paradise for explorin' the great\n",
      "outdoors. The misty veil also helps to keep the scurvy at bay, makin' it easier for landlubbers to\n",
      "get their daily dose o' vitamin C. And let's not forget the benefits for the city's coffee culture -\n",
      "the rain keeps the coffee shops cozy and inviting, perfect for a warm cup o' grog on a chilly day.\n",
      "Arrr, it be a pirate's life for me!\n",
      "\n",
      "\n",
      " now check the history\n",
      "Human: what is the weather like in Seattle WA?\n",
      "AI: \n",
      "\n",
      "Arrr, shiver me timbers! Seattle, ye say? Well, matey, I've had me share o' adventures on the high seas, but I've never set foot in that damp and drizzly place. But I've heard tell from me mateys who've sailed those waters that Seattle's weather be as unpredictable as a barnacle on a ship's hull!\n",
      "\n",
      "From what I've gathered, Seattle's got a reputation for bein' a soggy place, with rain comin' down like a stormy sea on most days o' the year. The clouds be gray and thick, like a pirate's beard after a long voyage at sea. And don't even get me started on the wind, matey! It be as fierce as a sea monster, blowin' in from the Pacific and makin' ye want to tie yerself to the mast!\n",
      "\n",
      "But, I've also heard that when the sun does come out, it be as bright as a chest overflowin' with gold doubloons! So, if ye be lookin' for a bit o' sunshine, ye might want to keep yer eye on the forecast, matey!\n",
      "\n",
      "So, there ye have it, me take on the weather in Seattle, WA. Now, if ye'll excuse me, I've got to get back to me ship and me trusty parrot, Polly. We've got a date with the high seas, and I don't want to be late!\n"
     ]
    }
   ],
   "source": [
    "### This below LEVARAGES the In-memory with multiple sessions and session id\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    #print(session_id)\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain = prompt | chatbedrock_llm | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print_ww(wrapped_chain.invoke(\n",
    "    {\"input\": \"what is the weather like in Seattle WA\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "))\n",
    "\n",
    "print(\"\\n\\n now ask another question and we will see the History conversation was maintained\")\n",
    "print_ww(wrapped_chain.invoke(\n",
    "    {\"input\": \"Ok what are benefits of this weather in 100 words?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "))\n",
    "\n",
    "print(\"\\n\\n now check the history\")\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we do a Conversation Chat Chain with History and add a Retriever to that convo\n",
    "\n",
    "\n",
    "[Docs links]('https://python.langchain.com/v0.2/docs/versions/migrating_chains/conversation_retrieval_chain/')\n",
    "\n",
    "**Chat History needs to be a list since this is message api so alternate with human and user**\n",
    "\n",
    "1. The ConversationalRetrievalChain was an all-in one way that combined retrieval-augmented generation with chat history, allowing you to \"chat with\" your documents.\n",
    "\n",
    "2. Advantages of switching to the LCEL implementation are similar to the RetrievalQA section above:\n",
    "\n",
    "3. Clearer internals. The ConversationalRetrievalChain chain hides an entire question rephrasing step which dereferences the initial query against the chat history.\n",
    "4. This means the class contains two sets of configurable prompts, LLMs, etc.\n",
    "5. More easily return source documents.\n",
    "6. Support for runnable methods like streaming and async operations.\n",
    "\n",
    "**Below are the key classes to be used**\n",
    "\n",
    "1. We create a QA Chain using the qa_chain as `create_stuff_documents_chain(chatbedrock_llm, qa_prompt)`\n",
    "2. Then we create the Retrieval History chain using the `create_retrieval_chain(history_aware_retriever, qa_chain)`\n",
    "3. Retriever is wrapped in as `create_history_aware_retriever`\n",
    "4. `{context}` goes as System prompts which goes into the Prompt templates\n",
    "5. `Chat History` goes in the Prompt templates like \"placeholder\", \"{chat_history}\")\n",
    "\n",
    "The LCEL implementation exposes the internals of what's happening around retrieving, formatting documents, and passing them through a prompt to the LLM, but it is more verbose. You can customize and wrap this composition logic in a helper function, or use the higher-level `create_retrieval_chain` and `create_stuff_documents_chain` helper method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS as VectorStore\n",
    "\n",
    "In order to be able to use embeddings for search, we need a store that can efficiently perform vector similarity searches. In this notebook we use FAISS, which is an in memory store. For permanently store vectors, one can use pgVector, Pinecone or Chroma.\n",
    "\n",
    "The langchain VectorStore API's are available [here](https://python.langchain.com/en/harrison-docs-refactor-3-24/reference/modules/vectorstore.html)\n",
    "\n",
    "To know more about the FAISS vector store please refer to this [document](https://arxiv.org/pdf/1702.08734.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titan embeddings Model\n",
    "\n",
    "Embeddings are a way to represent words, phrases or any other discrete items as vectors in a continuous vector space. This allows machine learning models to perform mathematical operations on these representations and capture semantic relationships between them.\n",
    "\n",
    "Embeddings are for example used for the RAG [document search capability](https://labelbox.com/blog/how-vector-similarity-search-works/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garygrewal/virtualenv/trainenv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `BedrockEmbeddings` was deprecated in LangChain 0.2.11 and will be removed in 0.4.0. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import BedrockEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv to rag_data/Amazon_SageMaker_FAQs.csv\n",
      "Number of documents=153\n",
      "Number of documents after split and chunking=154\n",
      "vectorstore_faiss_aws: number of elements in the index=154::\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "\n",
    "s3_path = \"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv\"\n",
    "!aws s3 cp $s3_path ./rag_data/Amazon_SageMaker_FAQs.csv\n",
    "\n",
    "loader = CSVLoader(\"./rag_data/Amazon_SageMaker_FAQs.csv\") # --- > 219 docs with 400 chars, each row consists in a question column and an answer column\n",
    "documents_aws = loader.load() #\n",
    "print(f\"Number of documents={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Number of documents after split and chunking={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "\n",
    "    \n",
    "vectorstore_faiss_aws = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "     embedding = br_embeddings\n",
    ")\n",
    "\n",
    "print(f\"vectorstore_faiss_aws: number of elements in the index={vectorstore_faiss_aws.index.ntotal}::\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we do the simple Retrieval QA chain -- No chat history but with retriver\n",
    "[Docs link]('https://python.langchain.com/v0.2/docs/versions/migrating_chains/retrieval_qa/')\n",
    "\n",
    "Key points\n",
    "1. The chain in QA uses the variable as the first value, can be input or question  and so the prompt template for the Human query has to have the `Question` or `input` as the variable\n",
    "2. This chain will re formulate the question, call the retriver and then answer the question\n",
    "3. Our prompt template removes any answer where retriver is not needed and so no answer is obtained\n",
    "4. Context goes into the system prompts section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\"), HumanMessage(content='test_input')])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ").invoke({'input': 'test_input', 'chat_history' : chat_history_messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'BedrockEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x111cb7790>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_faiss_aws.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The retriever invoke is called with the user input \n",
    "\n",
    "1. That will fetch the context and then add that as a string to the inputs \n",
    "2. The chain will use that as `context` based on the variable in the chain so we have the correct context\n",
    "3. This same process could have been done with the memory as well if we wanted to send a string as input\n",
    "\n",
    "The input is a string because we convert it to a dict as the very first step on the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug_inputs::input_dict:<class 'dict'>::value::{'context': '\\ufeffWhat is Amazon SageMaker?: When\n",
      "should I use reinforcement learning?\\nAmazon SageMaker is a fully managed service to prepare data\n",
      "and build, train, and deploy machine learning (ML) models for any use case with fully managed\n",
      "infrastructure, tools, and workflows.: While the goal of supervised learning techniques is to find\n",
      "the right answer based on the patterns in the training data, the goal of unsupervised learning\n",
      "techniques is to find similarities and differences between data points. In contrast, the goal of\n",
      "reinforcement learning (RL) techniques is to learn how to achieve a desired outcome even when it is\n",
      "not clear how to accomplish that outcome. As a result, RL is more suited to enabling intelligent\n",
      "applications where an agent can make autonomous decisions such as robotics, autonomous vehicles,\n",
      "HVAC, industrial control, and more.\\n\\n\\ufeffWhat is Amazon SageMaker?: Do I need to write my own RL\n",
      "agent algorithms to train RL models?\\nAmazon SageMaker is a fully managed service to prepare data\n",
      "and build, train, and deploy machine learning (ML) models for any use case with fully managed\n",
      "infrastructure, tools, and workflows.: No, Amazon SageMaker RL includes RL toolkits such as Coach\n",
      "and Ray RLLib that offer implementations of RL agent algorithms such as DQN, PPO, A3C, and many\n",
      "more.\\n\\n\\ufeffWhat is Amazon SageMaker?: How is reinforcement learning different from supervised\n",
      "learning?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy\n",
      "machine learning (ML) models for any use case with fully managed infrastructure, tools, and\n",
      "workflows.: Though both supervised and reinforcement learning use mapping between input and output,\n",
      "unlike supervised learning where the feedback provided to the agent is the correct set of actions\n",
      "for performing a task, reinforcement learning uses a delayed feedback where reward signals are\n",
      "optimized to ensure a long-term goal through a sequence of actions.\\n\\n\\ufeffWhat is Amazon\n",
      "SageMaker?: What is reinforcement learning?\\nAmazon SageMaker is a fully managed service to prepare\n",
      "data and build, train, and deploy machine learning (ML) models for any use case with fully managed\n",
      "infrastructure, tools, and workflows.: Reinforcement learning is a ML technique that enables an\n",
      "agent to learn in an interactive environment by trial and error using feedback from its own actions\n",
      "and experiences.', 'input': 'What are autonomous agents?'}::config={'tags': [], 'metadata': {},\n",
      "'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x11211e0d0>,\n",
      "'recursion_limit': 25, 'configurable': {}}\n",
      "\n",
      "\n",
      "I don't have enough context to answer this question. The provided context only mentions autonomous\n",
      "vehicles and industrial control as examples of applications where reinforcement learning can be\n",
      "used, but it does not define what autonomous agents are.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"), # expected by the qa chain as it sends in question as the variable\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    #print(docs)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#- second way to create a callback runnable function--\n",
    "def debug_inputs(input_dict: dict, config: RunnableConfig) -> dict:\n",
    "    print_ww(f\"debug_inputs::input_dict:{type(input_dict)}::value::{input_dict}::config={config}\") #- if we do dict at start of chain -- {'input': {'input': 'what is the weather like in Seattle WA?', 'chat_history':\n",
    "    return input_dict # return the text as is\n",
    "\n",
    "chat_user_debug = RunnableLambda(debug_inputs)\n",
    "\n",
    "# The chain \n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vectorstore_faiss_aws.as_retriever() | format_docs, # can work even without the format\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | chat_user_debug\n",
    "    | condense_question_prompt\n",
    "    | chatbedrock_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print_ww(qa_chain.invoke(input=\"What are autonomous agents?\")) # cannot be a dict object here because we create the dict from string as first step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask the same question to Meta Models\n",
    "**Note with the converse API we do not need to formulate or change any prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I don't have enough context to answer this question. The provided context only mentions autonomous\n",
      "vehicles and industrial control as examples of applications where reinforcement learning can be\n",
      "used, but it does not define what autonomous agents are.\n",
      "\n",
      "\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine\n",
      "learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"), # expected by the qa chain as it sends in question as the variable\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    #print(docs)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vectorstore_faiss_aws.as_retriever() | format_docs, # can work even without the format\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | condense_question_prompt\n",
    "    | chatbedrock_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print_ww(qa_chain.invoke(input=\"What are autonomous agents?\")) # cannot be a dict object here)\n",
    "\n",
    "print_ww(qa_chain.invoke(input=\"What is SageMaker used for?\")) # cannot be a dict object here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we get a real answer as we invoke where retriever gives context\n",
    "\n",
    "Use the Helper method to create the Retiever QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What are the options for model explainability in SageMaker?', 'config': {'configurable':\n",
      "{'session_id': 'abc123'}}, 'context': [Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 11}, page_content='\\ufeffWhat is Amazon SageMaker?:\n",
      "How does Amazon SageMaker Clarify improve model explainability?\\nAmazon SageMaker is a fully managed\n",
      "service to prepare data and build, train, and deploy machine learning (ML) models for any use case\n",
      "with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Clarify is integrated\n",
      "with Amazon SageMaker Experiments to provide a feature importance graph detailing the importance of\n",
      "each input for your model’s overall decision-making process after the model has been trained. These\n",
      "details can help determine if a particular model input has more influence than it should on overall\n",
      "model behavior. SageMaker Clarify also makes explanations for individual predictions available via\n",
      "an API.'), Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 19},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: What does Amazon SageMaker Model Dashboard\n",
      "do?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy\n",
      "machine learning (ML) models for any use case with fully managed infrastructure, tools, and\n",
      "workflows.: Amazon SageMaker Model Dashboard gives you a comprehensive overview of deployed models\n",
      "and endpoints, letting you track resources and model behavior violations through one pane. It allows\n",
      "you to monitor model behavior in four dimensions, including data and model quality, and bias and\n",
      "feature attribution drift through its integration with Amazon SageMaker Model Monitor and Amazon\n",
      "SageMaker Clarify. SageMaker Model Dashboard also provides an integrated experience to set up and\n",
      "receive alerts for missing and inactive model monitoring jobs, and deviations in model behavior for\n",
      "model quality, data quality, bias drift, and feature attribution drift. You can further inspect\n",
      "individual models and analyze factors impacting model performance over time. Then, you can follow up\n",
      "with ML practitioners to take corrective measures.'), Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9}, page_content='\\ufeffWhat is Amazon SageMaker?:\n",
      "How can I check for imbalances in my model?\\nAmazon SageMaker is a fully managed service to prepare\n",
      "data and build, train, and deploy machine learning (ML) models for any use case with fully managed\n",
      "infrastructure, tools, and workflows.: Amazon SageMaker Clarify\\xa0helps improve model transparency\n",
      "by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances\n",
      "during data preparation, after training, and ongoing over time, and also includes tools to help\n",
      "explain ML models and their predictions. Findings can be shared through explainability reports.'),\n",
      "Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 118},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: Why should I use\\xa0Amazon SageMaker Inference\n",
      "Recommender?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and\n",
      "deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and\n",
      "workflows.: You should use SageMaker Inference Recommender if you need recommendations for the right\n",
      "endpoint configuration to improve performance and reduce costs. Previously, data scientists who\n",
      "wanted to deploy their models had to run manual benchmarks to select the right endpoint\n",
      "configuration. They had to first select the right ML instance type out of the 70+ available instance\n",
      "types based on the resource requirements of their models and sample payloads, and then optimize the\n",
      "model to account for differing hardware. Then, they had to conduct extensive load tests to validate\n",
      "that latency and throughput requirements are met and that the costs are low. SageMaker Inference\n",
      "Recommender eliminates this complexity by making it easy for you to: 1) get started in minutes with\n",
      "an instance recommendation; 2) conduct load tests across instance types to get recommendations on\n",
      "your endpoint configuration within hours; and 3) automatically tune container and model server\n",
      "parameters as well as perform model optimizations for a given instance type.')], 'answer':\n",
      "\"\\n\\nAccording to the provided context, Amazon SageMaker Clarify provides model explainability\n",
      "by:\\n\\n* Providing a feature importance graph detailing the importance of each input for your\n",
      "model's overall decision-making process\\n* Making explanations for individual predictions available\n",
      "via an API\\n* Detecting statistical bias across the entire ML workflow\\n* Checking for imbalances\n",
      "during data preparation, after training, and ongoing over time\\n* Including tools to help explain ML\n",
      "models and their predictions\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "qa_chain = create_stuff_documents_chain(chatbedrock_llm, condense_question_prompt)\n",
    "\n",
    "convo_qa_chain = create_retrieval_chain(vectorstore_faiss_aws.as_retriever(), qa_chain)\n",
    "\n",
    "print_ww(convo_qa_chain.invoke(\n",
    "    {'input':\"What are the options for model explainability in SageMaker?\", \n",
    "      'config':{\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "    })) # cannot be a dict object here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we create Chat Conversation which has history and retrieval context\n",
    "So we use the HISTORY AWARE Retriever and create a chain\n",
    "\n",
    "1. We create a stuff chain\n",
    "2. Then we pass it to the create retrieval chain method -- we could have used the LCEL as well to create the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the options for model explainability in SageMaker?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 11}, page_content='\\ufeffWhat is Amazon SageMaker?: How does Amazon SageMaker Clarify improve model explainability?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Clarify is integrated with Amazon SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available via an API.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 19}, page_content='\\ufeffWhat is Amazon SageMaker?: What does Amazon SageMaker Model Dashboard do?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Model Dashboard gives you a comprehensive overview of deployed models and endpoints, letting you track resources and model behavior violations through one pane. It allows you to monitor model behavior in four dimensions, including data and model quality, and bias and feature attribution drift through its integration with Amazon SageMaker Model Monitor and Amazon SageMaker Clarify. SageMaker Model Dashboard also provides an integrated experience to set up and receive alerts for missing and inactive model monitoring jobs, and deviations in model behavior for model quality, data quality, bias drift, and feature attribution drift. You can further inspect individual models and analyze factors impacting model performance over time. Then, you can follow up with ML practitioners to take corrective measures.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9}, page_content='\\ufeffWhat is Amazon SageMaker?: How can I check for imbalances in my model?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Clarify\\xa0helps improve model transparency by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time, and also includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 118}, page_content='\\ufeffWhat is Amazon SageMaker?: Why should I use\\xa0Amazon SageMaker Inference Recommender?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: You should use SageMaker Inference Recommender if you need recommendations for the right endpoint configuration to improve performance and reduce costs. Previously, data scientists who wanted to deploy their models had to run manual benchmarks to select the right endpoint configuration. They had to first select the right ML instance type out of the 70+ available instance types based on the resource requirements of their models and sample payloads, and then optimize the model to account for differing hardware. Then, they had to conduct extensive load tests to validate that latency and throughput requirements are met and that the costs are low. SageMaker Inference Recommender eliminates this complexity by making it easy for you to: 1) get started in minutes with an instance recommendation; 2) conduct load tests across instance types to get recommendations on your endpoint configuration within hours; and 3) automatically tune container and model server parameters as well as perform model optimizations for a given instance type.')],\n",
       " 'answer': \"\\n\\nAmazon SageMaker provides model explainability through Amazon SageMaker Clarify, which offers feature importance graphs and individual prediction explanations. These features help determine the importance of each input for a model's decision-making process and provide insights into individual predictions.\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "contextualized_question_system_template = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualized_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualized_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chatbedrock_llm, vectorstore_faiss_aws.as_retriever(), contextualized_question_prompt\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{contex}\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"Explain this  {input}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(chatbedrock_llm, qa_prompt)\n",
    "\n",
    "convo_qa_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
    "\n",
    "convo_qa_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are the options for model explainability in SageMaker?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you\n",
      "can.'), HumanMessage(content='Human: what is the weather like in Seattle WA?'),\n",
      "AIMessage(content=\"AI: Ahoy matey! As a pirate, I don't spend much time on land, but I've heard\n",
      "tales of the weather in Seattle.\"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"\\n    You are an assistant for question-answering tasks. ONLY Use\n",
      "the following pieces of retrieved context to answer the question.\\n    If the answer is not in the\n",
      "context below , just say you do not have enough context. \\n    If you don't know the answer, just\n",
      "say that you don't know. \\n    Use three sentences maximum and keep the answer concise.\\n\n",
      "Context: this is a test context \\n    \"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"You are an assistant for question-answering tasks. Use the\n",
      "following pieces of retrieved context to answer the question. If you don't know the answer, say that\n",
      "you don't know. Use three sentences maximum and keep the answer concise.\\n\\nthis is a test\n",
      "context\"), HumanMessage(content='Human: what is the weather like in Seattle WA?'),\n",
      "AIMessage(content=\"AI: Ahoy matey! As a pirate, I don't spend much time on land, but I've heard\n",
      "tales of the weather in Seattle.\"), HumanMessage(content='Explain this  test_input.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat_history_messages = [\n",
    "        HumanMessage(\"Human: what is the weather like in Seattle WA?\"), # - normal string converts it to a Human message always but we need ai/human pairs\n",
    "        AIMessage(\"AI: Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages( # can create either as System Message Object or as TUPLE -- system, message\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"), # this assumes the messages are in list of messages format and this becomes MessagePlaceholder object\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- variable chat_history should be a list of base messages, got test_chat_history of type <class 'str'>\n",
    "#- this gets converted as a LIST of messages -- with each of the TUPLE or Object being executed with the variables when invoked\n",
    "print_ww(prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages}))\n",
    "\n",
    "# -- condense question prompt with CONTEXT\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- missing variables {'context'}. chat history will get ignored - variables are passed in as keys in the dict\n",
    "print(\"\\n\")\n",
    "print_ww(condense_question_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))\n",
    "\n",
    "# - Chat prompt template with Place holders\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{contex}\"),\n",
    "        MessagesPlaceholder(\"chat_history\"), # has to be in form of USER/AI messages\n",
    "        (\"human\", \"Explain this  {input}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print_ww(qa_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto add the history to the Chat with Retriever\n",
    "\n",
    "Wrap with Runnable Chat History with Session id and run the chat conversation\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/context_aware_history_retriever.png)\n",
    "\n",
    "borrowed from https://github.com/langchain-ai/langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "contextualized_question_system_template = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualized_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualized_question_system_template),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chatbedrock_llm, vectorstore_faiss_aws.as_retriever(), contextualized_question_prompt\n",
    ")\n",
    "\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If the answer is not present in the context, just say you do not have enough context to answer. \\\n",
    "If the input is not present in the context, just say you do not have enough context to answer. \\\n",
    "If the question is not present in the context, just say you do not have enough context to answer. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "question_answer_chain = create_stuff_documents_chain(chatbedrock_llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "#- Wrap the rag_chain with RunnableWithMessageHistory to automatically handle chat history:\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    #print(session_id)\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What kind of bias can SageMaker detect?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 10}, page_content=\"\\ufeffWhat is Amazon SageMaker?: What kind of bias does Amazon SageMaker Clarify detect?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You need to choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9}, page_content='\\ufeffWhat is Amazon SageMaker?: How can I check for imbalances in my model?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Clarify\\xa0helps improve model transparency by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time, and also includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 118}, page_content='\\ufeffWhat is Amazon SageMaker?: Why should I use\\xa0Amazon SageMaker Inference Recommender?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: You should use SageMaker Inference Recommender if you need recommendations for the right endpoint configuration to improve performance and reduce costs. Previously, data scientists who wanted to deploy their models had to run manual benchmarks to select the right endpoint configuration. They had to first select the right ML instance type out of the 70+ available instance types based on the resource requirements of their models and sample payloads, and then optimize the model to account for differing hardware. Then, they had to conduct extensive load tests to validate that latency and throughput requirements are met and that the costs are low. SageMaker Inference Recommender eliminates this complexity by making it easy for you to: 1) get started in minutes with an instance recommendation; 2) conduct load tests across instance types to get recommendations on your endpoint configuration within hours; and 3) automatically tune container and model server parameters as well as perform model optimizations for a given instance type.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 117}, page_content='\\ufeffWhat is Amazon SageMaker?: What is Amazon SageMaker Inference Recommender?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Inference Recommender\\xa0is a new capability of Amazon SageMaker that reduces the time required to get ML models in production by automating performance benchmarking and tuning model performance across SageMaker ML instances. You can now use SageMaker Inference Recommender to deploy your model to an endpoint that delivers the best performance and minimizes cost. You can get started with SageMaker Inference Recommender in minutes while selecting an instance type and get recommendations for optimal endpoint configurations within hours, eliminating weeks of manual testing and tuning time. With SageMaker Inference Recommender, you pay only for the SageMaker ML instances used during load testing, and there are no additional charges.')],\n",
       " 'answer': '\\n\\nAmazon SageMaker Clarify detects statistical bias across the entire ML workflow, including imbalances during data preparation, after training, and ongoing over time.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain_with_history.invoke(\n",
    "    {\"input\": \"What kind of bias can SageMaker detect?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a follow on question\n",
    "\n",
    "1. The phrase `it` will be converted based on the chat history\n",
    "2. Retriever gets invoked to get relevant content based on chat history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What are common ways of doing it?', 'chat_history': [HumanMessage(content='What kind of\n",
      "bias can SageMaker detect?'), AIMessage(content='\\n\\nAmazon SageMaker Clarify detects statistical\n",
      "bias across the entire ML workflow, including imbalances during data preparation, after training,\n",
      "and ongoing over time.')], 'context': [Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 10}, page_content=\"\\ufeffWhat is Amazon SageMaker?:\n",
      "What kind of bias does Amazon SageMaker Clarify detect?\\nAmazon SageMaker is a fully managed service\n",
      "to prepare data and build, train, and deploy machine learning (ML) models for any use case with\n",
      "fully managed infrastructure, tools, and workflows.: Measuring bias in ML models is a first step to\n",
      "mitigating bias. Bias may be measured before training and after training, as well as for inference\n",
      "for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even\n",
      "considering simple notions of fairness leads to many different measures applicable in various\n",
      "contexts. You need to choose bias notions and metrics that are valid for the application and the\n",
      "situation under investigation. SageMaker currently supports the computation of different bias\n",
      "metrics for training data (as part of SageMaker data preparation), for the trained model (as part of\n",
      "Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker\n",
      "Model Monitor). For example, before training, we provide metrics for checking whether the training\n",
      "data is representative (that is, whether one group is underrepresented) and whether there are\n",
      "differences in the label distribution across groups. After training or during deployment, metrics\n",
      "can be helpful to measure whether (and by how much) the performance of the model differs across\n",
      "groups. For example, start by comparing the error rates (how likely a model's prediction is to\n",
      "differ from the true label) or break further down into precision (how likely a positive prediction\n",
      "is to be correct) and recall (how likely the model will correctly label a positive example).\"),\n",
      "Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: How can I check for imbalances in my model?\\nAmazon\n",
      "SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning\n",
      "(ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon\n",
      "SageMaker Clarify\\xa0helps improve model transparency by detecting statistical bias across the\n",
      "entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training,\n",
      "and ongoing over time, and also includes tools to help explain ML models and their predictions.\n",
      "Findings can be shared through explainability reports.'), Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 118}, page_content='\\ufeffWhat is Amazon SageMaker?:\n",
      "Why should I use\\xa0Amazon SageMaker Inference Recommender?\\nAmazon SageMaker is a fully managed\n",
      "service to prepare data and build, train, and deploy machine learning (ML) models for any use case\n",
      "with fully managed infrastructure, tools, and workflows.: You should use SageMaker Inference\n",
      "Recommender if you need recommendations for the right endpoint configuration to improve performance\n",
      "and reduce costs. Previously, data scientists who wanted to deploy their models had to run manual\n",
      "benchmarks to select the right endpoint configuration. They had to first select the right ML\n",
      "instance type out of the 70+ available instance types based on the resource requirements of their\n",
      "models and sample payloads, and then optimize the model to account for differing hardware. Then,\n",
      "they had to conduct extensive load tests to validate that latency and throughput requirements are\n",
      "met and that the costs are low. SageMaker Inference Recommender eliminates this complexity by making\n",
      "it easy for you to: 1) get started in minutes with an instance recommendation; 2) conduct load tests\n",
      "across instance types to get recommendations on your endpoint configuration within hours; and 3)\n",
      "automatically tune container and model server parameters as well as perform model optimizations for\n",
      "a given instance type.'), Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv',\n",
      "'row': 105}, page_content='\\ufeffWhat is Amazon SageMaker?: When should I use reinforcement\n",
      "learning?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy\n",
      "machine learning (ML) models for any use case with fully managed infrastructure, tools, and\n",
      "workflows.: While the goal of supervised learning techniques is to find the right answer based on\n",
      "the patterns in the training data, the goal of unsupervised learning techniques is to find\n",
      "similarities and differences between data points. In contrast, the goal of reinforcement learning\n",
      "(RL) techniques is to learn how to achieve a desired outcome even when it is not clear how to\n",
      "accomplish that outcome. As a result, RL is more suited to enabling intelligent applications where\n",
      "an agent can make autonomous decisions such as robotics, autonomous vehicles, HVAC, industrial\n",
      "control, and more.')], 'answer': \"\\n\\n\\nI don't have enough context to answer. The provided context\n",
      "only mentions Amazon SageMaker Clarify, but it does not provide common ways of detecting bias.\"}\n"
     ]
    }
   ],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "print_ww(follow_up_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Will it help?', 'chat_history': [HumanMessage(content='What kind of bias can SageMaker\n",
      "detect?'), AIMessage(content='\\n\\nAmazon SageMaker Clarify detects statistical bias across the\n",
      "entire ML workflow, including imbalances during data preparation, after training, and ongoing over\n",
      "time.'), HumanMessage(content='What are common ways of doing it?'), AIMessage(content=\"\\n\\n\\nI don't\n",
      "have enough context to answer. The provided context only mentions Amazon SageMaker Clarify, but it\n",
      "does not provide common ways of detecting bias.\")], 'context': [Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 10}, page_content=\"\\ufeffWhat is Amazon SageMaker?:\n",
      "What kind of bias does Amazon SageMaker Clarify detect?\\nAmazon SageMaker is a fully managed service\n",
      "to prepare data and build, train, and deploy machine learning (ML) models for any use case with\n",
      "fully managed infrastructure, tools, and workflows.: Measuring bias in ML models is a first step to\n",
      "mitigating bias. Bias may be measured before training and after training, as well as for inference\n",
      "for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even\n",
      "considering simple notions of fairness leads to many different measures applicable in various\n",
      "contexts. You need to choose bias notions and metrics that are valid for the application and the\n",
      "situation under investigation. SageMaker currently supports the computation of different bias\n",
      "metrics for training data (as part of SageMaker data preparation), for the trained model (as part of\n",
      "Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker\n",
      "Model Monitor). For example, before training, we provide metrics for checking whether the training\n",
      "data is representative (that is, whether one group is underrepresented) and whether there are\n",
      "differences in the label distribution across groups. After training or during deployment, metrics\n",
      "can be helpful to measure whether (and by how much) the performance of the model differs across\n",
      "groups. For example, start by comparing the error rates (how likely a model's prediction is to\n",
      "differ from the true label) or break further down into precision (how likely a positive prediction\n",
      "is to be correct) and recall (how likely the model will correctly label a positive example).\"),\n",
      "Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9},\n",
      "page_content='\\ufeffWhat is Amazon SageMaker?: How can I check for imbalances in my model?\\nAmazon\n",
      "SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning\n",
      "(ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon\n",
      "SageMaker Clarify\\xa0helps improve model transparency by detecting statistical bias across the\n",
      "entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training,\n",
      "and ongoing over time, and also includes tools to help explain ML models and their predictions.\n",
      "Findings can be shared through explainability reports.'), Document(metadata={'source':\n",
      "'./rag_data/Amazon_SageMaker_FAQs.csv', 'row': 118}, page_content='\\ufeffWhat is Amazon SageMaker?:\n",
      "Why should I use\\xa0Amazon SageMaker Inference Recommender?\\nAmazon SageMaker is a fully managed\n",
      "service to prepare data and build, train, and deploy machine learning (ML) models for any use case\n",
      "with fully managed infrastructure, tools, and workflows.: You should use SageMaker Inference\n",
      "Recommender if you need recommendations for the right endpoint configuration to improve performance\n",
      "and reduce costs. Previously, data scientists who wanted to deploy their models had to run manual\n",
      "benchmarks to select the right endpoint configuration. They had to first select the right ML\n",
      "instance type out of the 70+ available instance types based on the resource requirements of their\n",
      "models and sample payloads, and then optimize the model to account for differing hardware. Then,\n",
      "they had to conduct extensive load tests to validate that latency and throughput requirements are\n",
      "met and that the costs are low. SageMaker Inference Recommender eliminates this complexity by making\n",
      "it easy for you to: 1) get started in minutes with an instance recommendation; 2) conduct load tests\n",
      "across instance types to get recommendations on your endpoint configuration within hours; and 3)\n",
      "automatically tune container and model server parameters as well as perform model optimizations for\n",
      "a given instance type.'), Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv',\n",
      "'row': 105}, page_content='\\ufeffWhat is Amazon SageMaker?: When should I use reinforcement\n",
      "learning?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy\n",
      "machine learning (ML) models for any use case with fully managed infrastructure, tools, and\n",
      "workflows.: While the goal of supervised learning techniques is to find the right answer based on\n",
      "the patterns in the training data, the goal of unsupervised learning techniques is to find\n",
      "similarities and differences between data points. In contrast, the goal of reinforcement learning\n",
      "(RL) techniques is to learn how to achieve a desired outcome even when it is not clear how to\n",
      "accomplish that outcome. As a result, RL is more suited to enabling intelligent applications where\n",
      "an agent can make autonomous decisions such as robotics, autonomous vehicles, HVAC, industrial\n",
      "control, and more.')], 'answer': '\\n\\nYes, Amazon SageMaker Clarify helps improve model transparency\n",
      "by detecting statistical bias across the entire ML workflow.'}\n"
     ]
    }
   ],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"Will it help?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "print_ww(follow_up_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now ask a random question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Give me a few tips on how to plant a  new garden.',\n",
       " 'chat_history': [HumanMessage(content='What kind of bias can SageMaker detect?'),\n",
       "  AIMessage(content='\\n\\nAmazon SageMaker Clarify detects statistical bias across the entire ML workflow, including imbalances during data preparation, after training, and ongoing over time.'),\n",
       "  HumanMessage(content='What are common ways of doing it?'),\n",
       "  AIMessage(content=\"\\n\\n\\nI don't have enough context to answer. The provided context only mentions Amazon SageMaker Clarify, but it does not provide common ways of detecting bias.\"),\n",
       "  HumanMessage(content='Will it help?'),\n",
       "  AIMessage(content='\\n\\nYes, Amazon SageMaker Clarify helps improve model transparency by detecting statistical bias across the entire ML workflow.')],\n",
       " 'context': [Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 10}, page_content=\"\\ufeffWhat is Amazon SageMaker?: What kind of bias does Amazon SageMaker Clarify detect?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You need to choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 9}, page_content='\\ufeffWhat is Amazon SageMaker?: How can I check for imbalances in my model?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Amazon SageMaker Clarify\\xa0helps improve model transparency by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time, and also includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 118}, page_content='\\ufeffWhat is Amazon SageMaker?: Why should I use\\xa0Amazon SageMaker Inference Recommender?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: You should use SageMaker Inference Recommender if you need recommendations for the right endpoint configuration to improve performance and reduce costs. Previously, data scientists who wanted to deploy their models had to run manual benchmarks to select the right endpoint configuration. They had to first select the right ML instance type out of the 70+ available instance types based on the resource requirements of their models and sample payloads, and then optimize the model to account for differing hardware. Then, they had to conduct extensive load tests to validate that latency and throughput requirements are met and that the costs are low. SageMaker Inference Recommender eliminates this complexity by making it easy for you to: 1) get started in minutes with an instance recommendation; 2) conduct load tests across instance types to get recommendations on your endpoint configuration within hours; and 3) automatically tune container and model server parameters as well as perform model optimizations for a given instance type.'),\n",
       "  Document(metadata={'source': './rag_data/Amazon_SageMaker_FAQs.csv', 'row': 105}, page_content='\\ufeffWhat is Amazon SageMaker?: When should I use reinforcement learning?\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: While the goal of supervised learning techniques is to find the right answer based on the patterns in the training data, the goal of unsupervised learning techniques is to find similarities and differences between data points. In contrast, the goal of reinforcement learning (RL) techniques is to learn how to achieve a desired outcome even when it is not clear how to accomplish that outcome. As a result, RL is more suited to enabling intelligent applications where an agent can make autonomous decisions such as robotics, autonomous vehicles, HVAC, industrial control, and more.')],\n",
       " 'answer': \"\\n\\nI don't have enough context to answer. The provided context only talks about Amazon SageMaker, a machine learning service, and does not mention gardening.\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"Give me a few tips on how to plant a  new garden.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "follow_up_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the semantic search works:\n",
    "1. First we calculate the embeddings vector for the query, and\n",
    "2. then we use this vector to do a similarity search on the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.171875, 0.33398438, 0.3125, -0.24316406, 0.60546875, 0.41992188, -0.36132812, -6.580353e-05, 0.3203125, -0.66796875]\n",
      "﻿What is Amazon SageMaker?: Is R supported with Amazon SageMaker?\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine\n",
      "learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Yes,\n",
      "R is supported with Amazon SageMaker. You can use R within SageMaker notebook instances, which\n",
      "include a preinstalled R kernel and the reticulate library. Reticulate offers an R interface for the\n",
      "Amazon SageMaker Python SDK, enabling ML practitioners to build, train, tune, and deploy R models.\n",
      "----\n",
      "﻿What is Amazon SageMaker?: What is RStudio on Amazon SageMaker?\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine\n",
      "learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.:\n",
      "RStudio on Amazon SageMaker is the first fully managed RStudio Workbench in the cloud. You can\n",
      "quickly launch the familiar RStudio integrated development environment (IDE) and dial up and down\n",
      "the underlying compute resources without interrupting your work, making it easy to build machine\n",
      "learning (ML) and analytics solutions in R at scale. You can seamlessly switch between the RStudio\n",
      "IDE and Amazon SageMaker Studio notebooks for R and Python development. All your work, including\n",
      "code, datasets, repositories, and other artifacts, is automatically synchronized between the two\n",
      "environments to reduce context switch and boost productivity.\n",
      "----\n",
      "﻿What is Amazon SageMaker?: What is Amazon SageMaker Studio?\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine\n",
      "learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.:\n",
      "Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all ML\n",
      "development steps. SageMaker Studio gives you complete access, control, and visibility into each\n",
      "step required to prepare data and build, train, and deploy models. You can quickly upload data,\n",
      "create new notebooks, train and tune models, move back and forth between steps to adjust\n",
      "experiments, compare results, and deploy models to production all in one place, making you much more\n",
      "productive. All ML development activities including notebooks, experiment management, automatic\n",
      "model creation, debugging and profiling, and model drift detection can be performed within the\n",
      "unified SageMaker Studio visual interface.\n",
      "----\n",
      "﻿What is Amazon SageMaker?: What is Amazon SageMaker Experiments?\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine\n",
      "learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.:\n",
      "Amazon SageMaker Experiments helps you organize and track iterations to ML models. SageMaker\n",
      "Experiments helps you manage iterations by automatically capturing the input parameters,\n",
      "configurations, and results, and storing them as \"experiments\". You can work within the visual\n",
      "interface of Amazon SageMaker Studio, where you can browse active experiments, search for previous\n",
      "experiments by their characteristics, review previous experiments with their results, and compare\n",
      "experiment results visually.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "v = br_embeddings.embed_query(\"R in SageMaker\")\n",
    "print(v[0:10])\n",
    "results = vectorstore_faiss_aws.similarity_search_by_vector(v, k=4)\n",
    "for r in results:\n",
    "    print_ww(r.page_content)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory\n",
    "In any chatbot we will need a QA Chain with various options which are customized by the use case. But in a chatbot we will always need to keep the history of the conversation so the model can take it into consideration to provide the answer. In this example we use the [ConversationalRetrievalChain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db) from LangChain, together with a ConversationBufferMemory to keep the history of the conversation.\n",
    "\n",
    "Source: https://python.langchain.com/docs/modules/chains/popular/chat_vector_db\n",
    "\n",
    "Set `verbose` to `True` to see all the what is going on behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following conversation and a follow up question, rephrase the follow up question to be a\n",
      "standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "{chat_history}\n",
      "Follow Up Input: {question}\n",
      "Standalone question:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "print_ww(CONDENSE_QUESTION_PROMPT.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters used for ConversationRetrievalChain\n",
    "* **retriever**: We used `VectorStoreRetriever`, which is backed by a `VectorStore`. To retrieve text, there are two search types you can choose: `\"similarity\"` or `\"mmr\"`. `search_type=\"similarity\"` uses similarity search in the retriever object where it selects text chunk vectors that are most similar to the question vector.\n",
    "\n",
    "* **memory**: Memory Chain to store the history \n",
    "\n",
    "* **condense_question_prompt**: Given a question from the user, we use the previous conversation and that question to make up a standalone question\n",
    "\n",
    "* **chain_type**: If the chat history is long and doesn't fit the context you use this parameter and the options are `stuff`, `refine`, `map_reduce`, `map-rerank`\n",
    "\n",
    "If the question asked is outside the scope of context, then the model will reply it doesn't know the answer\n",
    "\n",
    "**Note**: if you are curious how the chain works, uncomment the `verbose=True` line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do some prompt engineering\n",
    "\n",
    "You can \"tune\" your prompt to get more or less verbose answers. For example, try to change the number of sentences, or remove that instruction all-together. You might also need to change the number of `max_tokens` (eg 1000 or 2000) to get the full answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this demo we used Claude V3 sonnet LLM to create conversational interface with following patterns:\n",
    "\n",
    "1. Chatbot (Basic - without context)\n",
    "\n",
    "2. Chatbot using prompt template(Langchain)\n",
    "\n",
    "3. Chatbot with personas\n",
    "\n",
    "4. Chatbot with context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "trainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
