{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conversational Interface - Medical Clinic\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. Chatbots uses natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. They can be accessed through various channels such as websites, social media platforms, and messaging apps. In this notebook, we will build a chatbot using two popular Foundation Models (FMs) in Amazon Bedrock, Claude V3 Sonnet and Llama 3 8b."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set up: Introduction to ChatBedrock\n",
    "\n",
    "**Supports the following**\n",
    "1. Multiple Models from Bedrock \n",
    "2. Converse API\n",
    "3. Ability to do tool binding\n",
    "4. Ability to plug with LangGraph flows"
   ]
  },
  {
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-09-15T18:07:04.018956Z",
     "start_time": "2024-09-15T18:07:03.927675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import os\n",
    "import textwrap\n",
    "import boto3\n",
    "import sys\n",
    "from io import StringIO\n",
    "from botocore.config import Config\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "my_config = Config(\n",
    "    region_name = 'us-west-2',\n",
    "    signature_version = 'v4',\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_ACCESS_SECRET\")\n",
    "\n",
    "boto3_bedrock = boto3.client('bedrock-runtime',\n",
    "                      aws_access_key_id=aws_access_key_id,\n",
    "                      aws_secret_access_key=aws_secret_access_key,\n",
    "                    config=my_config\n",
    "                      )\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LangChain Expression Language (LCEL):\n",
    "According to LangChain: *\"LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production).\"*\n",
    "\n",
    "On this tutorial we will be using **LangChain Expression Language** to define and invoke our Chatbots."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chatbot Architectures\n",
    "Chatbots can come in many shape and sizes, all depending on its use-case. Some models are meant to return general information. Others might be catered to a particular audience thus its inferences might be curtained to a particular tone. And others might need relevant context to give out an informed response to the user. Most robust ones will draw from all architectures and build on it. Below are a few popular types of Chatbots.\n",
    "\n",
    "1. **Chatbot (Naive)** - Zero-Shot chatbot with using FM model trained knowledge.\n",
    "2. **Chatbot using prompt** - Template driven - Chatbot with some context provided in the prompt template.\n",
    "3. **Chatbot with persona** - Chatbot with defined roles. i.e. Career Coach and Human interactions.\n",
    "4. **Contextual-aware chatbot** - Passing in context through an external file by generating embeddings.\n",
    "\n",
    "On this demo we will build a chatbot that will leverage various prompting architectures."
   ]
  },
  {
   "metadata": {
    "tags": []
   },
   "cell_type": "markdown",
   "source": [
    "## Chatbot: Naive approach (without context)\n",
    "\n",
    "The simplest form of chatbot is one that responds to the user simply by answering the question or completing the task. At the core these chatbots use the knowledge they were trained on to mimic human text."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:07:10.398455Z",
     "start_time": "2024-09-15T18:07:07.101318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "modelId = 'meta.llama3-8b-instruct-v1:0'\n",
    "\n",
    "messages_list=[\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': \"What is quantum mechanics? \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'assistant', \n",
    "        \"content\":[{\n",
    "            'text': \"It is a branch of physics that describes how matter and energy interact with discrete energy values \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': \"Can you explain a bit more about discrete energies?\"\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "\n",
    "    \n",
    "response = boto3_bedrock.converse(\n",
    "    messages=messages_list, \n",
    "    modelId='meta.llama3-8b-instruct-v1:0',\n",
    "    inferenceConfig={\n",
    "        \"temperature\": 0.5,\n",
    "        \"maxTokens\": 100,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    ")\n",
    "response_body = response['output']['message']['content'][0]['text'] \\\n",
    "        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
    "        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
    "        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n'\n",
    "\n",
    "print(response_body)\n",
    "\n",
    "\n",
    "def invoke_meta_converse(prompt_str,boto3_bedrock ):\n",
    "    modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "    messages_list=[{ \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': prompt_str\n",
    "        }]\n",
    "    }]\n",
    "  \n",
    "    response = boto3_bedrock.converse(\n",
    "        messages=messages_list, \n",
    "        modelId=modelId,\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0.5,\n",
    "            \"maxTokens\": 100,\n",
    "            \"topP\": 0.9\n",
    "        }\n",
    "    )\n",
    "    response_body = response['output']['message']['content'][0]['text']\n",
    "    return response_body\n",
    "\n",
    "\n",
    "invoke_meta_converse(\"what is quantum mechanics\", boto3_bedrock)   "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In classical physics, energy is often thought of as being continuous, meaning it can take on any value within a certain range. For example, the energy of a rolling ball can be thought of as varying continuously from zero to a maximum value.\n",
      "\n",
      "In contrast, quantum mechanics introduces the concept of discrete energy levels or quantization of energy. This means that energy can only take on specific, distinct values, rather than being continuous. These discrete energy levels are often referred to as \"energy eigenvalues\".\n",
      "\n",
      "Think\n",
      "--- Latency: 1335ms - Input tokens:58 - Output tokens:100 ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nQuantum mechanics is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, such as atoms and subatomic particles. It provides a new and different framework for understanding physical phenomena, and it has been incredibly successful in explaining a wide range of experimental results and making accurate predictions.\\n\\nAt the heart of quantum mechanics is the concept of wave-particle duality, which suggests that particles, such as electrons, can exhibit both wave-like and particle-like behavior depending on how they'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Using `ChatBedrock` and `HumanMessage` objects to wrap up our message and invoke the LLM"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:07:18.951273Z",
     "start_time": "2024-09-15T18:07:12.454423Z"
    }
   },
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=model_id,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in Seattle WA\"\n",
    "    )\n",
    "]\n",
    "bedrock_llm.invoke(messages)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nSeattle, Washington is known for its mild and wet climate, with significant rainfall throughout the year. Here's a breakdown of the typical weather patterns in Seattle:\\n\\n1. Rainfall: Seattle is famous for its rain, with an average annual rainfall of around 37 inches (94 cm). The rainiest months are November to March, with an average of 15-20 rainy days per month.\\n2. Temperature: Seattle's average temperature ranges from 35°F (2°C) in January (the coldest month) to 77°F (25°C) in July (the warmest month). The average temperature is around 50°F (10°C) throughout the year.\\n3. Sunshine: Seattle gets an average of 154 sunny days per year, with the sunniest months being July and August. However, the sun can be obscured by clouds and fog, reducing the amount of direct sunlight.\\n4. Fog: Seattle is known for its fog, especially during the winter months. The city can experience fog for several days at a time, especially in the mornings.\\n5. Wind: Seattle is known for its strong winds, especially during the winter months. The city can experience gusts of up to 40 mph (64 km/h) during storms.\\n6. Snow: Seattle rarely sees significant snowfall, with an average annual snowfall of around 6 inches (15 cm). The snowiest month is usually January, with an average of 1-2 inches (2.5-5 cm) of snow.\\n7. Summer: Seattle's summer months (June to August) are mild and pleasant, with average highs in the mid-70s to mid-80s (23-30°C). However, the city can experience occasional heatwaves, with temperatures reaching up to 90°F (32°C) or more.\\n8. Winter: Seattle's winter months (December to February) are cool and wet, with average lows in the mid-30s to mid-40s (2-7°C). The city can experience occasional cold snaps, with temperatures dropping below 20°F (-7°C) for short periods.\\n\\nOverall, Seattle's weather is characterized by mild temperatures, significant rainfall, and overcast skies. It's essential to pack layers and waterproof clothing when visiting the city, especially during the winter months.\", response_metadata={'ResponseMetadata': {'RequestId': '2a706be7-6635-43a6-b88d-a28c044b8a3d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 15 Sep 2024 18:07:18 GMT', 'content-type': 'application/json', 'content-length': '2211', 'connection': 'keep-alive', 'x-amzn-requestid': '2a706be7-6635-43a6-b88d-a28c044b8a3d'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 6164}}, id='run-2ea19fcc-2fa0-4245-be6f-91b30ed75e15-0', usage_metadata={'input_tokens': 22, 'output_tokens': 472, 'total_tokens': 494})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chatbot with a role: Adding prompt templates, Zero Shot\n",
    "Getting a factual response is important, yet another aspect to keep in mind when building a Chatbot is the tone used in giving a user the response. Prompts are a powerful way of dictating the model what the tone and approach to answer question should be.\n",
    "\n",
    "1. You can define prompts as a list of messages, all modes expect SystemMessage, and then alternate with HumanMessage and AIMessage\n",
    "2. The Variables defined in the chat template need to be sent into the chain as dict with the keys being the variable names\n",
    "3. You can define the template as a tuple with (\"system\", \"message\") or can be using the class SystemMessage \n",
    "4. Invoke creates a final resulting object of type <class 'langchain_core.prompt_values.ChatPromptValue'> with the variables substituted with their values "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:07:24.678541Z",
     "start_time": "2024-09-15T18:07:22.054213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages( \n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pirate_chain = (\n",
    "    RunnablePassthrough()\n",
    "    | prompt\n",
    "    | bedrock_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print_ww(pirate_chain.invoke({\"input\":\"What is the weather in Texas?\"})) \n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arrr, shiver me timbers! As a pirate, I be more concerned with the weather on the high seas than on\n",
      "land. But, I be havin' a mate who's been to Texas, and he told me it be a mighty fine place with a\n",
      "whole lot o' sunshine! He said the weather be hot and dry in the summer, like a swelterin' tropical\n",
      "isle, and mild in the winter, like a gentle sea breeze on a balmy day.\n",
      "\n",
      "But, I be warnin' ye, matey, the weather in Texas be as unpredictable as a sea siren's song! Ye\n",
      "never know when a storm might be brewin' on the horizon, like a mighty tempest on the open waters!\n",
      "So, if ye be plannin' to set sail fer Texas, make sure ye be prepared fer any weather that comes yer\n",
      "way!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Few-shot approach to prompting: Selecting the right prompt for the job\n",
    "\n",
    "Chatbots can be powerful tools for automatising workflows and relaying accurate information, however sometimes the most difficult aspect of building a chatbot is conveying the right tone at the right time and with the right context. Something that one shot prompting (one time prompt at the beginning) seems to struggle with to address this issue we can introduce few-shot prompting to our chat. The goal of few-shot prompt is to dynamically select examples based on an input, and then format the examples into a final prompt to provide for the model."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:07:27.303840Z",
     "start_time": "2024-09-15T18:07:27.300690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "\n",
    "## In this example we use an in memory store called FAISS and AWS Titan Embeddings model\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:09.906261Z",
     "start_time": "2024-09-15T18:33:09.252912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "examples = [\n",
    "    {\"input\": \"What is the weather in New York city?\", \"output\": \"Respond using professional weatherman terminology:\\n\"},\n",
    "    {\"input\": \"Tell me a joke\", \"output\": \"Respond as if you were a famous comedian that likes to joke about farm animals:\\n\"},\n",
    "    {\"input\": \"Give me book recommendations\", \"output\": \"Respond as if you are a mistery novels fan:\\n\"},\n",
    "    {\"input\": \"There is something wrong with my computer\", \"output\": \"Respond as if you are a former pirate turned IT repair expert:\\n\"},\n",
    "]\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "embeddings = br_embeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(to_vectorize, embeddings, metadatas=examples)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With a vectorstore created, we can create the `example_selector` this object contains our vectorized prompt examples for later retrieval. Here we will instruct it to only fetch the top 1 example."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:11.396302Z",
     "start_time": "2024-09-15T18:33:11.231605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "# The prompt template will load examples by passing the input do the `select_examples` method\n",
    "example_selector.select_examples({\"input\": \"Is it hot out?\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'There is something wrong with my computer',\n",
       "  'output': 'Respond as if you are a former pirate turned IT repair expert:\\n'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:11.949138Z",
     "start_time": "2024-09-15T18:33:11.945249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the few-shot prompt.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # The input variables select the values to pass to the example_selector\n",
    "    input_variables=[\"input\"],\n",
    "    example_selector=example_selector,\n",
    "    # Define how each example will be formatted.\n",
    "    # In this case, each example will become 2 messages:\n",
    "    # 1 human, and 1 AI\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You curtain your answer based on the examples provided\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:15.146718Z",
     "start_time": "2024-09-15T18:33:13.202857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = final_prompt | bedrock_llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"What is the weather in Texas?\"})"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:34:34.717339Z",
     "start_time": "2024-09-15T18:34:34.713407Z"
    }
   },
   "cell_type": "code",
   "source": "response",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nAccording to current weather reports, a high-pressure system is dominating the Lone Star State, bringing mostly clear skies and warm temperatures to the region. Daytime highs are expected to reach the mid-70s to low 80s Fahrenheit (23-27°C) across the state, with overnight lows dipping into the mid-50s to low 60s Fahrenheit (13-18°C). However, it's worth noting that some areas in the Panhandle and West Texas may experience a slight chance of scattered thunderstorms later in the day.\", response_metadata={'ResponseMetadata': {'RequestId': '6fe91d0a-a575-4abf-9ffd-497634363670', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 15 Sep 2024 18:33:15 GMT', 'content-type': 'application/json', 'content-length': '677', 'connection': 'keep-alive', 'x-amzn-requestid': '6fe91d0a-a575-4abf-9ffd-497634363670'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 1456}}, id='run-57a38b85-baf5-4094-a46d-6ec4efc92fd2-0', usage_metadata={'input_tokens': 56, 'output_tokens': 110, 'total_tokens': 166})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:15.653489Z",
     "start_time": "2024-09-15T18:33:15.650302Z"
    }
   },
   "cell_type": "code",
   "source": "response.content",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nAccording to current weather reports, a high-pressure system is dominating the Lone Star State, bringing mostly clear skies and warm temperatures to the region. Daytime highs are expected to reach the mid-70s to low 80s Fahrenheit (23-27°C) across the state, with overnight lows dipping into the mid-50s to low 60s Fahrenheit (13-18°C). However, it's worth noting that some areas in the Panhandle and West Texas may experience a slight chance of scattered thunderstorms later in the day.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:20.599339Z",
     "start_time": "2024-09-15T18:33:17.444751Z"
    }
   },
   "cell_type": "code",
   "source": "chain.invoke({\"input\": \"Do you have any movie recommendations?\"}).content",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nI\\'m a huge fan of mystery and thriller movies! Here are a few recommendations:\\n\\n1. \"Seven\" (1995) - A gritty and intense crime thriller that follows two detectives as they hunt for a serial killer.\\n2. \"Memento\" (2000) - A mind-bending neo-noir that tells the story of a man with short-term memory loss trying to avenge his wife\\'s murder.\\n3. \"Prisoners\" (2013) - A tense and emotional thriller about two families whose daughters go missing and the desperate measures they take to find them.\\n4. \"Gone Girl\" (2014) - A twisty and suspenseful adaptation of Gillian Flynn\\'s bestselling novel about a couple whose seemingly perfect marriage turns out to be a facade.\\n5. \"Knives Out\" (2019) - A clever and entertaining whodunit that follows a detective as he investigates the murder of a wealthy author.\\n\\nI hope you enjoy these recommendations!'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:34:08.500117Z",
     "start_time": "2024-09-15T18:34:06.579198Z"
    }
   },
   "cell_type": "code",
   "source": "chain.invoke({\"input\": \"My computer keeps crashing, can you help me fix it?\"}).content",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nArrr, shiver me bytes! Computer crashin', eh? Alright then, matey, let's set sail fer troubleshootin'!\\n\\nFirst, tell me, when did this start happenin'? Was it sudden, or did it start slowin' down like a ship in shallow waters?\\n\\nAnd what's the error message sayin'? Is it a blue screen o' death, or just a plain ol' freeze?\\n\\nAlso, have ye made any recent changes to yer computer, like installin' new software or addin' hardware?\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
