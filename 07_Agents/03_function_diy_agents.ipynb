{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Invoking a function or do-it-yourself Agents with Bedrock\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio. You can also run on a local setup, as long as you have the right IAM credentials to invoke the Claude model via Bedrock*\n",
    "\n",
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate an implementation of Function Calling with Anthropic's Claude models via Bedrock. This notebook is inspired by the [original work](https://drive.google.com/drive/folders/1-94Fa3HxEMkxkwKppe8lp_9-IXXvsvv1) by the Anthropic Team and modified it for use with Amazon Bedrock.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeedd9f-f0a3-4f8e-934d-22f6f7a89de5",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. These use natural language processing (NLP) and machine learning algorithms to understand and respond to user queries and can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. usuallythey are augmented by fetching information from various channels such as websites, social media platforms, and messaging apps which involve a complex workflow as shown below\n",
    "\n",
    "\n",
    "### Chatbot using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Agents Interface](./images/agents.jpg)\n",
    "\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "1. **QA** - Respond to queries based on look ups\n",
    "2. **Contextual-aware chatbot** - Dialog turn by turn with extrenal look ups\n",
    "\n",
    "### Framework for building functions/Agents with Amazon Bedrock\n",
    "In Conversational interfaces such as chatbots, it is highly important to remember previous interactions, both at a short term but also at a long term level. Further we need to enhance with external tools which are needed to complete the queries. There are many providers which help to create functions or agents. We will look at a vanila implementation for how to do it yourself\n",
    "\n",
    "### Building  - Key Elements\n",
    "\n",
    "The first process in a building a contextual-aware chatbot is to identify the tools which can be called by the LLM's. \n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "### Architecture [Weather lookup]\n",
    "We Search and look for the Latitude and Longitude and then invoke the weather app to get predictions\n",
    "\n",
    "![Amazon Bedrock - Agents Interface](./images/weather.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27610c0f-7de6-4440-8f76-decf30e3c5ca",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've run the [Bedrock boto3 setup notebook](../00_Intro/bedrock_boto3_setup.ipynb#Prerequisites) notebook. ⚠️ ⚠️ ⚠️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xmltodict==0.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb6bee-7654-4269-9127-9afa4e823454",
   "metadata": {},
   "source": [
    "### Anthropic Claude\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"\\n\\nHuman:<prompt>\\n\\nAnswer:\",\n",
    "    \"max_tokens_to_sample\": 300,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"completion\": \"<output>\",\n",
    "    \"stop_reason\": \"stop_sequence\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c0fe6-576a-4380-89aa-726bab5d65ff",
   "metadata": {},
   "source": [
    "### Bedrock model\n",
    "\n",
    "Anthropic Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeff818",
   "metadata": {},
   "source": [
    "The key for this to work is to let LLm which is Claude models know about a set of `tools` that it has available i.e. functions it can call between a set of tags. This is possible because Anthropic's Claude models have been extensively trained on such tags in its training corpus.\n",
    "\n",
    "Then present a way to call the tools in a step by step fashion till it gets the right answer. We create a set of callable functions in another file called `tools.py`\n",
    "\n",
    "A sample `tools.py` is added to the same folder of this notebook and can be modified to suit your needs. Import it so that we have access to it in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tools_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee67162",
   "metadata": {},
   "source": [
    "#### Helper function to pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60ff51",
   "metadata": {},
   "source": [
    "###  Create a set of helper function\n",
    "\n",
    "we will create a set of functions which we can the re use in our application\n",
    "1. We will need to create a prompt template. This template helps Bedrock models understand the tools and how to invoke them.\n",
    "2. Create a method to read the available tools and add it to the prompt being used to invoke Claude\n",
    "3. Call function which will be respinsbile to actually invoke the function with the `right` parameters\n",
    "4. Format Results for helping the Model leverage the results for summarization\n",
    "5. Add to prompt. The result which come back need to be added to the the prompt and model invoked again to get the right results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ffb692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(tools_string, user_input):\n",
    "    prompt_template = f\"\"\"\n",
    "In this environment you have access to a set of tools you can use to answer the user's question.\n",
    "\n",
    "You may call them like this. Only invoke one function at a time and wait for the results before invoking another function:\n",
    "<function_calls>\n",
    "<invoke>\n",
    "<tool_name>$TOOL_NAME</tool_name>\n",
    "<parameters>\n",
    "<$PARAMETER_NAME>$PARAMETER_VALUE</$PARAMETER_NAME>\n",
    "...\n",
    "</parameters>\n",
    "</invoke>\n",
    "</function_calls>\n",
    "\n",
    "Here are the tools available:\n",
    "<tools>\n",
    "{tools_string}\n",
    "</tools>\n",
    "\n",
    "Human:\n",
    "{user_input}\n",
    "\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8103b5",
   "metadata": {},
   "source": [
    "### Add Tools\n",
    "\n",
    "Recusrively add the available tools from the tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177591b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tools():\n",
    "    tools_string = \"\"\n",
    "    for tool_spec in tools_agents.list_of_tools_specs:\n",
    "        tools_string += tool_spec\n",
    "    return tools_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a53483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from defusedxml import ElementTree\n",
    "from collections import defaultdict\n",
    "print(add_tools())\n",
    "# Uncomment print to test if tools is being imported correctly and your functions are correctly being interpreted via the tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c376be",
   "metadata": {},
   "source": [
    "This `call_function` will be used later to extract the name of the tool from your `tools.py` file and call it from the output of Bedrock model. A few more helper functions are defined and can be used as is without modification for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_function(tool_name, parameters):\n",
    "    func = getattr(tools_agents, tool_name)\n",
    "    output = func(**parameters)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61c225",
   "metadata": {},
   "source": [
    "To help Bedrock models understand the results and use that for generation we need to `format_results` for deciphering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_result(tool_name, output):\n",
    "    return f\"\"\"\n",
    "<function_results>\n",
    "<result>\n",
    "<tool_name>{tool_name}</tool_name>\n",
    "<stdout>\n",
    "{output}\n",
    "</stdout>\n",
    "</result>\n",
    "</function_results>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e4062d",
   "metadata": {},
   "source": [
    "Here is where we can glue all the pieces together. Print the final prompt data to double check if the input is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Can you check the weather for me in Marysville WA?\"\n",
    "tools_string = add_tools()\n",
    "prompt_data = create_prompt(tools_string, user_input)\n",
    "print_ww(prompt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe6dab",
   "metadata": {},
   "source": [
    "This next cell is to test the response of the Bedrock models based on your constructed input. Note that we have not instrumented output to call the actual functions, but this should give you an idea on how Claude's output can be parsed and the corresponding functions can be subsequently called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "def invoke_model(prompt):\n",
    "    body = json.dumps({\n",
    "        \"prompt\": prompt, \n",
    "        \"max_tokens_to_sample\": 1000,\n",
    "        \"temperature\": 0,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    })\n",
    "    modelId = \"anthropic.claude-v2\"\n",
    "\n",
    "    response = boto3_bedrock.invoke_model(\n",
    "        body=body, modelId=modelId, accept=\"application/json\", contentType=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    #print_ww(f\"invoke_model()::response_body={response_body}::\")\n",
    "    #print_ww(f\"invoke_model()::completion={response_body.get('completion')}::\")\n",
    "    #print_ww(f\"invoke_model()::stop_reason={response_body.get('stop_reason')}::\")\n",
    "    return (response_body.get(\"completion\"), response_body.get(\"stop_reason\"))\n",
    "\n",
    "#completion, stop_res = invoke_model(prompt_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386dd366",
   "metadata": {},
   "source": [
    "### Invoke and Add\n",
    "\n",
    "Here we will invoke the function as per what ever the model asks us too and append the results to the prompt for history and invoke again\n",
    "\n",
    "This process continues till the model is able to complete the task asked of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "\n",
    "def invoke_func_and_add_to_prompt(prompt, completion):\n",
    "    find_index=0\n",
    "    start_index = 0\n",
    "    end_index = start_index + 17\n",
    "    resp =  completion\n",
    "    func_invoked = False\n",
    "    #print(len(resp))\n",
    "    while start_index < len(resp) and find_index < len(resp) and end_index < len(resp):\n",
    "        start_index = resp.find(\"<function_calls>\", find_index)\n",
    "        end_index = resp.find(\"</function_calls>\", start_index)\n",
    "        if start_index < 0 or end_index < 0 or find_index <0:\n",
    "            break\n",
    "        func_dict = xmltodict.parse(resp[start_index:end_index+17])\n",
    "        print(start_index, end_index, find_index, func_dict) #, resp[start_index:end_index+17]\n",
    "        find_index = end_index+17\n",
    "        func_name = func_dict['function_calls']['invoke']['tool_name']\n",
    "        func_params = func_dict['function_calls']['invoke']['parameters']\n",
    "        function_result = call_function(func_name, func_params)\n",
    "        function_result = format_result(func_name, function_result)\n",
    "        # - have to put it back as XML for Claude to pick it up\n",
    "        print_ww(f\"invoke_func_and_add_to_prompt():: func:invoked::{func_name}::, params={func_params}::, result={function_result}::\")\n",
    "        prompt += \"</function_calls>\"\n",
    "        prompt += function_result\n",
    "        func_invoked = True\n",
    "    return (prompt, func_invoked)\n",
    "        \n",
    "#invoke_func_and_add_to_prompt(prompt_data, completion)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60eb1c",
   "metadata": {},
   "source": [
    "### Run loop\n",
    "\n",
    "This function is the actual orchestrator of the function calling logic. Here's how it works:\n",
    "\n",
    "1. We kick off a loop that first calls Bedrock model with our tool use prompt with the tool specs and the user input loaded into it.\n",
    "2. We get the completion from the model and check if the stop sequence for the completion was the closing tag for a function call, ```</function_calls>```\n",
    "3. If the completion does in fact contain a function call, we extract out the tool name and the tool parameters from the tags.\n",
    "4. We then call the function that model has decided to invoke using our helped auxillary function.\n",
    "5. We take the results of the function call, format them into an tag structure, and add them back to the prompt. This works because with subsequent calls, we are basically pre-filling the output of the model and asking it to pick up where it left off, with addition data from the previous results.\n",
    "6. We repeat the loop starting at step 1 with the original prompt plus the text that has been appended.\n",
    "7. This process continues until model finally outputs an answer and we break the loop.\n",
    "8. To avoid a never ending loop we will run the loop just a couple of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262dd848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_loop(prompt, curr_iteration=1, max_iterations=10):\n",
    "    \n",
    "    # Start function calling loop -- Ideally we will have this in a while True loop but we do not want to run forever in case of issues \n",
    "    # - hence we have constrained it \n",
    "    completion = \"\"\n",
    "    while curr_iteration < max_iterations:\n",
    "        curr_iteration += 1\n",
    "        \n",
    "        completion, stop_res = invoke_model(prompt)\n",
    "        #print_ww(f\"run_loop():: prompt model invoked:prompt={prompt}::stop_reason={stop_res}: completion={completion}\")\n",
    "        print_ww(completion)\n",
    "\n",
    "        # Append the completion to the end of the prommpt\n",
    "        prompt += completion\n",
    "        if stop_res == 'stop_sequence':\n",
    "            # If Claude made a function call\n",
    "            #print(completion)\n",
    "            prompt, func_invoked = invoke_func_and_add_to_prompt(prompt, completion)\n",
    "            if not func_invoked:\n",
    "                print(f\"run_loop()::Function invocation finished::Breaking:\")\n",
    "                break\n",
    "        else:\n",
    "            # If Claude did not make a function call\n",
    "            # outputted answer\n",
    "            print(f\"run_loop()::No more functions to be run::Breaking:\")\n",
    "            print(completion)\n",
    "            break\n",
    "        \n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234d381",
   "metadata": {},
   "source": [
    "Let's run it all together now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2bee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Can you check the weather for me in Marysville WA?\"\n",
    "tools_string = add_tools()\n",
    "prompt_data = create_prompt(tools_string, user_input)\n",
    "weather_data = run_loop(prompt_data, curr_iteration=1, max_iterations=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{weather_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a5501",
   "metadata": {},
   "source": [
    "### Check another location weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Can you check the weather for me in Seattle WA?\"\n",
    "tools_string = add_tools()\n",
    "prompt_data = create_prompt(tools_string, user_input)\n",
    "run_loop(prompt_data, curr_iteration=1, max_iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this notebook we showed some basic examples of leveraging tools and agents when invoking Amazon Bedrock models using the AWS Python SDK. You're now ready to explore the other labs to dive deeper on different use-cases and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733f3e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
